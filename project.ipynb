{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('project_dataset.zip') as zipped_file:\n",
    "    zipped_file.extractall(Path.cwd()/'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'development.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1833972543</td>\n",
       "      <td>Mon May 18 01:08:27 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Killandra</td>\n",
       "      <td>@MissBianca76 Yes, talking helps a lot.. going...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1980318193</td>\n",
       "      <td>Sun May 31 06:23:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>IMlisacowan</td>\n",
       "      <td>SUNSHINE. livingg itttt. imma lie on the grass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1994409198</td>\n",
       "      <td>Mon Jun 01 11:52:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>yaseminx3</td>\n",
       "      <td>@PleaseBeMine Something for your iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1824749377</td>\n",
       "      <td>Sun May 17 02:45:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>no_surprises</td>\n",
       "      <td>@GabrielSaporta couldn't get in to the after p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2001199113</td>\n",
       "      <td>Tue Jun 02 00:08:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Rhi_ShortStack</td>\n",
       "      <td>@bradiewebbstack awww is andy being mean again...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment         ids                          date      flag  \\\n",
       "0          1  1833972543  Mon May 18 01:08:27 PDT 2009  NO_QUERY   \n",
       "1          1  1980318193  Sun May 31 06:23:17 PDT 2009  NO_QUERY   \n",
       "2          1  1994409198  Mon Jun 01 11:52:54 PDT 2009  NO_QUERY   \n",
       "3          0  1824749377  Sun May 17 02:45:34 PDT 2009  NO_QUERY   \n",
       "4          0  2001199113  Tue Jun 02 00:08:07 PDT 2009  NO_QUERY   \n",
       "\n",
       "             user                                               text  \n",
       "0       Killandra  @MissBianca76 Yes, talking helps a lot.. going...  \n",
       "1     IMlisacowan  SUNSHINE. livingg itttt. imma lie on the grass...  \n",
       "2       yaseminx3           @PleaseBeMine Something for your iphone   \n",
       "3    no_surprises  @GabrielSaporta couldn't get in to the after p...  \n",
       "4  Rhi_ShortStack  @bradiewebbstack awww is andy being mean again...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 224994 entries, 0 to 224993\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   sentiment  224994 non-null  int64 \n",
      " 1   ids        224994 non-null  int64 \n",
      " 2   date       224994 non-null  object\n",
      " 3   flag       224994 non-null  object\n",
      " 4   user       224994 non-null  object\n",
      " 5   text       224994 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 10.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_dev.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    True\n",
       "ids          True\n",
       "date         True\n",
       "flag         True\n",
       "user         True\n",
       "text         True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.notna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No column has empty cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    130157\n",
       "0     94837\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more comments classified as positive (130157) than as negative (94837)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Date' analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Mon May 18 01:08:27 PDT 2009\n",
       "1    Sun May 31 06:23:17 PDT 2009\n",
       "2    Mon Jun 01 11:52:54 PDT 2009\n",
       "3    Sun May 17 02:45:34 PDT 2009\n",
       "4    Tue Jun 02 00:08:07 PDT 2009\n",
       "Name: date, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PDT    224994\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['date'].astype('string').str.split(' ').apply(lambda x : x[-2]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only timezone in the dataset is PDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009    224994\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['date'].astype('string').str.split(' ').apply(lambda x : x[-1]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tweets are from year 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>date_mod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon May 18 01:08:27 PDT 2009</td>\n",
       "      <td>2009-05-18 01:08:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sun May 31 06:23:17 PDT 2009</td>\n",
       "      <td>2009-05-31 06:23:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon Jun 01 11:52:54 PDT 2009</td>\n",
       "      <td>2009-06-01 11:52:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sun May 17 02:45:34 PDT 2009</td>\n",
       "      <td>2009-05-17 02:45:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tue Jun 02 00:08:07 PDT 2009</td>\n",
       "      <td>2009-06-02 00:08:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224989</th>\n",
       "      <td>Sat Jun 20 20:36:48 PDT 2009</td>\n",
       "      <td>2009-06-20 20:36:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224990</th>\n",
       "      <td>Mon Jun 01 01:25:45 PDT 2009</td>\n",
       "      <td>2009-06-01 01:25:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224991</th>\n",
       "      <td>Mon Jun 01 06:38:10 PDT 2009</td>\n",
       "      <td>2009-06-01 06:38:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224992</th>\n",
       "      <td>Fri Jun 19 08:51:56 PDT 2009</td>\n",
       "      <td>2009-06-19 08:51:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224993</th>\n",
       "      <td>Wed Jun 03 06:00:29 PDT 2009</td>\n",
       "      <td>2009-06-03 06:00:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224994 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date            date_mod\n",
       "0       Mon May 18 01:08:27 PDT 2009 2009-05-18 01:08:27\n",
       "1       Sun May 31 06:23:17 PDT 2009 2009-05-31 06:23:17\n",
       "2       Mon Jun 01 11:52:54 PDT 2009 2009-06-01 11:52:54\n",
       "3       Sun May 17 02:45:34 PDT 2009 2009-05-17 02:45:34\n",
       "4       Tue Jun 02 00:08:07 PDT 2009 2009-06-02 00:08:07\n",
       "...                              ...                 ...\n",
       "224989  Sat Jun 20 20:36:48 PDT 2009 2009-06-20 20:36:48\n",
       "224990  Mon Jun 01 01:25:45 PDT 2009 2009-06-01 01:25:45\n",
       "224991  Mon Jun 01 06:38:10 PDT 2009 2009-06-01 06:38:10\n",
       "224992  Fri Jun 19 08:51:56 PDT 2009 2009-06-19 08:51:56\n",
       "224993  Wed Jun 03 06:00:29 PDT 2009 2009-06-03 06:00:29\n",
       "\n",
       "[224994 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['date_mod'] = df_dev['date'].\\\n",
    "    astype('string')\\\n",
    "    .str.split(' ')\\\n",
    "    .apply(lambda x : ' '.join([x[i] for i in [1,2,3,5]]))\\\n",
    "    .pipe(pd.to_datetime)\n",
    "\n",
    "df_dev[['date','date_mod']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2009-04-06 22:19:57'), Timestamp('2009-06-25 10:28:28'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['date_mod'].min(), df_dev['date_mod'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets were written between 2009-04-06 22:19:57 and 2009-06-25 10:28:28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['date'] = df_dev['date_mod']\n",
    "df_dev.drop('date_mod',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'flag' analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NO_QUERY    224994\n",
       "Name: flag, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['flag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unique value for 'flag' is 'NO_QUERY', so this field can be discarded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.drop('flag',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'user' analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency of positive or negative for user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='sentiment', ylabel='Count'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "average_user_sentiment = df_dev.groupby('user')['sentiment'].mean()\n",
    "average_user_sentiment = pd.DataFrame(average_user_sentiment).reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "sns.histplot(data=average_user_sentiment, x='sentiment', bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many users that made almost all tweets classified as positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet for 'user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lost_dog           412\n",
       "webwoke            259\n",
       "tweetpet           232\n",
       "SallytheShizzle    211\n",
       "VioletsCRUK        209\n",
       "                  ... \n",
       "shadowsinstone      11\n",
       "lovenadav           11\n",
       "SharaBlckBarbie     11\n",
       "mementototem        11\n",
       "gracechareas        11\n",
       "Name: user, Length: 10647, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['user'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10647 distinct users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15,5))\n",
    "# user_value_counts = pd.DataFrame(df_dev['user'].value_counts())[:100].reset_index()\n",
    "# sns.barplot(data=user_value_counts, x='index', y='user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few users made many more tweets than others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main \"twetterers\" analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 'lost_dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>1686517168</td>\n",
       "      <td>2009-05-03 05:45:51</td>\n",
       "      <td>lost_dog</td>\n",
       "      <td>@bthenextstep I am lost. Please help me find a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>0</td>\n",
       "      <td>2322687735</td>\n",
       "      <td>2009-06-24 23:04:36</td>\n",
       "      <td>lost_dog</td>\n",
       "      <td>@marissa_in_cali I am lost. Please help me fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>0</td>\n",
       "      <td>1984183693</td>\n",
       "      <td>2009-05-31 14:36:16</td>\n",
       "      <td>lost_dog</td>\n",
       "      <td>@that_much I am lost. Please help me find a go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>0</td>\n",
       "      <td>1975991079</td>\n",
       "      <td>2009-05-30 16:20:57</td>\n",
       "      <td>lost_dog</td>\n",
       "      <td>@dcunited I am lost. Please help me find a goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>0</td>\n",
       "      <td>2220606046</td>\n",
       "      <td>2009-06-18 03:30:50</td>\n",
       "      <td>lost_dog</td>\n",
       "      <td>@aiimee_x I am lost. Please help me find a goo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment         ids                date      user  \\\n",
       "30            0  1686517168 2009-05-03 05:45:51  lost_dog   \n",
       "580           0  2322687735 2009-06-24 23:04:36  lost_dog   \n",
       "785           0  1984183693 2009-05-31 14:36:16  lost_dog   \n",
       "1536          0  1975991079 2009-05-30 16:20:57  lost_dog   \n",
       "1546          0  2220606046 2009-06-18 03:30:50  lost_dog   \n",
       "\n",
       "                                                   text  \n",
       "30    @bthenextstep I am lost. Please help me find a...  \n",
       "580   @marissa_in_cali I am lost. Please help me fin...  \n",
       "785   @that_much I am lost. Please help me find a go...  \n",
       "1536  @dcunited I am lost. Please help me find a goo...  \n",
       "1546  @aiimee_x I am lost. Please help me find a goo...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[df_dev['user'] == 'lost_dog'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the tweets made by 'lost_dog' follow the same pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    412\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[df_dev['user'] == 'lost_dog']['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All its tweets are classified as negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 'webwoke'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    196\n",
      "Name: sentiment, dtype: int64\n",
      "1    63\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_dev[df_dev['user'] == 'webwoke'].loc[df_dev['text'].str.contains('drop')]['sentiment'].value_counts())\n",
    "print(df_dev[df_dev['user'] == 'webwoke'].loc[df_dev['text'].str.contains('move up')]['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the the tweets made by 'webwoke' follow the same pattern; if the tweet contains the word 'drop', it is classified as negative, instead if it contains the expression 'move up' it is classified as positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 'tweetpet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0    232\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_dev[df_dev['user'] == 'tweetpet']['text'].str.contains('Clean Me').any())\n",
    "print(df_dev[df_dev['user'] == 'tweetpet']['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the tweets made by 'tweetpet' follow the same pattern, and contain 'Clean Me' as text.  \n",
    "All the tweets are classified as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. SallytheShizzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0</td>\n",
       "      <td>1956695119</td>\n",
       "      <td>2009-05-28 22:27:44</td>\n",
       "      <td>SallytheShizzle</td>\n",
       "      <td>@ColorMeKelly -sigh- we won't get to hear for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>0</td>\n",
       "      <td>2015645364</td>\n",
       "      <td>2009-06-03 05:11:07</td>\n",
       "      <td>SallytheShizzle</td>\n",
       "      <td>@OfficialAS  oh well. It's not the hol house t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>1</td>\n",
       "      <td>2015042588</td>\n",
       "      <td>2009-06-03 03:31:13</td>\n",
       "      <td>SallytheShizzle</td>\n",
       "      <td>@MAGGIECHICKEN meh same diff. Just still with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>0</td>\n",
       "      <td>1988723315</td>\n",
       "      <td>2009-05-31 23:11:16</td>\n",
       "      <td>SallytheShizzle</td>\n",
       "      <td>@OfficialAS sorry  I've been TRYING to finish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>1</td>\n",
       "      <td>2049858625</td>\n",
       "      <td>2009-06-05 17:37:39</td>\n",
       "      <td>SallytheShizzle</td>\n",
       "      <td>#kevinjonas is the best!  happy #kevinjonas day!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment         ids                date             user  \\\n",
       "485           0  1956695119 2009-05-28 22:27:44  SallytheShizzle   \n",
       "509           0  2015645364 2009-06-03 05:11:07  SallytheShizzle   \n",
       "2130          1  2015042588 2009-06-03 03:31:13  SallytheShizzle   \n",
       "2522          0  1988723315 2009-05-31 23:11:16  SallytheShizzle   \n",
       "2894          1  2049858625 2009-06-05 17:37:39  SallytheShizzle   \n",
       "\n",
       "                                                   text  \n",
       "485   @ColorMeKelly -sigh- we won't get to hear for ...  \n",
       "509   @OfficialAS  oh well. It's not the hol house t...  \n",
       "2130  @MAGGIECHICKEN meh same diff. Just still with ...  \n",
       "2522  @OfficialAS sorry  I've been TRYING to finish ...  \n",
       "2894  #kevinjonas is the best!  happy #kevinjonas day!   "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[df_dev['user'] == 'SallytheShizzle'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three users by number of tweets seem to be bots, while this one doesn't. I'll stop here this punctual analysis.  \n",
    "These users may be dropped during the preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'text' analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is created using sklearn's TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",  \n",
    "    binary=True, \n",
    "    use_idf=True, \n",
    "    norm='l2',\n",
    "    smooth_idf=True\n",
    ")\n",
    "\n",
    "wpm = vectorizer.fit_transform(df_dev['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "word_freq = pd.Series(\n",
    "    data = np.asarray(wpm.sum(axis=0)).squeeze(),\n",
    "    index = vectorizer.get_feature_names_out()\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "word_freq = word_freq[:N]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15,5))\n",
    "# sns.histplot(data=pd.DataFrame(word_freq,columns=['word_freq']),x='word_freq')\n",
    "# word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ind = [w in word_freq.index for w in vectorizer.get_feature_names_out()]\n",
    "\n",
    "words_df = pd.DataFrame(\n",
    "    data = wpm[:,word_ind].toarray(),\n",
    "    columns = vectorizer.get_feature_names_out()[word_ind],\n",
    "    index = df_dev.index\n",
    ").add_prefix('word_')\n",
    "\n",
    "# words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = words_df.values\n",
    "y = df_dev['sentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, train_size=0.7, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7088160731632195"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "reg = RandomForestClassifier(100, random_state=42, n_jobs=-2)\n",
    "reg.fit(X_train, y_train)\n",
    "f1_score(y_test, reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: best RandomForestClassifier hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# param_grid = {\n",
    "#     \"n_estimators\": [50,75,100],\n",
    "#     \"criterion\": [\"gini\", \"entropy\"],\n",
    "#     \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "#     \"random_state\": [42],\n",
    "#     \"n_jobs\": [-1],\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50,75],\n",
    "    \"criterion\": [\"entropy\"],\n",
    "    \"max_features\": [\"auto\"],\n",
    "    \"random_state\": [42],\n",
    "    \"n_jobs\": [-2],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    RandomForestClassifier(), \n",
    "    param_grid, \n",
    "    scoring=\"f1_macro\", \n",
    "    # n_jobs=-1, \n",
    "    cv=5,\n",
    "    verbose=3\n",
    ")\n",
    "# gs.fit(X, y)\n",
    "\n",
    "# print(gs.best_score_)\n",
    "# print(gs.best_estimator_)\n",
    "# print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinations of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",  \n",
    "    binary=True, \n",
    "    use_idf=True, \n",
    "    norm='l2',\n",
    "    smooth_idf=True\n",
    ")\n",
    "\n",
    "wpm = vectorizer.fit_transform(df_dev['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: removal of '@words' in 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_noAt = df_dev.copy()\n",
    "df_dev_noAt['text'] = df_dev['text'].str.split().apply(lambda x : ' '.join([i for i in x if not(i.startswith('@'))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",  \n",
    "    binary=True, \n",
    "    use_idf=True, \n",
    "    norm='l2',\n",
    "    smooth_idf=True\n",
    ")\n",
    "\n",
    "wpm = vectorizer.fit_transform(df_dev_noAt['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "word_freq = pd.Series(\n",
    "    data = np.asarray(wpm.sum(axis=0)).squeeze(),\n",
    "    index = vectorizer.get_feature_names_out()\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "word_freq = word_freq[:N]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15,5))\n",
    "# sns.histplot(data=pd.DataFrame(word_freq,columns=['word_freq']),x='word_freq')\n",
    "# word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ind = [w in word_freq.index for w in vectorizer.get_feature_names_out()]\n",
    "\n",
    "words_df = pd.DataFrame(\n",
    "    data = wpm[:,word_ind].toarray(),\n",
    "    columns = vectorizer.get_feature_names_out()[word_ind],\n",
    "    index = df_dev.index\n",
    ").add_prefix('word_')\n",
    "\n",
    "# words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = words_df.values\n",
    "y = df_dev['sentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, train_size=0.7, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7107991667455733"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "reg = RandomForestClassifier(100, random_state=42, n_jobs=-2)\n",
    "reg.fit(X_train, y_train)\n",
    "f1_score(y_test, reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result does not change, the words are the same. It may have some consequences if we apply stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer\n",
    "import nltk.sentiment.util as nsu\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "nltk_test = df_dev.copy()\n",
    "nltk_test['text'] = nltk_test['text'].str.split().apply(lambda x : [i for i in x if not(i.startswith('@'))])\n",
    "\n",
    "X = nltk_test['text'].values\n",
    "y = nltk_test['sentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, train_size=0.7, random_state=50)\n",
    "\n",
    "\n",
    "nltk_sa = SentimentAnalyzer()\n",
    "\n",
    "X_train_mod = nltk_sa.all_words([nsu.mark_negation(doc,shallow=True) for doc in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_feats = nltk_sa.unigram_word_feats(X_train_mod, top_n=150)\n",
    "\n",
    "nltk_sa.add_feat_extractor(nsu.extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = nltk_sa.apply_features(tuple([(x_t, y_t) for (x_t, y_t) in zip(X_train,y_train)]), labeled=True)\n",
    "test_set = nltk_sa.apply_features(tuple([(x_t, y_t) for (x_t, y_t) in zip(X_test,y_test)]), labeled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "trainer = NaiveBayesClassifier.train\n",
    "\n",
    "classifier = nltk_sa.train(trainer, training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.6568689906517133\n",
      "F-measure [0]: 0.4656838996931737\n",
      "F-measure [1]: 0.7472913552498064\n",
      "Precision [0]: 0.6775644468313641\n",
      "Precision [1]: 0.6510084976142045\n",
      "Recall [0]: 0.35475027239815826\n",
      "Recall [1]: 0.8769975414874002\n"
     ]
    }
   ],
   "source": [
    "for key,value in sorted(nltk_sa.evaluate(test_set).items()):\n",
    "    print(f'{key}: {value}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd8ee3559d665fee903f84f74f9742602cb00cb47768a52cae0fe6e115d1a823"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('data_science_lab_2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
