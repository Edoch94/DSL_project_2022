{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cupy \n",
    "# import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'development.csv')\n",
    "df_eval = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74999, 5)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_dev.append(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['date'] = df['date'].\\\n",
    "#     astype('string')\\\n",
    "#     .str.split(' ')\\\n",
    "#     .apply(lambda x : ' '.join([x[i] for i in [1,2,3,5]]))\\\n",
    "#     .pipe(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords analysis: sklearn, nltk, stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edo_c\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import download as nltk_download\n",
    "\n",
    "nltk_download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "sklearn_stopwords = list(text.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User manual filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[\n",
    "    (~(df['user'] == 'lost_dog') &\n",
    "    ~(df['user'] == 'webwoke') &\n",
    "    ~(df['user'] == 'tweetpet') &\n",
    "    ~(df['user'].str.contains('tweeteradder')) &\n",
    "    ~(df['user'].str.contains('tweetfollow')) &\n",
    "    ~(df['user'] == 'divxdownloads')) |\n",
    "    df['sentiment'].isna()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74999, 6)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['sentiment'].isna()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_token'] = df['text'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '@words', '&words', 'http:words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_noAt'] = df['text_token'].apply(lambda x : [i for i in x if not(i.startswith('@'))])\\\n",
    "    .apply(lambda x : [i for i in x if not(i.startswith('&'))])\\\n",
    "    .apply(lambda x : [i for i in x if not(i.startswith('http:'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '&amp', '&quot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_noAmpQuot'] = df['text_noAt']\\\n",
    "    .apply(lambda x : [i for i in x if '&amp' not in i])\\\n",
    "    .apply(lambda x : [i for i in x if '&quot' not in i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of repeated letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ula', 'bubu']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['ulaaa','bubu']\n",
    "import re\n",
    "[re.sub(r'(.)\\1+', r'\\1', word) for word in ['ulaaa','bubu']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "df['text_duplLetters'] = df['text_noAmpQuot']\\\n",
    "    .apply(lambda x : [re.sub(r'(.)\\1+', r'\\1', word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove puntuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string as py_string\n",
    "# df['text_noPunct'] = df['text_noAmpQuot']\\\n",
    "df['text_noPunct'] = df['text_duplLetters']\\\n",
    "    .apply(lambda x : [i.translate(str.maketrans('', '', py_string.punctuation)) for i in x])\\\n",
    "    .apply(lambda x : [i for i in x if i != ''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "# nltk_download('wordnet')\n",
    "# nltk_download('omw-1.4')\n",
    "# nltk_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk_stemmer = PorterStemmer()\n",
    "# nltk_stemmer = LancasterStemmer()\n",
    "# nltk_stemmer = SnowballStemmer('english') \n",
    "\n",
    "df['text_stemmed'] = df['text_noPunct'].apply(lambda x : [nltk_stemmer.stem(word) for word in x])\n",
    "# df['text_stemmed'] = df['text_noPunct'].apply(lambda x : [nltk_lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "df['text_neg'] = df['text_noPunct'].apply(lambda x : mark_negation(x))\n",
    "df['text_stemmed_neg'] = df['text_stemmed'].apply(lambda x : mark_negation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "stop_words_stopwords = get_stop_words('english')\n",
    "\n",
    "considered_stopwords = stop_words_stopwords + [f'{word}_neg' for word in stop_words_stopwords]\n",
    "\n",
    "df['text_noStopwords'] = df['text_neg'].apply(lambda x : [i for i in x if i not in considered_stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 4.266666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "linearSVC_params = {\n",
    "    'penalty' : ['l1','l2'], #['l1','l2']\n",
    "    'dual' : [False],\n",
    "    'tol' : [1e-2,1e-3],\n",
    "    'fit_intercept' : [False],\n",
    "    'class_weight' : ['balanced'],\n",
    "    'max_iter' : [50,100],\n",
    "    'random_state' : [42],\n",
    "    'C' : [1,10]\n",
    "}\n",
    "\n",
    "TfidfVectorizer_params = {\n",
    "    'stop_words' : [None],\n",
    "    'ngram_range' : [(1,3)],\n",
    "    'max_features' : [None],\n",
    "    'max_df' : [1.0,0.001],\n",
    "    'min_df' : [1,0.000001],\n",
    "    'binary' : [True,False],\n",
    "    'norm' : ['l1','l2'],\n",
    "    'use_idf' : [True],\n",
    "    'smooth_idf' : [True,False],\n",
    "    'sublinear_tf' : [False]\n",
    "}\n",
    "\n",
    "\n",
    "number_different_configurations = len(list(ParameterGrid(TfidfVectorizer_params)))*len(list(ParameterGrid(linearSVC_params)))\n",
    "\n",
    "print(number_different_configurations, number_different_configurations*30/60/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tfidf__stop_words': [None], 'tfidf__ngram_range': [(1, 3)], 'tfidf__max_features': [None], 'tfidf__max_df': [1.0, 0.001], 'tfidf__min_df': [1, 1e-06], 'tfidf__binary': [True, False], 'tfidf__norm': ['l1', 'l2'], 'tfidf__use_idf': [True], 'tfidf__smooth_idf': [True, False], 'tfidf__sublinear_tf': [False], 'linearSVC__penalty': ['l1', 'l2'], 'linearSVC__dual': [False], 'linearSVC__tol': [0.01, 0.001], 'linearSVC__fit_intercept': [False], 'linearSVC__class_weight': ['balanced'], 'linearSVC__max_iter': [50, 100], 'linearSVC__random_state': [42], 'linearSVC__C': [1, 10]}\n",
      "Fitting 3 folds for each of 512 candidates, totalling 1536 fits\n",
      "0.8007485288333881\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.77      0.77      9395\n",
      "         1.0       0.83      0.83      0.83     12900\n",
      "\n",
      "    accuracy                           0.81     22295\n",
      "   macro avg       0.80      0.80      0.80     22295\n",
      "weighted avg       0.81      0.81      0.81     22295\n",
      "\n",
      "[[ 7255  2140]\n",
      " [ 2196 10704]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "df_final = df\n",
    "# df_final['text_final'] = df_final['text_noPunct'].apply(lambda x : ' '.join(x))\n",
    "df_final['text_final'] = df_final['text_neg'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "X_train_valid = df_final.loc[mask_train_test,:]['text_final'].values\n",
    "y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "X_test = df_final.loc[~mask_train_test,:]['text_final'].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, \n",
    "    y_train_valid, \n",
    "    shuffle=True, \n",
    "    # stratify=y_train_valid, \n",
    "    train_size=0.9, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "vectorizer = 'tfidf'\n",
    "model = 'linearSVC'\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (vectorizer, TfidfVectorizer()),\n",
    "    (model, LinearSVC())\n",
    "])\n",
    "\n",
    "def params_for_GridSearchCV(params_IN,step_name):\n",
    "    return {f'{step_name}__{key}':value for (key,value) in params_IN.items()}\n",
    "\n",
    "params_all = params_for_GridSearchCV(TfidfVectorizer_params,vectorizer)\n",
    "params_all.update(params_for_GridSearchCV(linearSVC_params,model))\n",
    "print(params_all)\n",
    "\n",
    "gscv = GridSearchCV(pipe, params_all, cv = 3, verbose = 4, scoring='f1_macro', n_jobs=6)\n",
    "\n",
    "gscv.fit(X_train, y_train)\n",
    "\n",
    "f1 = f1_score(y_valid, gscv.predict(X_valid),average='macro')\n",
    "report = classification_report(y_valid, gscv.predict(X_valid))\n",
    "confusion = confusion_matrix(y_valid, gscv.predict(X_valid))\n",
    "\n",
    "print(f1)\n",
    "print(report)\n",
    "print(confusion)\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "results_name = Path.cwd()/'gscv_results'/'linearSVC_final_results.csv'\n",
    "\n",
    "pd.DataFrame(gscv.cv_results_).to_csv(results_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'linearSVC__C': 1, 'linearSVC__class_weight': 'balanced', 'linearSVC__dual': False, 'linearSVC__fit_intercept': False, 'linearSVC__max_iter': 50, 'linearSVC__penalty': 'l2', 'linearSVC__random_state': 42, 'linearSVC__tol': 0.001, 'tfidf__binary': True, 'tfidf__max_df': 1.0, 'tfidf__max_features': None, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 3), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': False, 'tfidf__stop_words': None, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True}\n",
      "{'linearSVC__C': 1, 'linearSVC__class_weight': 'balanced', 'linearSVC__dual': False, 'linearSVC__fit_intercept': False, 'linearSVC__max_iter': 50, 'linearSVC__penalty': 'l2', 'linearSVC__random_state': 42, 'linearSVC__tol': 0.001, 'tfidf__binary': True, 'tfidf__max_df': 1.0, 'tfidf__max_features': None, 'tfidf__min_df': 1e-06, 'tfidf__ngram_range': (1, 3), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': False, 'tfidf__stop_words': None, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv('gscv_results\\\\linearSVC_final_results2.csv')\n",
    "print(results.loc[results['rank_test_score']==1]['params'].iloc[0])\n",
    "print(results.loc[results['rank_test_score']==1]['params'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best models test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# df_final = df\n",
    "# # df_final['text_final'] = df_final['text_noPunct'].apply(lambda x : ' '.join(x))\n",
    "# df_final['text_final'] = df_final['text_neg'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "# mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "# X_train_valid = df_final.loc[mask_train_test,:]['text_final'].values\n",
    "# y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "# X_test = df_final.loc[~mask_train_test,:]['text_final'].values\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "#     X_train_valid, \n",
    "#     y_train_valid, \n",
    "#     shuffle=True, \n",
    "#     # stratify=y_train_valid, \n",
    "#     train_size=0.9, \n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# vectorizer = TfidfVectorizer(\n",
    "#     binary = True, \n",
    "#     max_df = 1.0, \n",
    "#     min_df = 1, \n",
    "#     max_features = None, \n",
    "#     ngram_range = (1,1), \n",
    "#     norm = 'l1', \n",
    "#     smooth_idf = True, \n",
    "#     stop_words = None, \n",
    "#     sublinear_tf = False, \n",
    "#     use_idf = True\n",
    "# )\n",
    "\n",
    "# model = LinearSVC(\n",
    "#     C = 1, \n",
    "#     class_weight = 'balanced', \n",
    "#     dual = False, \n",
    "#     fit_intercept = False, \n",
    "#     max_iter = 100, \n",
    "#     penalty = 'l1', \n",
    "#     random_state = 42, \n",
    "#     tol = 0.001\n",
    "# )\n",
    "\n",
    "# pipe = Pipeline([\n",
    "#     ('tfidf', vectorizer),\n",
    "#     ('linearSVC', model)\n",
    "# ])\n",
    "\n",
    "# pipe.fit(X_train, y_train)\n",
    "\n",
    "# f1 = f1_score(y_valid, pipe.predict(X_valid),average='macro')\n",
    "# report = classification_report(y_valid, pipe.predict(X_valid))\n",
    "# confusion = confusion_matrix(y_valid, pipe.predict(X_valid))\n",
    "\n",
    "# print(f1)\n",
    "# print(report)\n",
    "# print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(\n",
    "    C = 1, \n",
    "    class_weight = 'balanced', \n",
    "    dual = False, \n",
    "    fit_intercept = False, \n",
    "    max_iter = 50, \n",
    "    penalty = 'l2', \n",
    "    random_state = 42, \n",
    "    tol = 0.001\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    binary = True, \n",
    "    max_df = 1.0, \n",
    "    max_features = None, \n",
    "    min_df = 1, \n",
    "    ngram_range = (1,3), \n",
    "    norm = 'l2', \n",
    "    smooth_idf = False, \n",
    "    stop_words = None, \n",
    "    sublinear_tf = False, \n",
    "    use_idf = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIFIED: norm and ngram_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8019786529108555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.78      0.77      9395\n",
      "         1.0       0.84      0.83      0.83     12900\n",
      "\n",
      "    accuracy                           0.81     22295\n",
      "   macro avg       0.80      0.80      0.80     22295\n",
      "weighted avg       0.81      0.81      0.81     22295\n",
      "\n",
      "[[ 7282  2113]\n",
      " [ 2198 10702]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "df_final = df\n",
    "# df_final['text_final'] = df_final['text_noPunct'].apply(lambda x : ' '.join(x))\n",
    "# df_final['text_final'] = df_final['text_neg'].apply(lambda x : ' '.join(x))\n",
    "df_final['text_final'] = df_final['text_stemmed_neg'].apply(lambda x : ' '.join(x))\n",
    "# df_final['text_final'] = df_final['text_noStopwords'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "X_train_valid = df_final.loc[mask_train_test,:]['text_final'].values\n",
    "y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "X_test = df_final.loc[~mask_train_test,:]['text_final'].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, \n",
    "    y_train_valid, \n",
    "    shuffle=True, \n",
    "    # stratify=y_train_valid, \n",
    "    train_size=0.9, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = LinearSVC(\n",
    "    C = 1, \n",
    "    class_weight = 'balanced', \n",
    "    dual = False, \n",
    "    fit_intercept = False, \n",
    "    max_iter = 50, \n",
    "    penalty = 'l2', \n",
    "    random_state = 42, \n",
    "    tol = 0.001\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    binary = True, \n",
    "    max_df = 1.0, \n",
    "    max_features = None, \n",
    "    min_df = 1, \n",
    "    ngram_range = (1,3), \n",
    "    norm = 'l2', \n",
    "    smooth_idf = False, \n",
    "    stop_words = None, \n",
    "    sublinear_tf = False, \n",
    "    use_idf = True\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', vectorizer),\n",
    "    ('linearSVC', model)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "f1 = f1_score(y_valid, pipe.predict(X_valid),average='macro')\n",
    "report = classification_report(y_valid, pipe.predict(X_valid))\n",
    "confusion = confusion_matrix(y_valid, pipe.predict(X_valid))\n",
    "\n",
    "print(f1)\n",
    "print(report)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(\n",
    "    C = 1, \n",
    "    class_weight = 'balanced', \n",
    "    dual = False, \n",
    "    fit_intercept = False, \n",
    "    max_iter = 50, \n",
    "    penalty = 'l2', \n",
    "    random_state = 42, \n",
    "    tol = 0.001\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    binary = True, \n",
    "    max_df = 1.0, \n",
    "    max_features = None, \n",
    "    min_df = 1, \n",
    "    ngram_range = (1,3), \n",
    "    norm = 'l2', \n",
    "    smooth_idf = False, \n",
    "    stop_words = None, \n",
    "    sublinear_tf = False, \n",
    "    use_idf = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ NO stemming: 0.7953890537516292\n",
    "+ Snowball: 0.7994796670641988\n",
    "+ Lancaster: 0.7934227929849921\n",
    "+ Porter: 0.7982780155296589\n",
    "+ Wordnet: 0.7942085265529459"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train_valid, y_train_valid)\n",
    "y_pred = pipe.predict(X_test)\n",
    "pd.DataFrame(y_pred.astype(int),columns=['Predicted']).to_csv(\"output_gram13_80197.csv\",index_label=\"Id\", header=[\"Predicted\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd8ee3559d665fee903f84f74f9742602cb00cb47768a52cae0fe6e115d1a823"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('data_science_lab_2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
