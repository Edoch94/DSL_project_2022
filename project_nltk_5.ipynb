{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy \n",
    "import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'development.csv')\n",
    "df_eval = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74999, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_dev.append(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['date'] = df['date'].\\\n",
    "#     astype('string')\\\n",
    "#     .str.split(' ')\\\n",
    "#     .apply(lambda x : ' '.join([x[i] for i in [1,2,3,5]]))\\\n",
    "#     .pipe(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords analysis: sklearn, nltk, stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/edoch/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import download as nltk_download\n",
    "\n",
    "nltk_download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "sklearn_stopwords = list(text.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stop_words import get_stop_words\n",
    "\n",
    "# stop_words_stopwords = get_stop_words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should try to use both all stopwords, and sklearn and nltk stopwords singularly (and also no stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_list_gen(source_list = [sklearn_stopwords,nltk_stopwords], generate_neg = True):\n",
    "    stopwords_all_list = set()\n",
    "    for source in source_list:\n",
    "        stopwords_all_list = stopwords_all_list.union(set(source))\n",
    "        if generate_neg:\n",
    "            stopwords_all_list = stopwords_all_list.union(set([f'{i+\"_neg\"}' for i in source]))\n",
    "    return stopwords_all_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_token'] = df['text'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '@words', '&words', 'http:words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_noAt'] = df['text_token'].apply(lambda x : [i for i in x if not(i.startswith('@'))])\\\n",
    "    .apply(lambda x : [i for i in x if not(i.startswith('&'))])\\\n",
    "    .apply(lambda x : [i for i in x if not(i.startswith('http:'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '&amp', '&quot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_noAmpQuot'] = df['text_noAt']\\\n",
    "    .apply(lambda x : [i for i in x if '&amp' not in i])\\\n",
    "    .apply(lambda x : [i for i in x if '&quot' not in i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of repeated letters (incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'ulaaa'\n",
    "\n",
    "def remove_repeated_letters(word):\n",
    "    for letter in word:\n",
    "        print(letter)\n",
    "\n",
    "# nltk_stemmer.stem(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove puntuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string as py_string\n",
    "df['text_noPunct'] = df['text_noAmpQuot']\\\n",
    "    .apply(lambda x : [i.translate(str.maketrans('', '', py_string.punctuation)) for i in x])\\\n",
    "    .apply(lambda x : [i for i in x if i != ''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "df['text_neg'] = df['text_noPunct'].apply(lambda x : mark_negation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considered_stopwords = stop_words_stopwords + [f'{word}_neg' for word in stop_words_stopwords]\n",
    "\n",
    "# df['text_noStopwords'] = df['text_neg'].apply(lambda x : [i for i in x if i not in considered_stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "# nltk_download('wordnet')\n",
    "# nltk_download('omw-1.4')\n",
    "\n",
    "# nltk_stemmer = PorterStemmer()\n",
    "# # nltk_stemmer = LancasterStemmer()\n",
    "# # nltk_stemmer = SnowballStemmer('english')\n",
    "# nltk_lemmatizer = WordNetLemmatizer()\n",
    "# # from nltk.corpus import wordnet\n",
    "\n",
    "# df['text_stemmed'] = df['text_noPunct'].apply(lambda x : [nltk_stemmer.stem(word) for word in x])\n",
    "# df['text_stemmed'] = df['text_noPunct'].apply(lambda x : [nltk_lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User manual filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[\n",
    "    (~(df['user'] == 'lost_dog') &\n",
    "    ~(df['user'] == 'webwoke') &\n",
    "    ~(df['user'] == 'tweetpet') &\n",
    "    ~(df['user'].str.contains('tweeteradder')) &\n",
    "    ~(df['user'].str.contains('tweetfollow')) &\n",
    "    ~(df['user'] == 'divxdownloads')) |\n",
    "    df['sentiment'].isna()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74999, 11)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['sentiment'].isna()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "linearSVC_params = {\n",
    "    'penalty' : ['l1'], #['l1','l2']\n",
    "    'dual' : [False],\n",
    "    'tol' : [1e-3,1e-4],\n",
    "    'fit_intercept' : [False],\n",
    "    'class_weight' : ['balanced'],\n",
    "    'max_iter' : [100,300],\n",
    "    'random_state' : [42],\n",
    "    'C' : [1,10,50]\n",
    "}\n",
    "\n",
    "TfidfVectorizer_params = {\n",
    "    'stop_words' : [None],\n",
    "    'ngram_range' : [(1,1)],\n",
    "    'max_features' : [None, 15000, 30000],\n",
    "    'max_df' : [1.0,0.1,0.005],\n",
    "    'min_df' : [1,0.0001,0.00001],\n",
    "    'binary' : [True,False],\n",
    "    'norm' : ['l1','l2'],\n",
    "    'use_idf' : [True],\n",
    "    'smooth_idf' : [True,False],\n",
    "    'sublinear_tf' : [False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "0.0018055555555555557\n"
     ]
    }
   ],
   "source": [
    "print(len(list(ParameterGrid(TfidfVectorizer_params)))*len(list(ParameterGrid(linearSVC_params))))\n",
    "\n",
    "print(len(list(ParameterGrid(TfidfVectorizer_params)))*len(list(ParameterGrid(linearSVC_params)))*13/60/60/24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations:\t12\n",
      "1 {'max_features': None, 'ngram_range': (1, 1), 'stop_words': None} {'dual': False, 'penalty': 'l1'}\n",
      "2 {'max_features': 10000, 'ngram_range': (1, 1), 'stop_words': None} {'dual': False, 'penalty': 'l1'}\n",
      "3 {'max_features': 11000, 'ngram_range': (1, 1), 'stop_words': None} {'dual': False, 'penalty': 'l1'}\n",
      "4 {'max_features': 12000, 'ngram_range': (1, 1), 'stop_words': None} {'dual': False, 'penalty': 'l1'}\n",
      "5 {'max_features': 13000, 'ngram_range': (1, 1), 'stop_words': None} {'dual': False, 'penalty': 'l1'}\n",
      "6 {'max_features': 14000, 'ngram_range': (1, 1), 'stop_words': None} {'dual': False, 'penalty': 'l1'}\n",
      "7 {'max_features': 15000, 'ngram_range': (1, 1), 'stop_words': None} {'dual': False, 'penalty': 'l1'}\n",
      "8 {'max_features': 16000, 'ngram_range': (1, 1), 'stop_words': None} {'dual': False, 'penalty': 'l1'}\n",
      "9 {'max_features': 17000, 'ngram_range': (1, 1), 'stop_words': None} {'dual': False, 'penalty': 'l1'}\n",
      "10 {'max_features': 18000, 'ngram_range': (1, 1), 'stop_words': None} {'dual': False, 'penalty': 'l1'}\n",
      "11 {'max_features': 19000, 'ngram_range': (1, 1), 'stop_words': None} {'dual': False, 'penalty': 'l1'}\n",
      "12 {'max_features': 20000, 'ngram_range': (1, 1), 'stop_words': None} {'dual': False, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "logs_path = Path.cwd()/'logs'\n",
    "results_path = Path.cwd()/'results'\n",
    "logs_path.mkdir(exist_ok=True)\n",
    "results_path.mkdir(exist_ok=True)\n",
    "log_file_name = logs_path/f'log_out_{datetime.now()}.txt'\n",
    "results_file_name = results_path/f'results_out_{datetime.now()}.txt'\n",
    "with open(log_file_name, 'w') as file_log:\n",
    "    file_log.write('File output\\n\\n')\n",
    "with open(results_file_name, 'w') as file_result:\n",
    "    file_result.write('configuration_number,vectorizer_param,model_param,f1\\n')\n",
    "\n",
    "df_final = df\n",
    "# df_final['text_final'] = df_final['text_noPunct'].apply(lambda x : ' '.join(x))\n",
    "df_final['text_final'] = df_final['text_neg'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "X_train_valid = df_final.loc[mask_train_test,:]['text_final'].values\n",
    "y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "X_test = df_final.loc[~mask_train_test,:]['text_final'].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, \n",
    "    y_train_valid, \n",
    "    shuffle=True, \n",
    "    # stratify=y_train_valid, \n",
    "    train_size=0.8, \n",
    "    random_state=50\n",
    ")\n",
    "\n",
    "configuration_number = 0\n",
    "print(f'Number of combinations:\\t{len(list(ParameterGrid(TfidfVectorizer_params)))*len(list(ParameterGrid(linearSVC_params)))}')\n",
    "for vectorizer_param in ParameterGrid(TfidfVectorizer_params):\n",
    "    for model_param in ParameterGrid(linearSVC_params):\n",
    "        configuration_number += 1\n",
    "        print(configuration_number, vectorizer_param, model_param)\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(**vectorizer_param)),\n",
    "            ('linearSVC', LinearSVC(**model_param))\n",
    "        ])\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        f1 = f1_score(y_valid, pipe.predict(X_valid),average='macro')\n",
    "        report = classification_report(y_valid, pipe.predict(X_valid))\n",
    "        # confusion = confusion_matrix(y_valid, pipe.predict(X_valid))\n",
    "\n",
    "        with open(log_file_name, 'a') as file_log:\n",
    "            file_log.write(f'configuration_number: {configuration_number}\\n')\n",
    "            file_log.write(f'vectorizer_param: \\t{vectorizer_param}\\n')\n",
    "            file_log.write(f'model_param: \\t\\t{model_param}\\n')\n",
    "            file_log.write(f'f1_score:\\t\\t\\t{f1}\\n\\n')\n",
    "            file_log.write(f'{report}\\n\\n')\n",
    "            file_log.write(f'{\"*\"*150}\\n\\n')\n",
    "        with open(results_file_name, 'a') as file_result:\n",
    "            file_result.write(f'{configuration_number},{vectorizer_param},{model_param},{f1}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tfidf__stop_words': [None], 'tfidf__ngram_range': [(1, 1)], 'tfidf__max_features': [None, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000], 'linearSVC__penalty': ['l1'], 'linearSVC__dual': [False]}\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV 1/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=None, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.774 total time=   3.5s\n",
      "[CV 2/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=None, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.775 total time=   3.5s\n",
      "[CV 3/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=None, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.774 total time=   3.6s\n",
      "[CV 1/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=10000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.773 total time=   3.2s\n",
      "[CV 2/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=10000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.773 total time=   3.1s\n",
      "[CV 3/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=10000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.772 total time=   3.0s\n",
      "[CV 1/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=11000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.773 total time=   3.1s\n",
      "[CV 2/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=11000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.773 total time=   3.2s\n",
      "[CV 3/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=11000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.772 total time=   3.3s\n",
      "[CV 1/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=12000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.773 total time=   3.2s\n",
      "[CV 2/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=12000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.774 total time=   3.3s\n",
      "[CV 3/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=12000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.773 total time=   3.3s\n",
      "[CV 1/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=13000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.774 total time=   3.0s\n",
      "[CV 2/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=13000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.774 total time=   3.1s\n",
      "[CV 3/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=13000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.772 total time=   3.2s\n",
      "[CV 1/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=14000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.773 total time=   3.0s\n",
      "[CV 2/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=14000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.774 total time=   3.0s\n",
      "[CV 3/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=14000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.772 total time=   3.1s\n",
      "[CV 1/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=15000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.773 total time=   3.1s\n",
      "[CV 2/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=15000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.774 total time=   3.1s\n",
      "[CV 3/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=15000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.772 total time=   2.9s\n",
      "[CV 1/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=16000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.773 total time=   3.1s\n",
      "[CV 2/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=16000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.774 total time=   3.1s\n",
      "[CV 3/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=16000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.772 total time=   3.0s\n",
      "[CV 1/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=17000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.774 total time=   3.1s\n",
      "[CV 2/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=17000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.774 total time=   3.1s\n",
      "[CV 3/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=17000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.772 total time=   3.1s\n",
      "[CV 1/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=18000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.773 total time=   3.1s\n",
      "[CV 2/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=18000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.774 total time=   3.1s\n",
      "[CV 3/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=18000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.772 total time=   3.1s\n",
      "[CV 1/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=19000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.773 total time=   3.0s\n",
      "[CV 2/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=19000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.774 total time=   3.0s\n",
      "[CV 3/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=19000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.771 total time=   2.9s\n",
      "[CV 1/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=20000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.773 total time=   3.1s\n",
      "[CV 2/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=20000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.774 total time=   3.1s\n",
      "[CV 3/3] END linearSVC__dual=False, linearSVC__penalty=l1, tfidf__max_features=20000, tfidf__ngram_range=(1, 1), tfidf__stop_words=None;, score=0.771 total time=   3.0s\n",
      "0.776621327986464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.72      0.74     18783\n",
      "         1.0       0.80      0.83      0.82     25807\n",
      "\n",
      "    accuracy                           0.78     44590\n",
      "   macro avg       0.78      0.77      0.78     44590\n",
      "weighted avg       0.78      0.78      0.78     44590\n",
      "\n",
      "[[13476  5307]\n",
      " [ 4333 21474]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "df_final = df\n",
    "# df_final['text_final'] = df_final['text_noPunct'].apply(lambda x : ' '.join(x))\n",
    "df_final['text_final'] = df_final['text_neg'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "X_train_valid = df_final.loc[mask_train_test,:]['text_final'].values\n",
    "y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "X_test = df_final.loc[~mask_train_test,:]['text_final'].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, \n",
    "    y_train_valid, \n",
    "    shuffle=True, \n",
    "    # stratify=y_train_valid, \n",
    "    train_size=0.8, \n",
    "    random_state=50\n",
    ")\n",
    "\n",
    "vectorizer = 'tfidf'\n",
    "model = 'linearSVC'\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (vectorizer, TfidfVectorizer()),\n",
    "    (model, LinearSVC())\n",
    "])\n",
    "\n",
    "def params_for_GridSearchCV(params_IN,step_name):\n",
    "    return {f'{step_name}__{key}':value for (key,value) in params_IN.items()}\n",
    "\n",
    "params_all = params_for_GridSearchCV(TfidfVectorizer_params,vectorizer)\n",
    "params_all.update(params_for_GridSearchCV(linearSVC_params,model))\n",
    "print(params_all)\n",
    "\n",
    "gscv = GridSearchCV(pipe, params_all, cv = 3, verbose = 4, scoring='f1_macro')\n",
    "\n",
    "gscv.fit(X_train, y_train)\n",
    "\n",
    "f1 = f1_score(y_valid, gscv.predict(X_valid),average='macro')\n",
    "report = classification_report(y_valid, gscv.predict(X_valid))\n",
    "confusion = confusion_matrix(y_valid, gscv.predict(X_valid))\n",
    "\n",
    "print(f1)\n",
    "print(report)\n",
    "print(confusion)\n",
    "# configuration_number = 0\n",
    "# print(f'Number of combinations:\\t{len(list(ParameterGrid(TfidfVectorizer_params)))*len(list(ParameterGrid(linearSVC_params)))}')\n",
    "# for vectorizer_param in ParameterGrid(TfidfVectorizer_params):\n",
    "#     for model_param in ParameterGrid(linearSVC_params):\n",
    "#         configuration_number += 1\n",
    "#         print(configuration_number, vectorizer_param, model_param)\n",
    "\n",
    "#         pipe = Pipeline([\n",
    "#             ('tfidf', TfidfVectorizer(**vectorizer_param)),\n",
    "#             ('linearSVC', LinearSVC(**model_param))\n",
    "#         ])\n",
    "\n",
    "#         pipe.fit(X_train, y_train)\n",
    "\n",
    "#         f1 = f1_score(y_valid, pipe.predict(X_valid),average='macro')\n",
    "#         report = classification_report(y_valid, pipe.predict(X_valid))\n",
    "#         # confusion = confusion_matrix(y_valid, pipe.predict(X_valid))\n",
    "\n",
    "#         with open(log_file_name, 'a') as file_log:\n",
    "#             file_log.write(f'configuration_number: {configuration_number}\\n')\n",
    "#             file_log.write(f'vectorizer_param: \\t{vectorizer_param}\\n')\n",
    "#             file_log.write(f'model_param: \\t\\t{model_param}\\n')\n",
    "#             file_log.write(f'f1_score:\\t\\t\\t{f1}\\n\\n')\n",
    "#             file_log.write(f'{report}\\n\\n')\n",
    "#             file_log.write(f'{\"*\"*150}\\n\\n')\n",
    "#         with open(results_file_name, 'a') as file_result:\n",
    "#             file_result.write(f'{configuration_number},{vectorizer_param},{model_param},{f1}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([3.0788544 , 2.61653725, 2.67823307, 2.76365209, 2.62333131,\n",
       "        2.55601279, 2.55158011, 2.60176015, 2.61510547, 2.60183247,\n",
       "        2.48423521, 2.59180554]),\n",
       " 'std_fit_time': array([0.05695392, 0.07970918, 0.05352276, 0.04373008, 0.05687926,\n",
       "        0.0360079 , 0.0691899 , 0.02841061, 0.01559789, 0.01321297,\n",
       "        0.01071902, 0.07057189]),\n",
       " 'mean_score_time': array([0.49050371, 0.49622178, 0.4972016 , 0.50624243, 0.49720343,\n",
       "        0.47108515, 0.46942592, 0.47684622, 0.47558371, 0.47171442,\n",
       "        0.47391669, 0.47064034]),\n",
       " 'std_score_time': array([0.00104816, 0.00214462, 0.00671898, 0.00108378, 0.00478194,\n",
       "        0.00092891, 0.00280431, 0.00194455, 0.00256603, 0.00093681,\n",
       "        0.00098326, 0.00383064]),\n",
       " 'param_linearSVC__dual': masked_array(data=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_linearSVC__penalty': masked_array(data=['l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1', 'l1',\n",
       "                    'l1', 'l1', 'l1'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tfidf__max_features': masked_array(data=[None, 10000, 11000, 12000, 13000, 14000, 15000, 16000,\n",
       "                    17000, 18000, 19000, 20000],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tfidf__ngram_range': masked_array(data=[(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1),\n",
       "                    (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tfidf__stop_words': masked_array(data=[None, None, None, None, None, None, None, None, None,\n",
       "                    None, None, None],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'linearSVC__dual': False,\n",
       "   'linearSVC__penalty': 'l1',\n",
       "   'tfidf__max_features': None,\n",
       "   'tfidf__ngram_range': (1, 1),\n",
       "   'tfidf__stop_words': None},\n",
       "  {'linearSVC__dual': False,\n",
       "   'linearSVC__penalty': 'l1',\n",
       "   'tfidf__max_features': 10000,\n",
       "   'tfidf__ngram_range': (1, 1),\n",
       "   'tfidf__stop_words': None},\n",
       "  {'linearSVC__dual': False,\n",
       "   'linearSVC__penalty': 'l1',\n",
       "   'tfidf__max_features': 11000,\n",
       "   'tfidf__ngram_range': (1, 1),\n",
       "   'tfidf__stop_words': None},\n",
       "  {'linearSVC__dual': False,\n",
       "   'linearSVC__penalty': 'l1',\n",
       "   'tfidf__max_features': 12000,\n",
       "   'tfidf__ngram_range': (1, 1),\n",
       "   'tfidf__stop_words': None},\n",
       "  {'linearSVC__dual': False,\n",
       "   'linearSVC__penalty': 'l1',\n",
       "   'tfidf__max_features': 13000,\n",
       "   'tfidf__ngram_range': (1, 1),\n",
       "   'tfidf__stop_words': None},\n",
       "  {'linearSVC__dual': False,\n",
       "   'linearSVC__penalty': 'l1',\n",
       "   'tfidf__max_features': 14000,\n",
       "   'tfidf__ngram_range': (1, 1),\n",
       "   'tfidf__stop_words': None},\n",
       "  {'linearSVC__dual': False,\n",
       "   'linearSVC__penalty': 'l1',\n",
       "   'tfidf__max_features': 15000,\n",
       "   'tfidf__ngram_range': (1, 1),\n",
       "   'tfidf__stop_words': None},\n",
       "  {'linearSVC__dual': False,\n",
       "   'linearSVC__penalty': 'l1',\n",
       "   'tfidf__max_features': 16000,\n",
       "   'tfidf__ngram_range': (1, 1),\n",
       "   'tfidf__stop_words': None},\n",
       "  {'linearSVC__dual': False,\n",
       "   'linearSVC__penalty': 'l1',\n",
       "   'tfidf__max_features': 17000,\n",
       "   'tfidf__ngram_range': (1, 1),\n",
       "   'tfidf__stop_words': None},\n",
       "  {'linearSVC__dual': False,\n",
       "   'linearSVC__penalty': 'l1',\n",
       "   'tfidf__max_features': 18000,\n",
       "   'tfidf__ngram_range': (1, 1),\n",
       "   'tfidf__stop_words': None},\n",
       "  {'linearSVC__dual': False,\n",
       "   'linearSVC__penalty': 'l1',\n",
       "   'tfidf__max_features': 19000,\n",
       "   'tfidf__ngram_range': (1, 1),\n",
       "   'tfidf__stop_words': None},\n",
       "  {'linearSVC__dual': False,\n",
       "   'linearSVC__penalty': 'l1',\n",
       "   'tfidf__max_features': 20000,\n",
       "   'tfidf__ngram_range': (1, 1),\n",
       "   'tfidf__stop_words': None}],\n",
       " 'split0_test_score': array([0.77355375, 0.77331472, 0.77300513, 0.77340593, 0.77351203,\n",
       "        0.77347738, 0.77336721, 0.77338175, 0.77355586, 0.77305823,\n",
       "        0.77291895, 0.77313808]),\n",
       " 'split1_test_score': array([0.77493873, 0.77344422, 0.77314804, 0.77385636, 0.77350941,\n",
       "        0.77397099, 0.77404496, 0.7740451 , 0.77408573, 0.77377958,\n",
       "        0.77395657, 0.77412198]),\n",
       " 'split2_test_score': array([0.77366386, 0.77241773, 0.7722408 , 0.7726169 , 0.77214883,\n",
       "        0.77235098, 0.77230573, 0.77206176, 0.77239792, 0.77177248,\n",
       "        0.77149554, 0.77135436]),\n",
       " 'mean_test_score': array([0.77405211, 0.77305889, 0.77279799, 0.77329306, 0.77305675,\n",
       "        0.77326645, 0.7732393 , 0.77316287, 0.7733465 , 0.7728701 ,\n",
       "        0.77279035, 0.77287147]),\n",
       " 'std_test_score': array([0.00062854, 0.00045644, 0.00039829, 0.00051226, 0.000642  ,\n",
       "        0.00067798, 0.00071577, 0.00082436, 0.00070477, 0.00083012,\n",
       "        0.00100882, 0.0011455 ]),\n",
       " 'rank_test_score': array([ 1,  7, 11,  3,  8,  4,  5,  6,  2, 10, 12,  9], dtype=int32)}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cuML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import ParameterGrid\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from cuml.feature_extraction.text import TfidfVectorizer as cuml_TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from cuml.svm import LinearSVC as cuml_LinearSVC\n",
    "# from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# from datetime import datetime\n",
    "# from pathlib import Path\n",
    "\n",
    "# logs_path = Path.cwd()/'logs'\n",
    "# results_path = Path.cwd()/'results'\n",
    "# logs_path.mkdir(exist_ok=True)\n",
    "# results_path.mkdir(exist_ok=True)\n",
    "# log_file_name = logs_path/f'log_out_{datetime.now()}.txt'\n",
    "# results_file_name = results_path/f'results_out_{datetime.now()}.txt'\n",
    "# with open(log_file_name, 'w') as file_log:\n",
    "#     file_log.write('File output\\n\\n')\n",
    "# with open(results_file_name, 'w') as file_result:\n",
    "#     file_result.write('configuration_number,vectorizer_param,model_param,f1\\n')\n",
    "\n",
    "# df_final = df\n",
    "# df_final['text_final'] = df_final['text_noPunct'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "# mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "# X_train_valid = df_final.loc[mask_train_test,:]['text_final'].values\n",
    "# y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "# X_test = df_final.loc[~mask_train_test,:]['text_final'].values\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "#     X_train_valid, \n",
    "#     y_train_valid, \n",
    "#     shuffle=True, \n",
    "#     # stratify=y_train_valid, \n",
    "#     train_size=0.8, \n",
    "#     random_state=50\n",
    "# )\n",
    "\n",
    "# configuration_number = 0\n",
    "# print(f'Number of combinations:\\t{len(list(ParameterGrid(TfidfVectorizer_params)))*len(list(ParameterGrid(linearSVC_params)))}')\n",
    "# for vectorizer_param in ParameterGrid(TfidfVectorizer_params):\n",
    "#     for model_param in ParameterGrid(linearSVC_params):\n",
    "#         configuration_number += 1\n",
    "#         print(configuration_number, vectorizer_param, model_param)\n",
    "\n",
    "#         # pipe = Pipeline([\n",
    "#         #     ('tfidf', TfidfVectorizer(**vectorizer_param)),\n",
    "#         #     ('linearSVC', LinearSVC(**model_param))\n",
    "#         # ])\n",
    "\n",
    "#         tfidf = TfidfVectorizer(**vectorizer_param)\n",
    "#         linearSVC = LinearSVC(**model_param)\n",
    "\n",
    "\n",
    "#         cuml_X_train = cudf.DataFrame(cudf.Series(X_train, name='text'))\n",
    "#         cuml_X_valid = cudf.DataFrame(cudf.Series(X_valid, name='text'))\n",
    "#         cuml_y_train = cupy.asarray(y_train)\n",
    "\n",
    "#         X_train = tfidf.fit_transform(X_train)\n",
    "#         X_valid = tfidf.transform(X_valid)\n",
    "\n",
    "#         linearSVC.fit(X_train,y_train)\n",
    "\n",
    "#         y_pred = linearSVC.predict(X_valid)\n",
    "\n",
    "#         y_pred = cupy.asnumpy(y_pred)\n",
    "\n",
    "#         f1 = f1_score(y_valid, y_pred,average='macro')\n",
    "#         report = classification_report(y_valid, y_pred)\n",
    "#         # confusion = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "#         with open(log_file_name, 'a') as file_log:\n",
    "#             file_log.write(f'configuration_number: {configuration_number}\\n')\n",
    "#             file_log.write(f'vectorizer_param: \\t{vectorizer_param}\\n')\n",
    "#             file_log.write(f'model_param: \\t\\t{model_param}\\n')\n",
    "#             file_log.write(f'f1_score:\\t\\t\\t{f1}\\n\\n')\n",
    "#             file_log.write(f'{report}\\n\\n')\n",
    "#             file_log.write(f'{\"*\"*150}\\n\\n')\n",
    "#         with open(results_file_name, 'a') as file_result:\n",
    "#             file_result.write(f'{configuration_number},{vectorizer_param},{model_param},{f1}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd8ee3559d665fee903f84f74f9742602cb00cb47768a52cae0fe6e115d1a823"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('data_science_lab_2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
