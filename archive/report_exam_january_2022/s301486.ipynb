{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'development.csv')\n",
    "df_eval = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'evaluation.csv')\n",
    "df = df_dev.append(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '&amp\\;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_noAmpEnt(df, field_IN, field_OUT):\n",
    "    df[field_OUT] = df[field_IN].str.replace('&amp;','',case=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '&quot\\;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_noQuotEnt(df, field_IN, field_OUT):\n",
    "    df[field_OUT] = df[field_IN].str.replace('&quot;','',case=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '@words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_noAt(df, field_IN, field_OUT):\n",
    "    df[field_OUT] = df[field_IN]\\\n",
    "        .str.split()\\\n",
    "        .apply(lambda x : [i for i in x if not(i.startswith('@'))])\\\n",
    "        .str.join(' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of 'http:words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_noHttp(df, field_IN, field_OUT):\n",
    "    df[field_OUT] = df[field_IN]\\\n",
    "        .str.split()\\\n",
    "        .apply(lambda x : [i for i in x if not(i.startswith('http'))])\\\n",
    "        .str.join(' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of repeated letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def text_noDuplLetters(df, field_IN, field_OUT):\n",
    "    df[field_OUT] = df[field_IN]\\\n",
    "        .str.split()\\\n",
    "        .apply(lambda x : [re.sub(r'(.)\\1+', r'\\1', word) for word in x])\\\n",
    "        .str.join(' ')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of puntuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string as py_string\n",
    "def text_noPunctuation(df, field_IN, field_OUT):\n",
    "    df[field_OUT] = df[field_IN].str.translate(str.maketrans('', '', py_string.punctuation))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "def text_stem(df, field_IN, field_OUT, stemmer):\n",
    "    if stemmer == SnowballStemmer:\n",
    "        stemmer_to_use = SnowballStemmer('english')\n",
    "    else: \n",
    "        stemmer_to_use = stemmer()\n",
    "    df[field_OUT] = df[field_IN]\\\n",
    "        .str.split()\\\n",
    "        .apply(lambda x : [stemmer_to_use.stem(word) for word in x])\\\n",
    "        .str.join(' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word negations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.util import mark_negation\n",
    "def text_neg(df, field_IN, field_OUT):\n",
    "    df[field_OUT] = df[field_IN]\\\n",
    "        .str.split()\\\n",
    "        .apply(lambda x : mark_negation(x))\\\n",
    "        .str.join(' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User suspiciousness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extreme_sentiment: (1.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "average_user_sentiment = df_dev.groupby('user')['sentiment'].mean()\n",
    "average_user_sentiment = pd.Series(pd.DataFrame(average_user_sentiment)['sentiment'])\n",
    "extreme_sentiment = np.abs(0.5-average_user_sentiment)\n",
    "extreme_sentiment = pd.Series(minmax_scale(extreme_sentiment.values),index=extreme_sentiment.index)\n",
    "print(f'extreme_sentiment: {extreme_sentiment.max(), extreme_sentiment.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_similarity: (1.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvect = CountVectorizer()\n",
    "df_test = df.copy(deep=True)\n",
    "X_count = countvect.fit_transform(df_test['text'])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "user_similarity = {}\n",
    "for user in df_test['user'].unique():\n",
    "    X_count_user = X_count[df_test['user'] == user,:]\n",
    "    X_similarity_user = cosine_similarity(X_count_user)\n",
    "    np.fill_diagonal(X_similarity_user,np.nan)\n",
    "    user_similarity[user] = np.nanmean(X_similarity_user)\n",
    "\n",
    "user_similarity = pd.Series(user_similarity)\n",
    "user_similarity = pd.Series(minmax_scale(user_similarity.values),index=user_similarity.index)\n",
    "print(f'user_similarity: {user_similarity.max(), user_similarity.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_suspiciousness: (1.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "user_suspiciousness = (extreme_sentiment*user_similarity)\n",
    "print(f'user_suspiciousness: {user_suspiciousness.max(), user_suspiciousness.min()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "multinomialNB_params = {\n",
    "    'alpha' : [.01,.02,.05,.1,.2,.5,1.0,2.0]\n",
    "}\n",
    "\n",
    "linearSVC_params = {\n",
    "    'penalty' : ['l1','l2'],\n",
    "    'dual' : [False],\n",
    "    'tol' : [1e-2,1e-3],\n",
    "    'fit_intercept' : [False],\n",
    "    'class_weight' : ['balanced'],\n",
    "    'max_iter' : [100],\n",
    "    'random_state' : [42],\n",
    "    'C' : [1,10]\n",
    "}\n",
    "\n",
    "TfidfVectorizer_params = {\n",
    "    'stop_words' : [None,'english'],\n",
    "    'ngram_range' : [(1,1),(1,2),(1,3)],\n",
    "    'max_features' : [None,20000],\n",
    "    'max_df' : [1.0],\n",
    "    'min_df' : [1],\n",
    "    'binary' : [True,False],\n",
    "    'norm' : ['l1','l2'],\n",
    "    'use_idf' : [True],\n",
    "    'smooth_idf' : [True,False],\n",
    "    'sublinear_tf' : [False]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of a function that helps to generate the parameters to use in a pipeline in a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_for_GridSearchCV(params_IN,step_name):\n",
    "    return {f'{step_name}__{key}':value for (key,value) in params_IN.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "df_final = df.copy(deep=True)\n",
    "\n",
    "# ---- REMOVAL OF RECORDS WITH DUPLICATE IDS ---- #\n",
    "duplicated_ids = df_dev['ids'].value_counts()\n",
    "duplicated_ids = duplicated_ids[duplicated_ids > 1] \n",
    "df_final = df_final.loc[\n",
    "    ~(df_final['ids'].isin(list(duplicated_ids.index))) | \n",
    "    df_final['sentiment'].isna()\n",
    "]\n",
    "\n",
    "# ---- REMOVAL OF BOTS ---- #\n",
    "df_final = df_final.loc[\n",
    "    ~(df_final['user'].isin(user_suspiciousness[user_suspiciousness>.9].index)) | \n",
    "    df_final['sentiment'].isna()\n",
    "]\n",
    "\n",
    "# ---- OTHER PREPROCESSING STEPS ---- #\n",
    "df_final = df_final\\\n",
    "    .pipe(text_noAmpEnt, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noQuotEnt, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noAt, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noHttp, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noDuplLetters, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noPunctuation, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_stem, field_IN='text', field_OUT='text', stemmer=SnowballStemmer)\\\n",
    "    .pipe(text_neg, field_IN='text', field_OUT='text')\n",
    "\n",
    "df_final['text_final'] = df_final['text']\n",
    "\n",
    "# ---- TRAIN - VALIDATION - TEST SPLIT ---- #\n",
    "mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "X_train_valid = df_final.loc[mask_train_test,:]['text_final'].values\n",
    "y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "X_test = df_final.loc[~mask_train_test,:]['text_final'].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, \n",
    "    y_train_valid, \n",
    "    shuffle=True, \n",
    "    train_size=0.9, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ---- PIPELINE DEFINITION ---- #\n",
    "vectorizer = 'tfidf'\n",
    "model = 'linearSVC'\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (vectorizer, TfidfVectorizer()),\n",
    "    (model, LinearSVC())\n",
    "])\n",
    "\n",
    "# ---- GRID SEARCH ---- #\n",
    "params_all = params_for_GridSearchCV(TfidfVectorizer_params,vectorizer)\n",
    "params_all.update(params_for_GridSearchCV(linearSVC_params,model))\n",
    "gscv = GridSearchCV(pipe, params_all, cv = 3, verbose = 4, scoring='f1_macro', n_jobs=4)\n",
    "\n",
    "gscv.fit(X_train, y_train)\n",
    "\n",
    "# ---- BEST MODEL EVALUATION ---- #\n",
    "f1 = f1_score(y_valid, gscv.predict(X_valid),average='macro')\n",
    "report = classification_report(y_valid, gscv.predict(X_valid))\n",
    "confusion = confusion_matrix(y_valid, gscv.predict(X_valid))\n",
    "\n",
    "print(f1)\n",
    "print(report)\n",
    "print(confusion)\n",
    "\n",
    "# ---- GRID SEARCH RESULTS TO CSV ---- #\n",
    "results_name = Path.cwd()/'linearSVC_results.csv'\n",
    "pd.DataFrame(gscv.cv_results_).to_csv(results_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "df_final = df.copy(deep=True)\n",
    "\n",
    "# ---- REMOVAL OF RECORDS WITH DUPLICATE IDS ---- #\n",
    "duplicated_ids = df_dev['ids'].value_counts()\n",
    "duplicated_ids = duplicated_ids[duplicated_ids > 1] \n",
    "df_final = df_final.loc[\n",
    "    ~(df_final['ids'].isin(list(duplicated_ids.index))) | \n",
    "    df_final['sentiment'].isna()\n",
    "]\n",
    "\n",
    "# ---- REMOVAL OF BOTS ---- #\n",
    "df_final = df_final.loc[\n",
    "    ~(df_final['user'].isin(user_suspiciousness[user_suspiciousness>.9].index)) | \n",
    "    df_final['sentiment'].isna()\n",
    "]\n",
    "\n",
    "# ---- OTHER PREPROCESSING STEPS ---- #\n",
    "df_final = df_final\\\n",
    "    .pipe(text_noAmpEnt, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noQuotEnt, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noAt, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noHttp, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noDuplLetters, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noPunctuation, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_stem, field_IN='text', field_OUT='text', stemmer=SnowballStemmer)\\\n",
    "    .pipe(text_neg, field_IN='text', field_OUT='text')\n",
    "\n",
    "df_final['text_final'] = df_final['text']\n",
    "\n",
    "# ---- TRAIN - VALIDATION - TEST SPLIT ---- #\n",
    "mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "X_train_valid = df_final.loc[mask_train_test,:]['text_final'].values\n",
    "y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "X_test = df_final.loc[~mask_train_test,:]['text_final'].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, \n",
    "    y_train_valid, \n",
    "    shuffle=True, \n",
    "    train_size=0.9, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ---- PIPELINE DEFINITION ---- #\n",
    "vectorizer = 'tfidf'\n",
    "model = 'multinomialNB'\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (vectorizer, TfidfVectorizer()),\n",
    "    (model, MultinomialNB())\n",
    "])\n",
    "\n",
    "# ---- GRID SEARCH ---- #\n",
    "params_all = params_for_GridSearchCV(TfidfVectorizer_params,vectorizer)\n",
    "params_all.update(params_for_GridSearchCV(multinomialNB_params,model))\n",
    "gscv = GridSearchCV(pipe, params_all, cv = 3, verbose = 4, scoring='f1_macro', n_jobs=4)\n",
    "\n",
    "gscv.fit(X_train, y_train)\n",
    "\n",
    "# ---- BEST MODEL EVALUATION ---- #\n",
    "f1 = f1_score(y_valid, gscv.predict(X_valid),average='macro')\n",
    "report = classification_report(y_valid, gscv.predict(X_valid))\n",
    "confusion = confusion_matrix(y_valid, gscv.predict(X_valid))\n",
    "\n",
    "print(f1)\n",
    "print(report)\n",
    "print(confusion)\n",
    "\n",
    "# ---- GRID SEARCH RESULTS TO CSV ---- #\n",
    "results_name = Path.cwd()/'multinomialNB.csv'\n",
    "pd.DataFrame(gscv.cv_results_).to_csv(results_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best models test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'linearSVC__C': 1, 'linearSVC__class_weight': 'balanced', 'linearSVC__dual': False, 'linearSVC__fit_intercept': False, 'linearSVC__max_iter': 100, 'linearSVC__penalty': 'l2', 'linearSVC__random_state': 42, 'linearSVC__tol': 0.001, 'tfidf__binary': True, 'tfidf__max_df': 1.0, 'tfidf__max_features': None, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 3), 'tfidf__norm': 'l2', 'tfidf__smooth_idf': False, 'tfidf__stop_words': None, 'tfidf__sublinear_tf': False, 'tfidf__use_idf': True}\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "results = pd.read_csv(Path.cwd()/'linearSVC_results.csv')\n",
    "results[results['rank_test_score'] == 1]['params'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8038680794656297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.78      0.78      9559\n",
      "         1.0       0.83      0.83      0.83     12727\n",
      "\n",
      "    accuracy                           0.81     22286\n",
      "   macro avg       0.80      0.80      0.80     22286\n",
      "weighted avg       0.81      0.81      0.81     22286\n",
      "\n",
      "[[ 7412  2147]\n",
      " [ 2135 10592]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "df_final = df.copy(deep=True)\n",
    "\n",
    "# ---- REMOVAL OF RECORDS WITH DUPLICATE IDS ---- #\n",
    "duplicated_ids = df_dev['ids'].value_counts()\n",
    "duplicated_ids = duplicated_ids[duplicated_ids > 1] \n",
    "df_final = df_final.loc[\n",
    "    ~(df_final['ids'].isin(list(duplicated_ids.index))) | \n",
    "    df_final['sentiment'].isna()\n",
    "]\n",
    "\n",
    "# ---- REMOVAL OF BOTS ---- #\n",
    "df_final = df_final.loc[\n",
    "    ~(df_final['user'].isin(user_suspiciousness[user_suspiciousness>.9].index)) | \n",
    "    df_final['sentiment'].isna()\n",
    "]\n",
    "\n",
    "# ---- OTHER PREPROCESSING STEPS ---- #\n",
    "df_final = df_final\\\n",
    "    .pipe(text_noAmpEnt, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noQuotEnt, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noAt, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noHttp, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noDuplLetters, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noPunctuation, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_stem, field_IN='text', field_OUT='text', stemmer=SnowballStemmer)\\\n",
    "    .pipe(text_neg, field_IN='text', field_OUT='text')\n",
    "\n",
    "df_final['text_final'] = df_final['text']\n",
    "\n",
    "# ---- TRAIN - VALIDATION - TEST SPLIT ---- #\n",
    "mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "X_train_valid = df_final.loc[mask_train_test,:]['text_final'].values\n",
    "y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "X_test = df_final.loc[~mask_train_test,:]['text_final'].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, \n",
    "    y_train_valid, \n",
    "    shuffle=True, \n",
    "    # stratify=y_train_valid, \n",
    "    train_size=0.9, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ---- PREPROCESS AND MODEL CONFIGURATION ---- #\n",
    "model = LinearSVC(\n",
    "    C = 1, \n",
    "    class_weight = 'balanced', \n",
    "    dual = False, \n",
    "    fit_intercept = False, \n",
    "    max_iter = 100, \n",
    "    penalty = 'l2', \n",
    "    random_state = 42, \n",
    "    tol = 0.001\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    binary = True, \n",
    "    max_df = 1.0, \n",
    "    max_features = None, \n",
    "    min_df = 1, \n",
    "    ngram_range = (1,3), \n",
    "    norm = 'l2', \n",
    "    smooth_idf = False, \n",
    "    stop_words = None, \n",
    "    sublinear_tf = False, \n",
    "    use_idf = True\n",
    ")\n",
    "\n",
    "# ---- PIPELINE DEFINITION ---- #\n",
    "ml_pipe = Pipeline([\n",
    "    ('tfidf', vectorizer),\n",
    "    ('linearSVC', model)\n",
    "])\n",
    "\n",
    "ml_pipe.fit(X_train, y_train)\n",
    "\n",
    "f1 = f1_score(y_valid, ml_pipe.predict(X_valid),average='macro')\n",
    "report = classification_report(y_valid, ml_pipe.predict(X_valid))\n",
    "confusion = confusion_matrix(y_valid, ml_pipe.predict(X_valid))\n",
    "\n",
    "print(f1)\n",
    "print(report)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results csv export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipe.fit(X_train_valid, y_train_valid)\n",
    "y_pred = ml_pipe.predict(X_test)\n",
    "pd.DataFrame(y_pred.astype(int),columns=['Predicted']).to_csv(\"output.csv\",index_label=\"Id\", header=[\"Predicted\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd8ee3559d665fee903f84f74f9742602cb00cb47768a52cae0fe6e115d1a823"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('data_science_lab_2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
