%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Based on IEEE the conference template available                     %
% at https://www.ieee.org/conferences/publishing/templates.html       %
% Adapted for the Data Science Lab course at Politecnico di Torino    %
% by Giuseppe Attanasio, Flavio Giobergia                             %
% 2020, DataBase and Data Mining Group                                %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\graphicspath{ {./images/} }

\begin{document}

\title{Sentiment prediction of Twitter contents}

\author{\IEEEauthorblockN{Edoardo Chi√≤}
\IEEEauthorblockA{\textit{Politecnico di Torino} \\
Student id: s301486 \\
s301486@studenti.polito.it}
}

\maketitle

\begin{abstract}
The abstract goes here. Keep it short (approx. 3-4 sentences)
\end{abstract}

\section{Problem overview}
This project concerns a classification problem applied to a collection of Twitter posts (i.e., \textit{tweets}) written by different users. The goal of the project is to perform a sentiment analysis of the posts contained in the dataset.\\
Training and validation of the models are conducted on a development set, containing 224,994 labelled recordings, while the set to test the models contains 74,999 recordings.\\
The development set is composed of six fields:
\begin{itemize}
    \item \textit{sentiment}: sentiment labels;
    \item \textit{ids}: numerical identifier of the tweet;
    \item \textit{date}: publication date of the tweet;
    \item \textit{flag}: query used to collect the tweet;
    \item \textit{user}: name of the user that posted the tweet;
    \item \textit{text}: text of the tweet. 
\end{itemize}
The \textit{sentiment} field contains a label for each record. There are two classes: the text is considered having a positive trait if the label value is \textbf{1}, instead it is considered negative if the label value is \textbf{0}. The classes are not well balanced, indeed there are 130,157 data points having label 1, and just 94,837 having value 0. An exploratory analysis of the dataset was performed, to study the contents of all the fields and understand which features should be taken into account.\\
The \textit{ids} field (i.e., the row identifiers) presents 278 pairs of duplicates, each one having one row of the related pair of rows associated to a positive sentiment, and the other one to a negative sentiment; since it is not possible to know the correct sentiment for these records, they do not add any insight and so they were removed during the data cleaning. \\
The \textit{date} field shows that the tweets, both in the development dataset and in the evaluation dataset, were posted between April 6\textsuperscript{th} 2009 and June 25\textsuperscript{th} 2009; this suggests that the data should be homogeneous across the datasets, with a similar distribution of sentiment. \\
The \textit{flag} field presents a unique value in all tweets in both the datasets, so it is not useful for the analysis and it can be discarded. \\
The \textit{user} field contains the names of 10,647 distinct users; each user wrote at least 15 tweets (considering the datasets), but some users were much more prolific than others (figure \ref{fig:user_per_tweet}).
\begin{figure}
    \includegraphics[width=0.5\textwidth]{user_per_tweet}
    \caption[Histogram of users per tweets]{Histogram representing the number of users per number of tweets}
    \label{fig:user_per_tweet}
\end{figure}
Analysing the user average sentiment distribution (figure \ref{fig:average_user_sentiment}), it is possible to notice that there are many users whose tweets are almost exclusively positive.
\begin{figure}
    \includegraphics[width=0.5\textwidth]{average_user_sentiment}
    \caption[User average sentiment]{Histogram representing the number of users per average sentiment}
    \label{fig:average_user_sentiment}
\end{figure}
This odd behaviour led to a deeper investigation of the texts written by each user; a mean cosine similarity among the tweets posted by the same user was computed (figure \ref{fig:internal_similarity_users}), and it showed that a small but significant group of users used the exact same words in multiple tweets.  
\begin{figure}
    \includegraphics[width=0.5\textwidth]{internal_similarity_users}
    \caption[Mean cosine similarity]{Histogram representing the number of users per mean cosine similarity computed on the posted tweets of each user}
    \label{fig:internal_similarity_users}
\end{figure}
Some of the users that posted the largest number of tweets, also have the highest average cosine similarity among their tweets; a precise inspection of these users suggests that most of them behave as bots, regularly posting the same message. The problem of bot identification can be addressed using machine learning frameworks, as the one suggested by Kantepe and Ganiz \cite{8093483}, however for this project a simple removal of suspicious users is performed in the preprocessing step. \\
Finally, the \textit{text} field collects the tweet messages. The messages are written in english, they may contain tags (i.e., starting with "@"), hashtags (i.e., starting with "\#"), and URLs. HTML character entities (e.g., "\textit{\&amp;}", "\textit{\&quot;}") are present too. \\
The \textit{text} field is the only one which is going to be considered to train the model and to predict the text sentiment; indeed, the \textit{date} field is only useful to show that both the development set and the evaluation set refer to the same time period; instead, the \textit{user} field is not taken into account because this would instruct the model to predict the sentiment based also on the user who posted the message, loosing the more general scope the model should have. 

\section{Proposed approach}
\subsection{Preprocessing}
The model evaluation is made using the evaluation set as reference. As said before, the development set and the evaluation set share the same characteristics, and, presumably, the same source. Basing the model performance on this evaluation set may lead to overfitting, making the model inadequate to predict the tweet sentiment outside this project. Therefore, two possible preprocessing paths have been designed: one more complex, composed of several steps, and more suitable to a general application; and one much simpler, consisting just in the first two steps of the complex one, that performs better, but is probably less general. \\
The message cleaning steps composing the more articulate preprocessing path were partially influenced by \cite{ZainuddinNurulhuda2016TFSa}. The steps are the following
\begin{enumerate}
    \item "\textit{\&amp;}" HTML entities are removed;
    \item "\textit{\&quot;}" HTML entities are removed; 
    \item Words staring with \textit{@} are removed: these words are user tags, and do not give insights regarding the sentiment of the message; 
    \item Words staring with \textit{http} are removed: these words are URLs, and do not give insights regarding the sentiment of the message;
    \item Punctuation is removed;
    \item Stemming is performed, using the Snowball stemmer by Porter \cite{Porter2006};
    \item Negations in sentences are stressed appending in subsequent words a "\textit{\_NEG}" suffix.
    \item Exclusion of users behaving as bots: as seen before, some users write the same message multiple times (i.e., users that have an high average cosine similarity computed on their tweets), always associated with the same sentiment. These users should be excluded, since they poll
\end{enumerate}
After these steps, the features are extracted tokeninzing the meassages and creating a bag-of-words; the schema applied to weight the words is the \textbf{tf-idf} (i.e., term frequency - inverse document frequency), meaning that each word importance increases as its frequency in the same meassage increases, and lowers the more the word is used across all the messages in the collection. No maximum or minimum document frequency is set for a word to be considered, and also no maximum number of words is set; this means that the bag-of-words includes all the words the appear in the document collection. Other preprocessing options, such as the removal of stopwords, the type of normalization, and the N-Gram (i.e., the contiguous sequence of N items from a given sample of text or speech) length, are considered as hyperparameters of the preprocessing step.
\subsection{Model selection}
Two fast and light classification models were assessed: 
\begin{itemize}
    \item Naive Bayes Classifiers: it is a classifier family commonly employed in sentiment analysis contexts \cite{MEDHAT20141093}. It uses the Bayes Theorem to predict the probability that a given feature set belongs to a particular label. The Naive Bayes Classifier family include several algorithms, in particular the Multinomial and the Bernoulli classifiers are often for document classification, with similar performance \cite{8776800}. For this project, given that the tf-idf word weighting schema is used instead of a basic term occurrence, the Multinomial Naive Bayes classifier was selected. 
    \item Support Vector Machine Classifier: these algorithms carry the classification task determining the linear separators in the search space which can best separate the different classes. SVCs have already been used in sentiment analysis contexts, achieving good results \cite{6914200} Because of the extent of the dataset, in particular concerning the feaures, only the simpler Linear Support Vector Classifier was considered. 
\end{itemize}
The best performing hyperparameter configuration was identified using a grid search.
\subsection{Hyperparameters tuning}
The development set is 90/10 train/validation splitted, given that the development set is fairly large, and a grid search is run on the traning subset. A linear SVM and a Multinomial Naive Bayes with their default configurations are trained and their performances are assessed based on the resulting macro f1 score.
\section{Results}
Here you will present your results (models \& configurations selected, performance achieved)

\section{Discussion}
Any relevant discussion goes here.
In this section, you will present your solution. Please fill in accordingly.
You can use citations as follows: \cite{goodfellow2016deep} (you can add BibTeX citations in the \textit{bibliography.bib} file).

\bibliography{bibliography}
\bibliographystyle{ieeetr}

\end{document}
