{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'development.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['date'] = df_dev['date'].\\\n",
    "    astype('string')\\\n",
    "    .str.split(' ')\\\n",
    "    .apply(lambda x : ' '.join([x[i] for i in [1,2,3,5]]))\\\n",
    "    .pipe(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords analysis: sklearn VS nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edo_c\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download as nltk_download\n",
    "\n",
    "nltk_download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "print(stopwords.words('english'))\n",
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sometimes', 'thin', 'amongst', 'mostly', 'empty', 'namely', 'therefore', 'here', 'been', 'twenty', 'whither', 'whom', 'call', 'against', 'con', 'her', 'some', 'afterwards', 'indeed', 'wherever', 'below', 'whereafter', 'might', 'too', 'thus', 'both', 'after', 'ten', 'five', 'forty', 'hasnt', 'less', 'thereupon', 'whence', 'be', 'someone', 'also', 'eleven', 'mill', 'out', 'formerly', 'three', 'will', 'cry', 'which', 'again', 'very', 'when', 'do', 'himself', 'my', 'hereby', 'twelve', 'at', 'under', 'throughout', 'down', 'otherwise', 're', 'before', 'couldnt', 'beyond', 'amount', 'up', 'much', 'yourselves', 'towards', 'whether', 'thereafter', 'hereafter', 'that', 'perhaps', 'system', 'meanwhile', 'they', 'part', 'alone', 'for', 'him', 'me', 'put', 'nowhere', 'yourself', 'due', 'nor', 'ours', 'now', 'hereupon', 'fire', 'fill', 'show', 'next', 'further', 'bottom', 'toward', 'is', 'several', 'somehow', 'to', 'thereby', 'first', 'we', 'move', 'while', 'across', 'own', 'all', 'etc', 'un', 'among', 'then', 'yours', 'elsewhere', 'becoming', 'done', 'somewhere', 'your', 'others', 'get', 'behind', 'eight', 'between', 'become', 'bill', 'us', 'if', 'often', 'anyway', 'sincere', 'yet', 'take', 'hence', 'nine', 'why', 'on', 'still', 'have', 'may', 'co', 'am', 'about', 'keep', 'name', 'everywhere', 'off', 'can', 'he', 'the', 'top', 'almost', 'eg', 'none', 'an', 'everyone', 'has', 'seeming', 'our', 'cannot', 'not', 'had', 'fifteen', 'beside', 'together', 'along', 'must', 'six', 'former', 'side', 'though', 'third', 'see', 'herself', 'only', 'would', 'itself', 'those', 'two', 'many', 'its', 'whose', 'herein', 'full', 'whereby', 'no', 'besides', 'sixty', 'one', 'therein', 'their', 'last', 'were', 'but', 'being', 'inc', 'via', 'hers', 'thru', 'who', 'into', 'where', 'i', 'always', 'moreover', 'whatever', 'neither', 'every', 'other', 'once', 'well', 'thence', 'them', 'few', 'what', 'everything', 'than', 'although', 'any', 'back', 'ourselves', 'give', 'four', 'least', 'noone', 'cant', 'above', 'wherein', 'either', 'it', 'his', 'nobody', 'anyone', 'or', 'seems', 'beforehand', 'thick', 'most', 'made', 'myself', 'amoungst', 'detail', 'per', 'mine', 'seemed', 'even', 'could', 'how', 'each', 'whoever', 'else', 'from', 'latter', 'in', 'rather', 'so', 'anyhow', 'interest', 'you', 'because', 'anywhere', 'already', 'go', 'nothing', 'with', 'within', 'latterly', 'whereupon', 'over', 'these', 'such', 'since', 'except', 'de', 'sometime', 'something', 'as', 'however', 'she', 'find', 'enough', 'same', 'should', 'fifty', 'front', 'more', 'ever', 'through', 'seem', 'whenever', 'without', 'serious', 'onto', 'ie', 'this', 'there', 'please', 'found', 'whereas', 'becomes', 'by', 'themselves', 'became', 'upon', 'never', 'and', 'describe', 'ltd', 'whole', 'a', 'hundred', 'until', 'around', 'are', 'of', 'anything', 'was', 'another', 'during', 'nevertheless']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "sklearn_stopwords = list(text.ENGLISH_STOP_WORDS)\n",
    "print(list(text.ENGLISH_STOP_WORDS))\n",
    "len(list(text.ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_stopwords</th>\n",
       "      <th>sklearn</th>\n",
       "      <th>nltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sometimes</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thin</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>two</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>many</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>its</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>herself</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>only</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>would</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>itself</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>those</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>378 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    all_stopwords  sklearn   nltk\n",
       "0       sometimes     True  False\n",
       "1            thin     True  False\n",
       "2             two     True  False\n",
       "3            many     True  False\n",
       "4             its     True   True\n",
       "..            ...      ...    ...\n",
       "373       herself     True   True\n",
       "374          only     True   True\n",
       "375         would     True  False\n",
       "376        itself     True   True\n",
       "377         those     True   True\n",
       "\n",
       "[378 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_from_all_sources = pd.DataFrame(list(set(nltk_stopwords).union(set(sklearn_stopwords))), columns=['all_stopwords'])\n",
    "\n",
    "stopwords_from_all_sources['sklearn'] = False\n",
    "stopwords_from_all_sources.loc[stopwords_from_all_sources['all_stopwords'].isin(sklearn_stopwords),['sklearn']] = True\n",
    "\n",
    "stopwords_from_all_sources['nltk'] = False\n",
    "stopwords_from_all_sources.loc[stopwords_from_all_sources['all_stopwords'].isin(nltk_stopwords),['nltk']] = True\n",
    "\n",
    "stopwords_from_all_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should try to use both all stopwords, and sklearn and nltk stopwords singularly (and also no stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_all_list = list(stopwords_from_all_sources['all_stopwords'])\n",
    "\n",
    "stopwords_all_NEG_list = [f'{i+\"_neg\"}' for i in stopwords_all_list]\n",
    "\n",
    "def stopwords_list_gen(source_list = [sklearn_stopwords,nltk_stopwords], generate_neg = True):\n",
    "    stopwords_all_list = set()\n",
    "    for source in source_list:\n",
    "        stopwords_all_list = stopwords_all_list.union(set(source))\n",
    "        if generate_neg:\n",
    "            stopwords_all_list = stopwords_all_list.union(set([f'{i+\"_neg\"}' for i in source]))\n",
    "    return stopwords_all_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing with stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['text_token'] = df_dev['text'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '@words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['text_noAt'] = df_dev['text_token'].apply(lambda x : [i for i in x if not(i.startswith('@'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '&amp' and '&quot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['text_noAmpQuot'] = df_dev['text_noAt']\\\n",
    "    .apply(lambda x : [i for i in x if '&amp' not in i])\\\n",
    "    .apply(lambda x : [i for i in x if '&quot' not in i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elimination of repeated letters (incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'ulaaa'\n",
    "\n",
    "def remove_repeated_letters(word):\n",
    "    for letter in word:\n",
    "        print(letter)\n",
    "\n",
    "# nltk_stemmer.stem(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove puntuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string as py_string\n",
    "df_dev['text_noPunct'] = df_dev['text_noAmpQuot']\\\n",
    "    .apply(lambda x : [i.translate(str.maketrans('', '', py_string.punctuation)) for i in x])\\\n",
    "    .apply(lambda x : [i for i in x if i != ''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\edo_c\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\edo_c\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk_download('wordnet')\n",
    "nltk_download('omw-1.4')\n",
    "\n",
    "nltk_stemmer = PorterStemmer()\n",
    "# nltk_stemmer = LancasterStemmer()\n",
    "# nltk_stemmer = SnowballStemmer('english')\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "# from nltk.corpus import wordnet\n",
    "\n",
    "df_dev['text_stemmed'] = df_dev['text_noPunct'].apply(lambda x : [nltk_stemmer.stem(word) for word in x])\n",
    "# df_dev['text_stemmed'] = df_dev['text_noPunct'].apply(lambda x : [nltk_lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "df_dev['text_neg'] = df_dev['text_stemmed'].apply(lambda x : mark_negation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User manual filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = df_dev.loc[\n",
    "    ~(df_dev['user'] == 'lost_dog') &\n",
    "    ~(df_dev['user'] == 'webwoke') &\n",
    "    ~(df_dev['user'] == 'tweetpet') &\n",
    "    ~(df_dev['user'].str.contains('tweeteradder')) &\n",
    "    ~(df_dev['user'].str.contains('tweetfollow')) &\n",
    "    ~(df_dev['user'] == 'divxdownloads')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df_dev_final = df_dev\n",
    "df_dev_final['text_final'] = df_dev['text_neg'].apply(lambda x : ' '.join(x))\n",
    "# df_dev_final['text_final'] = df_dev_final['text']\n",
    "\n",
    "# stopwords_to_use = [nltk_stemmer.stem(word) for word in stopwords_list_gen()]\n",
    "stopwords_to_use = [nltk_stemmer.stem(word) for word in stopwords_all_list]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    # stop_words = stopwords_to_use,\n",
    "    # stop_words = 'english',\n",
    "    stop_words = None,\n",
    "    binary=True, \n",
    "    use_idf=True, \n",
    "    norm='l2',\n",
    "    smooth_idf=True\n",
    ")\n",
    "\n",
    "wpm = vectorizer.fit_transform(df_dev_final['text_final'])\n",
    "\n",
    "N = 2000\n",
    "\n",
    "word_freq = pd.Series(\n",
    "    data = np.asarray(wpm.sum(axis=0)).squeeze(),\n",
    "    index = vectorizer.get_feature_names_out()\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "word_freq = word_freq[:N]\n",
    "\n",
    "word_ind = [w in word_freq.index for w in vectorizer.get_feature_names_out()]\n",
    "\n",
    "words_df = pd.DataFrame(\n",
    "    data = wpm[:,word_ind].toarray(),\n",
    "    columns = vectorizer.get_feature_names_out()[word_ind],\n",
    "    index = df_dev.index\n",
    ").add_prefix('word_')\n",
    "\n",
    "# words_df\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = words_df.values\n",
    "y = df_dev['sentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, train_size=0.7, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df_analysis = words_df.join(df_dev['sentiment']).groupby(['sentiment']).sum().transpose()\n",
    "words_df_analysis.columns\n",
    "words_df_analysis['diff'] = (words_df_analysis.iloc[:,1] - words_df_analysis.iloc[:,0]) / (words_df_analysis.iloc[:,1] + words_df_analysis.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sentiment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word_damn</th>\n",
       "      <td>329.246677</td>\n",
       "      <td>108.698845</td>\n",
       "      <td>-0.503596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_cant_neg</th>\n",
       "      <td>141.002922</td>\n",
       "      <td>43.463834</td>\n",
       "      <td>-0.528762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_bored</th>\n",
       "      <td>248.789705</td>\n",
       "      <td>76.521481</td>\n",
       "      <td>-0.529549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_want_neg</th>\n",
       "      <td>300.128996</td>\n",
       "      <td>91.835226</td>\n",
       "      <td>-0.531410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_feel_neg</th>\n",
       "      <td>176.485389</td>\n",
       "      <td>50.287525</td>\n",
       "      <td>-0.556494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_stupid</th>\n",
       "      <td>190.261406</td>\n",
       "      <td>54.017513</td>\n",
       "      <td>-0.557739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_lost</th>\n",
       "      <td>239.839159</td>\n",
       "      <td>67.327007</td>\n",
       "      <td>-0.561625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_crap</th>\n",
       "      <td>144.567724</td>\n",
       "      <td>38.978176</td>\n",
       "      <td>-0.575276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_bad</th>\n",
       "      <td>577.674582</td>\n",
       "      <td>150.989862</td>\n",
       "      <td>-0.585571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_sorry</th>\n",
       "      <td>908.412752</td>\n",
       "      <td>235.021268</td>\n",
       "      <td>-0.588920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_pain</th>\n",
       "      <td>142.961618</td>\n",
       "      <td>36.917548</td>\n",
       "      <td>-0.589529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_work_neg</th>\n",
       "      <td>299.545908</td>\n",
       "      <td>75.866148</td>\n",
       "      <td>-0.595825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_happened</th>\n",
       "      <td>144.716302</td>\n",
       "      <td>36.457886</td>\n",
       "      <td>-0.597538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_sigh</th>\n",
       "      <td>141.439925</td>\n",
       "      <td>35.501307</td>\n",
       "      <td>-0.598722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_miss</th>\n",
       "      <td>808.733939</td>\n",
       "      <td>196.097958</td>\n",
       "      <td>-0.609690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_missing</th>\n",
       "      <td>176.748640</td>\n",
       "      <td>42.255124</td>\n",
       "      <td>-0.614115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_find_neg</th>\n",
       "      <td>144.455818</td>\n",
       "      <td>33.069078</td>\n",
       "      <td>-0.627443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_wanna_neg</th>\n",
       "      <td>144.098258</td>\n",
       "      <td>30.350199</td>\n",
       "      <td>-0.652044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_anymore_neg</th>\n",
       "      <td>190.348124</td>\n",
       "      <td>35.532279</td>\n",
       "      <td>-0.685389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_working_neg</th>\n",
       "      <td>160.539967</td>\n",
       "      <td>28.362060</td>\n",
       "      <td>-0.699717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_hate</th>\n",
       "      <td>600.514048</td>\n",
       "      <td>99.562884</td>\n",
       "      <td>-0.715566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_cry</th>\n",
       "      <td>250.362063</td>\n",
       "      <td>38.610581</td>\n",
       "      <td>-0.732773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_poor</th>\n",
       "      <td>298.202462</td>\n",
       "      <td>45.008690</td>\n",
       "      <td>-0.737720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_headache</th>\n",
       "      <td>216.143129</td>\n",
       "      <td>28.378952</td>\n",
       "      <td>-0.767882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_suck</th>\n",
       "      <td>531.029793</td>\n",
       "      <td>67.709561</td>\n",
       "      <td>-0.773826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_sick</th>\n",
       "      <td>327.969492</td>\n",
       "      <td>39.201194</td>\n",
       "      <td>-0.786469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_ugh</th>\n",
       "      <td>301.240497</td>\n",
       "      <td>29.964156</td>\n",
       "      <td>-0.819060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_hurt</th>\n",
       "      <td>385.839629</td>\n",
       "      <td>35.488977</td>\n",
       "      <td>-0.831538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_sad</th>\n",
       "      <td>726.874941</td>\n",
       "      <td>47.304265</td>\n",
       "      <td>-0.877795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "sentiment                  0           1      diff\n",
       "word_damn         329.246677  108.698845 -0.503596\n",
       "word_cant_neg     141.002922   43.463834 -0.528762\n",
       "word_bored        248.789705   76.521481 -0.529549\n",
       "word_want_neg     300.128996   91.835226 -0.531410\n",
       "word_feel_neg     176.485389   50.287525 -0.556494\n",
       "word_stupid       190.261406   54.017513 -0.557739\n",
       "word_lost         239.839159   67.327007 -0.561625\n",
       "word_crap         144.567724   38.978176 -0.575276\n",
       "word_bad          577.674582  150.989862 -0.585571\n",
       "word_sorry        908.412752  235.021268 -0.588920\n",
       "word_pain         142.961618   36.917548 -0.589529\n",
       "word_work_neg     299.545908   75.866148 -0.595825\n",
       "word_happened     144.716302   36.457886 -0.597538\n",
       "word_sigh         141.439925   35.501307 -0.598722\n",
       "word_miss         808.733939  196.097958 -0.609690\n",
       "word_missing      176.748640   42.255124 -0.614115\n",
       "word_find_neg     144.455818   33.069078 -0.627443\n",
       "word_wanna_neg    144.098258   30.350199 -0.652044\n",
       "word_anymore_neg  190.348124   35.532279 -0.685389\n",
       "word_working_neg  160.539967   28.362060 -0.699717\n",
       "word_hate         600.514048   99.562884 -0.715566\n",
       "word_cry          250.362063   38.610581 -0.732773\n",
       "word_poor         298.202462   45.008690 -0.737720\n",
       "word_headache     216.143129   28.378952 -0.767882\n",
       "word_suck         531.029793   67.709561 -0.773826\n",
       "word_sick         327.969492   39.201194 -0.786469\n",
       "word_ugh          301.240497   29.964156 -0.819060\n",
       "word_hurt         385.839629   35.488977 -0.831538\n",
       "word_sad          726.874941   47.304265 -0.877795"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df_analysis.sort_values(by = 'diff', axis=0, ascending=False).iloc[470:499,:]\n",
    "\n",
    "# words_df_analysis.loc[words_df_analysis.index.str.contains('movie')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text_token</th>\n",
       "      <th>text_noAt</th>\n",
       "      <th>text_noAmpQuot</th>\n",
       "      <th>text_noPunct</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_neg</th>\n",
       "      <th>text_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "      <td>2069225157</td>\n",
       "      <td>2009-06-07 15:29:23</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>icysun23</td>\n",
       "      <td>@SackPackies Yes  @ohflawless I know   I think...</td>\n",
       "      <td>[@SackPackies, Yes, @ohflawless, I, know, I, t...</td>\n",
       "      <td>[Yes, I, know, I, think, I'll, just, wait, for...</td>\n",
       "      <td>[Yes, I, know, I, think, I'll, just, wait, for...</td>\n",
       "      <td>[Yes, I, know, I, think, Ill, just, wait, for,...</td>\n",
       "      <td>[Yes, I, know, I, think, Ill, just, wait, for,...</td>\n",
       "      <td>[Yes, I, know, I, think, Ill, just, wait, for,...</td>\n",
       "      <td>Yes I know I think Ill just wait for tomorrow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>1</td>\n",
       "      <td>2064066330</td>\n",
       "      <td>2009-06-07 05:06:26</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>misharae74</td>\n",
       "      <td>@dave_annable B&amp;amp;S is in my NetFlix queue &amp;...</td>\n",
       "      <td>[@dave_annable, B&amp;amp;S, is, in, my, NetFlix, ...</td>\n",
       "      <td>[B&amp;amp;S, is, in, my, NetFlix, queue, &amp;amp;, I...</td>\n",
       "      <td>[is, in, my, NetFlix, queue, I, should, receiv...</td>\n",
       "      <td>[is, in, my, NetFlix, queue, I, should, receiv...</td>\n",
       "      <td>[is, in, my, NetFlix, queue, I, should, receiv...</td>\n",
       "      <td>[is, in, my, NetFlix, queue, I, should, receiv...</td>\n",
       "      <td>is in my NetFlix queue I should receive the fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>0</td>\n",
       "      <td>2251729570</td>\n",
       "      <td>2009-06-20 04:24:29</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>roxiijonas</td>\n",
       "      <td>I wanna watch PPP so bad! But my dads watching...</td>\n",
       "      <td>[I, wanna, watch, PPP, so, bad!, But, my, dads...</td>\n",
       "      <td>[I, wanna, watch, PPP, so, bad!, But, my, dads...</td>\n",
       "      <td>[I, wanna, watch, PPP, so, bad!, But, my, dads...</td>\n",
       "      <td>[I, wanna, watch, PPP, so, bad, But, my, dads,...</td>\n",
       "      <td>[I, wanna, watch, PPP, so, bad, But, my, dad, ...</td>\n",
       "      <td>[I, wanna, watch, PPP, so, bad, But, my, dad, ...</td>\n",
       "      <td>I wanna watch PPP so bad But my dad watching t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>0</td>\n",
       "      <td>2051330834</td>\n",
       "      <td>2009-06-05 21:41:00</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Xboxking</td>\n",
       "      <td>@CrazyEcho YAH... ohh wait... We'll miss you C...</td>\n",
       "      <td>[@CrazyEcho, YAH..., ohh, wait..., We'll, miss...</td>\n",
       "      <td>[YAH..., ohh, wait..., We'll, miss, you, Chuck...</td>\n",
       "      <td>[YAH..., ohh, wait..., We'll, miss, you, Chuck...</td>\n",
       "      <td>[YAH, ohh, wait, Well, miss, you, Chuck, lets,...</td>\n",
       "      <td>[YAH, ohh, wait, Well, miss, you, Chuck, let, ...</td>\n",
       "      <td>[YAH, ohh, wait, Well, miss, you, Chuck, let, ...</td>\n",
       "      <td>YAH ohh wait Well miss you Chuck let take a mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>1</td>\n",
       "      <td>2183744916</td>\n",
       "      <td>2009-06-15 14:55:43</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Mum_of_Six</td>\n",
       "      <td>@KP_eezy  I'll definitely keep you posted. Hop...</td>\n",
       "      <td>[@KP_eezy, I'll, definitely, keep, you, posted...</td>\n",
       "      <td>[I'll, definitely, keep, you, posted., Hoping,...</td>\n",
       "      <td>[I'll, definitely, keep, you, posted., Hoping,...</td>\n",
       "      <td>[Ill, definitely, keep, you, posted, Hoping, t...</td>\n",
       "      <td>[Ill, definitely, keep, you, posted, Hoping, t...</td>\n",
       "      <td>[Ill, definitely, keep, you, posted, Hoping, t...</td>\n",
       "      <td>Ill definitely keep you posted Hoping to go ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223943</th>\n",
       "      <td>0</td>\n",
       "      <td>2004066058</td>\n",
       "      <td>2009-06-02 07:36:39</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>FaithfulChosen</td>\n",
       "      <td>@jubean true  But wait till she becomes his se...</td>\n",
       "      <td>[@jubean, true, But, wait, till, she, becomes,...</td>\n",
       "      <td>[true, But, wait, till, she, becomes, his, sec...</td>\n",
       "      <td>[true, But, wait, till, she, becomes, his, sec...</td>\n",
       "      <td>[true, But, wait, till, she, becomes, his, sec...</td>\n",
       "      <td>[true, But, wait, till, she, becomes, his, sec...</td>\n",
       "      <td>[true, But, wait, till, she, becomes, his, sec...</td>\n",
       "      <td>true But wait till she becomes his second mommy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224019</th>\n",
       "      <td>1</td>\n",
       "      <td>2002619508</td>\n",
       "      <td>2009-06-02 04:43:38</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>its4am</td>\n",
       "      <td>@durian_girl lol you have to let the little on...</td>\n",
       "      <td>[@durian_girl, lol, you, have, to, let, the, l...</td>\n",
       "      <td>[lol, you, have, to, let, the, little, one, sl...</td>\n",
       "      <td>[lol, you, have, to, let, the, little, one, sl...</td>\n",
       "      <td>[lol, you, have, to, let, the, little, one, sl...</td>\n",
       "      <td>[lol, you, have, to, let, the, little, one, sl...</td>\n",
       "      <td>[lol, you, have, to, let, the, little, one, sl...</td>\n",
       "      <td>lol you have to let the little one sleep You h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224764</th>\n",
       "      <td>1</td>\n",
       "      <td>1984915911</td>\n",
       "      <td>2009-05-31 16:03:41</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Jennifer_x_</td>\n",
       "      <td>Today was soooo good ! M&amp;amp;Ds was aweeesome,...</td>\n",
       "      <td>[Today, was, soooo, good, !, M&amp;amp;Ds, was, aw...</td>\n",
       "      <td>[Today, was, soooo, good, !, M&amp;amp;Ds, was, aw...</td>\n",
       "      <td>[Today, was, soooo, good, !, was, aweeesome,, ...</td>\n",
       "      <td>[Today, was, soooo, good, was, aweeesome, BBQ,...</td>\n",
       "      <td>[Today, wa, soooo, good, wa, aweeesome, BBQ, w...</td>\n",
       "      <td>[Today, wa, soooo, good, wa, aweeesome, BBQ, w...</td>\n",
       "      <td>Today wa soooo good wa aweeesome BBQ wa so yum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224767</th>\n",
       "      <td>0</td>\n",
       "      <td>2201725794</td>\n",
       "      <td>2009-06-16 20:41:04</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xPurplexMuffinx</td>\n",
       "      <td>Just woke up... School today  Can't wait til i...</td>\n",
       "      <td>[Just, woke, up..., School, today, Can't, wait...</td>\n",
       "      <td>[Just, woke, up..., School, today, Can't, wait...</td>\n",
       "      <td>[Just, woke, up..., School, today, Can't, wait...</td>\n",
       "      <td>[Just, woke, up, School, today, Cant, wait, ti...</td>\n",
       "      <td>[Just, woke, up, School, today, Cant, wait, ti...</td>\n",
       "      <td>[Just, woke, up, School, today, Cant, wait, ti...</td>\n",
       "      <td>Just woke up School today Cant wait til it over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224768</th>\n",
       "      <td>0</td>\n",
       "      <td>2246465602</td>\n",
       "      <td>2009-06-19 17:38:14</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>HibaNick</td>\n",
       "      <td>@JonasLover_01 Aw  It's okay.. Just wait till ...</td>\n",
       "      <td>[@JonasLover_01, Aw, It's, okay.., Just, wait,...</td>\n",
       "      <td>[Aw, It's, okay.., Just, wait, till, they, com...</td>\n",
       "      <td>[Aw, It's, okay.., Just, wait, till, they, com...</td>\n",
       "      <td>[Aw, Its, okay, Just, wait, till, they, come, ...</td>\n",
       "      <td>[Aw, Its, okay, Just, wait, till, they, come, ...</td>\n",
       "      <td>[Aw, Its, okay, Just, wait, till, they, come, ...</td>\n",
       "      <td>Aw Its okay Just wait till they come to Oz and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1025 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment         ids                date      flag             user  \\\n",
       "195             0  2069225157 2009-06-07 15:29:23  NO_QUERY         icysun23   \n",
       "323             1  2064066330 2009-06-07 05:06:26  NO_QUERY       misharae74   \n",
       "470             0  2251729570 2009-06-20 04:24:29  NO_QUERY       roxiijonas   \n",
       "532             0  2051330834 2009-06-05 21:41:00  NO_QUERY         Xboxking   \n",
       "536             1  2183744916 2009-06-15 14:55:43  NO_QUERY       Mum_of_Six   \n",
       "...           ...         ...                 ...       ...              ...   \n",
       "223943          0  2004066058 2009-06-02 07:36:39  NO_QUERY   FaithfulChosen   \n",
       "224019          1  2002619508 2009-06-02 04:43:38  NO_QUERY           its4am   \n",
       "224764          1  1984915911 2009-05-31 16:03:41  NO_QUERY      Jennifer_x_   \n",
       "224767          0  2201725794 2009-06-16 20:41:04  NO_QUERY  xPurplexMuffinx   \n",
       "224768          0  2246465602 2009-06-19 17:38:14  NO_QUERY         HibaNick   \n",
       "\n",
       "                                                     text  \\\n",
       "195     @SackPackies Yes  @ohflawless I know   I think...   \n",
       "323     @dave_annable B&amp;S is in my NetFlix queue &...   \n",
       "470     I wanna watch PPP so bad! But my dads watching...   \n",
       "532     @CrazyEcho YAH... ohh wait... We'll miss you C...   \n",
       "536     @KP_eezy  I'll definitely keep you posted. Hop...   \n",
       "...                                                   ...   \n",
       "223943  @jubean true  But wait till she becomes his se...   \n",
       "224019  @durian_girl lol you have to let the little on...   \n",
       "224764  Today was soooo good ! M&amp;Ds was aweeesome,...   \n",
       "224767  Just woke up... School today  Can't wait til i...   \n",
       "224768  @JonasLover_01 Aw  It's okay.. Just wait till ...   \n",
       "\n",
       "                                               text_token  \\\n",
       "195     [@SackPackies, Yes, @ohflawless, I, know, I, t...   \n",
       "323     [@dave_annable, B&amp;S, is, in, my, NetFlix, ...   \n",
       "470     [I, wanna, watch, PPP, so, bad!, But, my, dads...   \n",
       "532     [@CrazyEcho, YAH..., ohh, wait..., We'll, miss...   \n",
       "536     [@KP_eezy, I'll, definitely, keep, you, posted...   \n",
       "...                                                   ...   \n",
       "223943  [@jubean, true, But, wait, till, she, becomes,...   \n",
       "224019  [@durian_girl, lol, you, have, to, let, the, l...   \n",
       "224764  [Today, was, soooo, good, !, M&amp;Ds, was, aw...   \n",
       "224767  [Just, woke, up..., School, today, Can't, wait...   \n",
       "224768  [@JonasLover_01, Aw, It's, okay.., Just, wait,...   \n",
       "\n",
       "                                                text_noAt  \\\n",
       "195     [Yes, I, know, I, think, I'll, just, wait, for...   \n",
       "323     [B&amp;S, is, in, my, NetFlix, queue, &amp;, I...   \n",
       "470     [I, wanna, watch, PPP, so, bad!, But, my, dads...   \n",
       "532     [YAH..., ohh, wait..., We'll, miss, you, Chuck...   \n",
       "536     [I'll, definitely, keep, you, posted., Hoping,...   \n",
       "...                                                   ...   \n",
       "223943  [true, But, wait, till, she, becomes, his, sec...   \n",
       "224019  [lol, you, have, to, let, the, little, one, sl...   \n",
       "224764  [Today, was, soooo, good, !, M&amp;Ds, was, aw...   \n",
       "224767  [Just, woke, up..., School, today, Can't, wait...   \n",
       "224768  [Aw, It's, okay.., Just, wait, till, they, com...   \n",
       "\n",
       "                                           text_noAmpQuot  \\\n",
       "195     [Yes, I, know, I, think, I'll, just, wait, for...   \n",
       "323     [is, in, my, NetFlix, queue, I, should, receiv...   \n",
       "470     [I, wanna, watch, PPP, so, bad!, But, my, dads...   \n",
       "532     [YAH..., ohh, wait..., We'll, miss, you, Chuck...   \n",
       "536     [I'll, definitely, keep, you, posted., Hoping,...   \n",
       "...                                                   ...   \n",
       "223943  [true, But, wait, till, she, becomes, his, sec...   \n",
       "224019  [lol, you, have, to, let, the, little, one, sl...   \n",
       "224764  [Today, was, soooo, good, !, was, aweeesome,, ...   \n",
       "224767  [Just, woke, up..., School, today, Can't, wait...   \n",
       "224768  [Aw, It's, okay.., Just, wait, till, they, com...   \n",
       "\n",
       "                                             text_noPunct  \\\n",
       "195     [Yes, I, know, I, think, Ill, just, wait, for,...   \n",
       "323     [is, in, my, NetFlix, queue, I, should, receiv...   \n",
       "470     [I, wanna, watch, PPP, so, bad, But, my, dads,...   \n",
       "532     [YAH, ohh, wait, Well, miss, you, Chuck, lets,...   \n",
       "536     [Ill, definitely, keep, you, posted, Hoping, t...   \n",
       "...                                                   ...   \n",
       "223943  [true, But, wait, till, she, becomes, his, sec...   \n",
       "224019  [lol, you, have, to, let, the, little, one, sl...   \n",
       "224764  [Today, was, soooo, good, was, aweeesome, BBQ,...   \n",
       "224767  [Just, woke, up, School, today, Cant, wait, ti...   \n",
       "224768  [Aw, Its, okay, Just, wait, till, they, come, ...   \n",
       "\n",
       "                                             text_stemmed  \\\n",
       "195     [Yes, I, know, I, think, Ill, just, wait, for,...   \n",
       "323     [is, in, my, NetFlix, queue, I, should, receiv...   \n",
       "470     [I, wanna, watch, PPP, so, bad, But, my, dad, ...   \n",
       "532     [YAH, ohh, wait, Well, miss, you, Chuck, let, ...   \n",
       "536     [Ill, definitely, keep, you, posted, Hoping, t...   \n",
       "...                                                   ...   \n",
       "223943  [true, But, wait, till, she, becomes, his, sec...   \n",
       "224019  [lol, you, have, to, let, the, little, one, sl...   \n",
       "224764  [Today, wa, soooo, good, wa, aweeesome, BBQ, w...   \n",
       "224767  [Just, woke, up, School, today, Cant, wait, ti...   \n",
       "224768  [Aw, Its, okay, Just, wait, till, they, come, ...   \n",
       "\n",
       "                                                 text_neg  \\\n",
       "195     [Yes, I, know, I, think, Ill, just, wait, for,...   \n",
       "323     [is, in, my, NetFlix, queue, I, should, receiv...   \n",
       "470     [I, wanna, watch, PPP, so, bad, But, my, dad, ...   \n",
       "532     [YAH, ohh, wait, Well, miss, you, Chuck, let, ...   \n",
       "536     [Ill, definitely, keep, you, posted, Hoping, t...   \n",
       "...                                                   ...   \n",
       "223943  [true, But, wait, till, she, becomes, his, sec...   \n",
       "224019  [lol, you, have, to, let, the, little, one, sl...   \n",
       "224764  [Today, wa, soooo, good, wa, aweeesome, BBQ, w...   \n",
       "224767  [Just, woke, up, School, today, Cant, wait, ti...   \n",
       "224768  [Aw, Its, okay, Just, wait, till, they, come, ...   \n",
       "\n",
       "                                               text_final  \n",
       "195     Yes I know I think Ill just wait for tomorrow ...  \n",
       "323     is in my NetFlix queue I should receive the fi...  \n",
       "470     I wanna watch PPP so bad But my dad watching t...  \n",
       "532     YAH ohh wait Well miss you Chuck let take a mo...  \n",
       "536     Ill definitely keep you posted Hoping to go ov...  \n",
       "...                                                   ...  \n",
       "223943    true But wait till she becomes his second mommy  \n",
       "224019  lol you have to let the little one sleep You h...  \n",
       "224764  Today wa soooo good wa aweeesome BBQ wa so yum...  \n",
       "224767    Just woke up School today Cant wait til it over  \n",
       "224768  Aw Its okay Just wait till they come to Oz and...  \n",
       "\n",
       "[1025 rows x 13 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_final.loc[df_dev_final['text_final'].str.contains(' wait '),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = 'vip'\n",
    "# print(df_dev.loc[df_dev['text'].str.contains(word),:]['sentiment'].value_counts())\n",
    "# print(df_dev.loc[df_dev['text'].str.contains(word),:]['user'].value_counts())\n",
    "# print(df_dev.loc[df_dev['text'].str.contains(word),:]['user'].value_counts().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3664453306305709\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     28199\n",
      "           1       0.58      1.00      0.73     38686\n",
      "\n",
      "    accuracy                           0.58     66885\n",
      "   macro avg       0.29      0.50      0.37     66885\n",
      "weighted avg       0.33      0.58      0.42     66885\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\data_science_lab_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\envs\\data_science_lab_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda\\envs\\data_science_lab_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators = 150, \n",
    "    random_state = 42, \n",
    "    min_impurity_decrease = 0.0,\n",
    "    n_jobs=-6\n",
    ")\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "f1 = f1_score(y_test, rfc.predict(X_test),average='macro')\n",
    "report = classification_report(y_test, rfc.predict(X_test))\n",
    "\n",
    "print(f1)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7679078743418951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.68      0.72     28199\n",
      "           1       0.79      0.85      0.81     38686\n",
      "\n",
      "    accuracy                           0.78     66885\n",
      "   macro avg       0.77      0.76      0.77     66885\n",
      "weighted avg       0.78      0.78      0.78     66885\n",
      "\n",
      "[[19266  8933]\n",
      " [ 5962 32724]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "rfc = LinearSVC(random_state=70)\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "f1 = f1_score(y_test, rfc.predict(X_test),average='macro')\n",
    "report = classification_report(y_test, rfc.predict(X_test))\n",
    "confusion = confusion_matrix(y_test, rfc.predict(X_test))\n",
    "\n",
    "print(f1)\n",
    "print(report)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7023192652722203\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.65      0.65     28199\n",
      "           1       0.75      0.76      0.75     38686\n",
      "\n",
      "    accuracy                           0.71     66885\n",
      "   macro avg       0.70      0.70      0.70     66885\n",
      "weighted avg       0.71      0.71      0.71     66885\n",
      "\n",
      "[[18285  9914]\n",
      " [ 9464 29222]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "rfc = GaussianNB()\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "f1 = f1_score(y_test, rfc.predict(X_test),average='macro')\n",
    "report = classification_report(y_test, rfc.predict(X_test))\n",
    "confusion = confusion_matrix(y_test, rfc.predict(X_test))\n",
    "\n",
    "print(f1)\n",
    "print(report)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7335232472069035\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.63      0.68     28199\n",
      "           1       0.75      0.83      0.79     38686\n",
      "\n",
      "    accuracy                           0.75     66885\n",
      "   macro avg       0.74      0.73      0.73     66885\n",
      "weighted avg       0.75      0.75      0.74     66885\n",
      "\n",
      "[[17625 10574]\n",
      " [ 6392 32294]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "rfc = LogisticRegression(n_jobs=-4)\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "f1 = f1_score(y_test, rfc.predict(X_test),average='macro')\n",
    "report = classification_report(y_test, rfc.predict(X_test))\n",
    "confusion = confusion_matrix(y_test, rfc.predict(X_test))\n",
    "\n",
    "print(f1)\n",
    "print(report)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "\n",
    "params = {\n",
    "    'penalty' : ['l1','l2','elasticnet','none'],\n",
    "    'tol' : [1e-3,1e-4,1e-5],\n",
    "    'solver' : ['newton-cg','lbfgs','liblinear','sag','saga']\n",
    "}\n",
    "\n",
    "# gsCV = GridSearchCV()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.56999814\n",
      "Iteration 2, loss = 0.54440468\n",
      "Iteration 3, loss = 0.54240847\n",
      "Iteration 4, loss = 0.54047893\n",
      "Iteration 5, loss = 0.53869067\n",
      "Iteration 6, loss = 0.53682975\n",
      "Iteration 7, loss = 0.53520252\n",
      "Iteration 8, loss = 0.53349645\n",
      "Iteration 9, loss = 0.53177883\n",
      "Iteration 10, loss = 0.53044132\n",
      "Iteration 11, loss = 0.52913975\n",
      "Iteration 12, loss = 0.52780389\n",
      "Iteration 13, loss = 0.52658603\n",
      "Iteration 14, loss = 0.52547633\n",
      "Iteration 15, loss = 0.52441353\n",
      "Iteration 16, loss = 0.52312409\n",
      "Iteration 17, loss = 0.52223343\n",
      "Iteration 18, loss = 0.52109321\n",
      "Iteration 19, loss = 0.52009236\n",
      "Iteration 20, loss = 0.51897507\n",
      "Iteration 21, loss = 0.51800636\n",
      "Iteration 22, loss = 0.51728088\n",
      "Iteration 23, loss = 0.51633078\n",
      "Iteration 24, loss = 0.51520809\n",
      "Iteration 25, loss = 0.51445030\n",
      "Iteration 26, loss = 0.51337090\n",
      "Iteration 27, loss = 0.51238160\n",
      "Iteration 28, loss = 0.51141686\n",
      "Iteration 29, loss = 0.51072541\n",
      "Iteration 30, loss = 0.50988250\n",
      "Iteration 31, loss = 0.50899344\n",
      "Iteration 32, loss = 0.50826318\n",
      "Iteration 33, loss = 0.50726533\n",
      "Iteration 34, loss = 0.50649312\n",
      "Iteration 35, loss = 0.50581515\n",
      "Iteration 36, loss = 0.50500407\n",
      "Iteration 37, loss = 0.50396329\n",
      "Iteration 38, loss = 0.50337145\n",
      "Iteration 39, loss = 0.50265093\n",
      "Iteration 40, loss = 0.50173117\n",
      "Iteration 41, loss = 0.50110331\n",
      "Iteration 42, loss = 0.50056807\n",
      "Iteration 43, loss = 0.49948784\n",
      "Iteration 44, loss = 0.49934846\n",
      "Iteration 45, loss = 0.49822473\n",
      "Iteration 46, loss = 0.49775140\n",
      "Iteration 47, loss = 0.49704685\n",
      "Iteration 48, loss = 0.49640872\n",
      "Iteration 49, loss = 0.49572421\n",
      "Iteration 50, loss = 0.49530441\n",
      "Iteration 51, loss = 0.49450472\n",
      "Iteration 52, loss = 0.49386477\n",
      "Iteration 53, loss = 0.49323782\n",
      "Iteration 54, loss = 0.49261984\n",
      "Iteration 55, loss = 0.49214881\n",
      "Iteration 56, loss = 0.49174902\n",
      "Iteration 57, loss = 0.49135601\n",
      "Iteration 58, loss = 0.49069099\n",
      "Iteration 59, loss = 0.49024220\n",
      "Iteration 60, loss = 0.48964359\n",
      "Iteration 61, loss = 0.48927331\n",
      "Iteration 62, loss = 0.48876362\n",
      "Iteration 63, loss = 0.48839561\n",
      "Iteration 64, loss = 0.48792037\n",
      "Iteration 65, loss = 0.48742557\n",
      "Iteration 66, loss = 0.48687435\n",
      "Iteration 67, loss = 0.48621394\n",
      "Iteration 68, loss = 0.48639992\n",
      "Iteration 69, loss = 0.48540499\n",
      "Iteration 70, loss = 0.48512978\n",
      "Iteration 71, loss = 0.48463025\n",
      "Iteration 72, loss = 0.48418264\n",
      "Iteration 73, loss = 0.48418107\n",
      "Iteration 74, loss = 0.48368838\n",
      "Iteration 75, loss = 0.48330779\n",
      "Iteration 76, loss = 0.48297823\n",
      "Iteration 77, loss = 0.48238191\n",
      "Iteration 78, loss = 0.48193929\n",
      "Iteration 79, loss = 0.48163352\n",
      "Iteration 80, loss = 0.48102396\n",
      "Iteration 81, loss = 0.48115805\n",
      "Iteration 82, loss = 0.48074661\n",
      "Iteration 83, loss = 0.48007899\n",
      "Iteration 84, loss = 0.47997423\n",
      "Iteration 85, loss = 0.47947263\n",
      "Iteration 86, loss = 0.47949214\n",
      "Iteration 87, loss = 0.47882485\n",
      "Iteration 88, loss = 0.47839626\n",
      "Iteration 89, loss = 0.47808161\n",
      "Iteration 90, loss = 0.47824514\n",
      "Iteration 91, loss = 0.47746157\n",
      "Iteration 92, loss = 0.47709259\n",
      "Iteration 93, loss = 0.47679371\n",
      "Iteration 94, loss = 0.47687866\n",
      "Iteration 95, loss = 0.47621285\n",
      "Iteration 96, loss = 0.47587529\n",
      "Iteration 97, loss = 0.47578932\n",
      "Iteration 98, loss = 0.47547553\n",
      "Iteration 99, loss = 0.47574092\n",
      "Iteration 100, loss = 0.47479792\n",
      "Iteration 101, loss = 0.47467059\n",
      "Iteration 102, loss = 0.47432237\n",
      "Iteration 103, loss = 0.47391281\n",
      "Iteration 104, loss = 0.47381527\n",
      "Iteration 105, loss = 0.47361481\n",
      "Iteration 106, loss = 0.47362390\n",
      "Iteration 107, loss = 0.47327003\n",
      "Iteration 108, loss = 0.47320742\n",
      "Iteration 109, loss = 0.47240850\n",
      "Iteration 110, loss = 0.47232742\n",
      "Iteration 111, loss = 0.47228565\n",
      "Iteration 112, loss = 0.47187560\n",
      "Iteration 113, loss = 0.47155186\n",
      "Iteration 114, loss = 0.47121665\n",
      "Iteration 115, loss = 0.47099586\n",
      "Iteration 116, loss = 0.47089477\n",
      "Iteration 117, loss = 0.47038668\n",
      "Iteration 118, loss = 0.47035444\n",
      "Iteration 119, loss = 0.47018064\n",
      "Iteration 120, loss = 0.46998908\n",
      "Iteration 121, loss = 0.46973086\n",
      "Iteration 122, loss = 0.46978718\n",
      "Iteration 123, loss = 0.46903292\n",
      "Iteration 124, loss = 0.46947611\n",
      "Iteration 125, loss = 0.46928190\n",
      "Iteration 126, loss = 0.46856363\n",
      "Iteration 127, loss = 0.46856651\n",
      "Iteration 128, loss = 0.46835439\n",
      "Iteration 129, loss = 0.46828559\n",
      "Iteration 130, loss = 0.46838590\n",
      "Iteration 131, loss = 0.46764582\n",
      "Iteration 132, loss = 0.46744351\n",
      "Iteration 133, loss = 0.46726611\n",
      "Iteration 134, loss = 0.46728538\n",
      "Iteration 135, loss = 0.46695136\n",
      "Iteration 136, loss = 0.46689397\n",
      "Iteration 137, loss = 0.46647098\n",
      "Iteration 138, loss = 0.46636817\n",
      "Iteration 139, loss = 0.46621752\n",
      "Iteration 140, loss = 0.46572121\n",
      "Iteration 141, loss = 0.46616133\n",
      "Iteration 142, loss = 0.46560096\n",
      "Iteration 143, loss = 0.46553456\n",
      "Iteration 144, loss = 0.46526015\n",
      "Iteration 145, loss = 0.46514600\n",
      "Iteration 146, loss = 0.46525145\n",
      "Iteration 147, loss = 0.46524189\n",
      "Iteration 148, loss = 0.46451600\n",
      "Iteration 149, loss = 0.46472063\n",
      "Iteration 150, loss = 0.46466727\n",
      "Iteration 151, loss = 0.46396733\n",
      "Iteration 152, loss = 0.46419255\n",
      "Iteration 153, loss = 0.46394523\n",
      "Iteration 154, loss = 0.46388715\n",
      "Iteration 155, loss = 0.46335796\n",
      "Iteration 156, loss = 0.46325564\n",
      "Iteration 157, loss = 0.46320558\n",
      "Iteration 158, loss = 0.46311735\n",
      "Iteration 159, loss = 0.46296980\n",
      "Iteration 160, loss = 0.46329244\n",
      "Iteration 161, loss = 0.46278070\n",
      "Iteration 162, loss = 0.46243918\n",
      "Iteration 163, loss = 0.46245430\n",
      "Iteration 164, loss = 0.46219095\n",
      "Iteration 165, loss = 0.46205871\n",
      "Iteration 166, loss = 0.46198166\n",
      "Iteration 167, loss = 0.46185078\n",
      "Iteration 168, loss = 0.46189362\n",
      "Iteration 169, loss = 0.46150283\n",
      "Iteration 170, loss = 0.46101131\n",
      "Iteration 171, loss = 0.46120805\n",
      "Iteration 172, loss = 0.46126215\n",
      "Iteration 173, loss = 0.46087396\n",
      "Iteration 174, loss = 0.46096510\n",
      "Iteration 175, loss = 0.46046987\n",
      "Iteration 176, loss = 0.46046666\n",
      "Iteration 177, loss = 0.46044936\n",
      "Iteration 178, loss = 0.46056235\n",
      "Iteration 179, loss = 0.46025065\n",
      "Iteration 180, loss = 0.46014711\n",
      "Iteration 181, loss = 0.45987615\n",
      "Iteration 182, loss = 0.45985106\n",
      "Iteration 183, loss = 0.45978112\n",
      "Iteration 184, loss = 0.45944542\n",
      "Iteration 185, loss = 0.45967315\n",
      "Iteration 186, loss = 0.45940383\n",
      "Iteration 187, loss = 0.45914220\n",
      "Iteration 188, loss = 0.45924586\n",
      "Iteration 189, loss = 0.45891563\n",
      "Iteration 190, loss = 0.45919264\n",
      "Iteration 191, loss = 0.45883107\n",
      "Iteration 192, loss = 0.45859420\n",
      "Iteration 193, loss = 0.45898136\n",
      "Iteration 194, loss = 0.45840057\n",
      "Iteration 195, loss = 0.45864569\n",
      "Iteration 196, loss = 0.45803618\n",
      "Iteration 197, loss = 0.45761392\n",
      "Iteration 198, loss = 0.45785852\n",
      "Iteration 199, loss = 0.45787796\n",
      "Iteration 200, loss = 0.45752831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\data_science_lab_2\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6693705135515255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.52      0.59     28451\n",
      "           1       0.70      0.81      0.75     39048\n",
      "\n",
      "    accuracy                           0.69     67499\n",
      "   macro avg       0.68      0.67      0.67     67499\n",
      "weighted avg       0.69      0.69      0.68     67499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "rfc = MLPClassifier(verbose = True)\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "f1 = f1_score(y_test, rfc.predict(X_test),average='macro')\n",
    "report = classification_report(y_test, rfc.predict(X_test))\n",
    "\n",
    "print(f1)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7041128433588708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.52      0.62     28199\n",
      "           1       0.71      0.89      0.79     38686\n",
      "\n",
      "    accuracy                           0.73     66885\n",
      "   macro avg       0.74      0.70      0.70     66885\n",
      "weighted avg       0.74      0.73      0.72     66885\n",
      "\n",
      "[[14526 13673]\n",
      " [ 4388 34298]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "rfc = SGDClassifier(n_jobs=-5, random_state=50)\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "f1 = f1_score(y_test, rfc.predict(X_test),average='macro')\n",
    "report = classification_report(y_test, rfc.predict(X_test))\n",
    "confusion = confusion_matrix(y_test, rfc.predict(X_test))\n",
    "\n",
    "print(f1)\n",
    "print(report)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "rfc = SVC(random_state=50)\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "f1 = f1_score(y_test, rfc.predict(X_test),average='macro')\n",
    "report = classification_report(y_test, rfc.predict(X_test))\n",
    "confusion = confusion_matrix(y_test, rfc.predict(X_test))\n",
    "\n",
    "print(f1)\n",
    "print(report)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\edo_c\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\edo_c\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "df_eval['text_token'] = df_eval['text'].str.split()\n",
    "\n",
    "df_eval['text_noAt'] = df_eval['text_token'].apply(lambda x : [i for i in x if not(i.startswith('@'))])\n",
    "\n",
    "df_eval['text_noAmpQuot'] = df_eval['text_noAt']\\\n",
    "    .apply(lambda x : [i for i in x if '&amp' not in i])\\\n",
    "    .apply(lambda x : [i for i in x if '&quot' not in i])\n",
    "\n",
    "import string as py_string\n",
    "df_eval['text_noPunct'] = df_eval['text_noAmpQuot']\\\n",
    "    .apply(lambda x : [i.translate(str.maketrans('', '', py_string.punctuation)) for i in x])\\\n",
    "    .apply(lambda x : [i for i in x if i != ''])\n",
    "\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk_download('wordnet')\n",
    "nltk_download('omw-1.4')\n",
    "\n",
    "nltk_stemmer = PorterStemmer()\n",
    "# nltk_stemmer = LancasterStemmer()\n",
    "# nltk_stemmer = SnowballStemmer('english')\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "# from nltk.corpus import wordnet\n",
    "\n",
    "df_eval['text_stemmed'] = df_eval['text_noPunct'].apply(lambda x : [nltk_stemmer.stem(word) for word in x])\n",
    "# df_eval['text_stemmed'] = df_eval['text_noPunct'].apply(lambda x : [nltk_lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "df_eval['text_neg'] = df_eval['text_stemmed'].apply(lambda x : mark_negation(x))\n",
    "\n",
    "## Model creation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df_eval_final = df_eval\n",
    "df_eval_final['text_final'] = df_eval['text_neg'].apply(lambda x : ' '.join(x))\n",
    "# df_eval_final['text_final'] = df_eval_final['text']\n",
    "\n",
    "# stopwords_to_use = [nltk_stemmer.stem(word) for word in stopwords_list_gen()]\n",
    "stopwords_to_use = [nltk_stemmer.stem(word) for word in stopwords_all_list]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    # stop_words = stopwords_to_use,\n",
    "    # stop_words = 'english',\n",
    "    stop_words = None,\n",
    "    binary=True, \n",
    "    use_idf=True, \n",
    "    norm='l2',\n",
    "    smooth_idf=True\n",
    ")\n",
    "\n",
    "wpm = vectorizer.fit_transform(df_eval_final['text_final'])\n",
    "\n",
    "N = 2000\n",
    "\n",
    "word_freq = pd.Series(\n",
    "    data = np.asarray(wpm.sum(axis=0)).squeeze(),\n",
    "    index = vectorizer.get_feature_names_out()\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "word_freq = word_freq[:N]\n",
    "\n",
    "word_ind = [w in word_freq.index for w in vectorizer.get_feature_names_out()]\n",
    "\n",
    "words_df = pd.DataFrame(\n",
    "    data = wpm[:,word_ind].toarray(),\n",
    "    columns = vectorizer.get_feature_names_out()[word_ind],\n",
    "    index = df_eval.index\n",
    ").add_prefix('word_')\n",
    "\n",
    "X = words_df.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74999, 12)\n",
      "(74999, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(df_eval.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "rfc = LinearSVC(random_state=50)\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred,columns=['Predicted']).to_csv(\"output.csv\",index_label=\"Id\", header=[\"Predicted\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd8ee3559d665fee903f84f74f9742602cb00cb47768a52cae0fe6e115d1a823"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('data_science_lab_2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
