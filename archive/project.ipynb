{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('project_dataset.zip') as zipped_file:\n",
    "    zipped_file.extractall(Path.cwd()/'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'development.csv')\n",
    "df_eval = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'evaluation.csv')\n",
    "df = df_dev.append(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment                                                    1\n",
       "ids                                                 1969463473\n",
       "date                              Sat May 30 00:06:54 PDT 2009\n",
       "flag                                                  NO_QUERY\n",
       "user                                                 kamiNcali\n",
       "text         @ladyspeaker yes! you gave me a ff shout out r...\n",
       "Name: 219050, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.iloc[219050,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1833972543</td>\n",
       "      <td>Mon May 18 01:08:27 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Killandra</td>\n",
       "      <td>@MissBianca76 Yes, talking helps a lot.. going...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1980318193</td>\n",
       "      <td>Sun May 31 06:23:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>IMlisacowan</td>\n",
       "      <td>SUNSHINE. livingg itttt. imma lie on the grass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1994409198</td>\n",
       "      <td>Mon Jun 01 11:52:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>yaseminx3</td>\n",
       "      <td>@PleaseBeMine Something for your iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1824749377</td>\n",
       "      <td>Sun May 17 02:45:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>no_surprises</td>\n",
       "      <td>@GabrielSaporta couldn't get in to the after p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2001199113</td>\n",
       "      <td>Tue Jun 02 00:08:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Rhi_ShortStack</td>\n",
       "      <td>@bradiewebbstack awww is andy being mean again...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment         ids                          date      flag  \\\n",
       "0          1  1833972543  Mon May 18 01:08:27 PDT 2009  NO_QUERY   \n",
       "1          1  1980318193  Sun May 31 06:23:17 PDT 2009  NO_QUERY   \n",
       "2          1  1994409198  Mon Jun 01 11:52:54 PDT 2009  NO_QUERY   \n",
       "3          0  1824749377  Sun May 17 02:45:34 PDT 2009  NO_QUERY   \n",
       "4          0  2001199113  Tue Jun 02 00:08:07 PDT 2009  NO_QUERY   \n",
       "\n",
       "             user                                               text  \n",
       "0       Killandra  @MissBianca76 Yes, talking helps a lot.. going...  \n",
       "1     IMlisacowan  SUNSHINE. livingg itttt. imma lie on the grass...  \n",
       "2       yaseminx3           @PleaseBeMine Something for your iphone   \n",
       "3    no_surprises  @GabrielSaporta couldn't get in to the after p...  \n",
       "4  Rhi_ShortStack  @bradiewebbstack awww is andy being mean again...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 224994 entries, 0 to 224993\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   sentiment  224994 non-null  int64 \n",
      " 1   ids        224994 non-null  int64 \n",
      " 2   date       224994 non-null  object\n",
      " 3   flag       224994 non-null  object\n",
      " 4   user       224994 non-null  object\n",
      " 5   text       224994 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 10.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_dev.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    True\n",
       "ids          True\n",
       "date         True\n",
       "flag         True\n",
       "user         True\n",
       "text         True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.notna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No column has empty cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    130157\n",
       "0     94837\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more comments classified as positive (130157) than as negative (94837)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Date' analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Mon May 18 01:08:27 PDT 2009\n",
       "1    Sun May 31 06:23:17 PDT 2009\n",
       "2    Mon Jun 01 11:52:54 PDT 2009\n",
       "3    Sun May 17 02:45:34 PDT 2009\n",
       "4    Tue Jun 02 00:08:07 PDT 2009\n",
       "Name: date, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PDT    224994\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['date'].astype('string').str.split(' ').apply(lambda x : x[-2]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only timezone in the dataset is PDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009    224994\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['date'].astype('string').str.split(' ').apply(lambda x : x[-1]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tweets are from year 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['date_mod'] = df_dev['date'].\\\n",
    "    astype('string')\\\n",
    "    .str.split(' ')\\\n",
    "    .apply(lambda x : ' '.join([x[i] for i in [1,2,3,5]]))\\\n",
    "    .pipe(pd.to_datetime)\n",
    "\n",
    "df_eval['date_mod'] = df_dev['date'].\\\n",
    "    astype('string')\\\n",
    "    .str.split(' ')\\\n",
    "    .apply(lambda x : ' '.join([x[i] for i in [1,2,3,5]]))\\\n",
    "    .pipe(pd.to_datetime)\n",
    "\n",
    "# df_dev[['date','date_mod']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2009-05-18 01:08:27\n",
       "1        2009-05-31 06:23:17\n",
       "2        2009-06-01 11:52:54\n",
       "3        2009-05-17 02:45:34\n",
       "4        2009-06-02 00:08:07\n",
       "                 ...        \n",
       "224989   2009-06-20 20:36:48\n",
       "224990   2009-06-01 01:25:45\n",
       "224991   2009-06-01 06:38:10\n",
       "224992   2009-06-19 08:51:56\n",
       "224993   2009-06-03 06:00:29\n",
       "Name: date_mod, Length: 224994, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['date_mod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2009-04-06 22:19:57'), Timestamp('2009-06-25 10:28:28'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['date_mod'].min(), df_dev['date_mod'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2009-05-18 01:08:27\n",
       "1       2009-05-31 06:23:17\n",
       "2       2009-06-01 11:52:54\n",
       "3       2009-05-17 02:45:34\n",
       "4       2009-06-02 00:08:07\n",
       "                ...        \n",
       "74994   2009-05-10 05:43:24\n",
       "74995   2009-06-06 01:43:47\n",
       "74996   2009-05-14 00:29:34\n",
       "74997   2009-04-19 05:48:44\n",
       "74998   2009-06-01 04:32:25\n",
       "Name: date_mod, Length: 74999, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval['date_mod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2009-04-06 22:20:03'), Timestamp('2009-06-25 10:28:28'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval['date_mod'].min(), df_eval['date_mod'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets were written between 2009-04-06 22:19:57 and 2009-06-25 10:28:28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['date'] = df_dev['date_mod']\n",
    "df_dev.drop('date_mod',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'flag' analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NO_QUERY    224994\n",
       "Name: flag, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NO_QUERY    74999\n",
       "Name: flag, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval['flag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unique value for 'flag' is 'NO_QUERY', so this field can be discarded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev.drop('flag',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'user' analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency of positive or negative for user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10647"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['user'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10647"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval['user'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10647"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['user'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lost_dog           549\n",
       "webwoke            345\n",
       "tweetpet           310\n",
       "SallytheShizzle    281\n",
       "VioletsCRUK        279\n",
       "                  ... \n",
       "strangetymes        15\n",
       "bianckikay          15\n",
       "linalrae            15\n",
       "iSpunk              15\n",
       "bdothill            15\n",
       "Name: user, Length: 10647, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['user'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='sentiment', ylabel='Count'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "average_user_sentiment = df_dev.groupby('user')['sentiment'].mean()\n",
    "average_user_sentiment = pd.DataFrame(average_user_sentiment).reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "sns.histplot(data=average_user_sentiment, x='sentiment', bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many users that made almost all tweets classified as positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet for 'user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lost_dog           412\n",
       "webwoke            259\n",
       "tweetpet           232\n",
       "SallytheShizzle    211\n",
       "VioletsCRUK        209\n",
       "mcraddictal        207\n",
       "tsarnick           186\n",
       "what_bugs_u        184\n",
       "Karen230683        178\n",
       "DarkPiano          177\n",
       "SongoftheOss       170\n",
       "Jayme1988          169\n",
       "keza34             164\n",
       "ramdomthoughts     162\n",
       "shanajaca          160\n",
       "wowlew             159\n",
       "TraceyHewins       158\n",
       "nuttychris         158\n",
       "thisgoeshere       155\n",
       "Spidersamm         154\n",
       "StDAY              151\n",
       "felicityfuller     146\n",
       "Dogbook            144\n",
       "_magic8ball        142\n",
       "Djalfy             136\n",
       "torilovesbradie    136\n",
       "Dutchrudder        136\n",
       "twebbstack         135\n",
       "Quimo              135\n",
       "Broooooke_         134\n",
       "Name: user, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['user'].value_counts().head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10647 distinct users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15,5))\n",
    "# user_value_counts = pd.DataFrame(df_dev['user'].value_counts())[:100].reset_index()\n",
    "# sns.barplot(data=user_value_counts, x='index', y='user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few users made many more tweets than others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main \"twetterers\" analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Killandra         0.073697\n",
       "IMlisacowan       0.053146\n",
       "yaseminx3         0.051634\n",
       "no_surprises      0.041548\n",
       "Rhi_ShortStack    0.039583\n",
       "                    ...   \n",
       "sanasaleem        0.042040\n",
       "skweeds           0.071574\n",
       "samozzy           0.069901\n",
       "hEATHER_nVA       0.067190\n",
       "Rokkster          0.047421\n",
       "Length: 10647, dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvect = CountVectorizer()\n",
    "\n",
    "df_test = df.copy(deep=True)\n",
    "\n",
    "X_count = countvect.fit_transform(df_test['text'])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "user_similarity = {}\n",
    "for user in df_test['user'].unique():\n",
    "    X_count_user = X_count[df_test['user'] == user,:]\n",
    "    X_similarity_user = cosine_similarity(X_count_user)\n",
    "    np.fill_diagonal(X_similarity_user,np.nan)\n",
    "    user_similarity[user] = np.nanmean(X_similarity_user)\n",
    "\n",
    "user_similarity = pd.Series(user_similarity)\n",
    "user_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_similarity.to_csv('internal_similarity_users.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BecReedman       0.395609\n",
       "Kyle270          0.388842\n",
       "chlon2005        0.385649\n",
       "isdown           0.379656\n",
       "Twitilicious     0.375937\n",
       "tns_fan          0.373911\n",
       "TimmyGrunt       0.360351\n",
       "iwishiwas        0.358073\n",
       "SusanCosmos      0.352087\n",
       "momspark         0.345954\n",
       "LunaCafe         0.345529\n",
       "khrystar         0.340303\n",
       "TakeAWish        0.338444\n",
       "ThatsTwit        0.337704\n",
       "RandomPenguin    0.337658\n",
       "ChupaCabras69    0.337178\n",
       "marianaguidil    0.334181\n",
       "jovlynlouise     0.333932\n",
       "WHEREonPre       0.331830\n",
       "MariamSales      0.331659\n",
       "dtype: float64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_similarity.sort_values(ascending=False)[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689       1 exam done...went ok - headache=MASSIVE OW  h...\n",
       "25527     loves her Liberty print bunny her mummy got he...\n",
       "49168     halfway through reading dissertation through.....\n",
       "56806     one hour late-bad plumbers  http://tinyurl.com...\n",
       "59270     is going home tomorrow  http://tinyurl.com/r5rz9z\n",
       "67903     halfway through reading dissertation through.....\n",
       "76299     carrots and hummous mmmmmmm  http://tinyurl.co...\n",
       "91763      ah pubmed...so useful  http://tinyurl.com/rywqpp\n",
       "93111       is off to meet Vicky  http://tinyurl.com/ccmmsv\n",
       "117555    one hour late-bad plumbers  http://tinyurl.com...\n",
       "194778     ah pubmed...so useful  http://tinyurl.com/rywqpp\n",
       "27191     word count 2500/3000....SO CLOSE  http://tinyu...\n",
       "27248     1 exam done...went ok - headache=MASSIVE OW  h...\n",
       "27751     hates this stupid dissertation and can't do it...\n",
       "61204     one hour late-bad plumbers  http://tinyurl.com...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = 'MariamSales'\n",
    "df.loc[df['user']==user]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 'lost_dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>1686517168</td>\n",
       "      <td>2009-05-03 05:45:51</td>\n",
       "      <td>lost_dog</td>\n",
       "      <td>@bthenextstep I am lost. Please help me find a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>0</td>\n",
       "      <td>2322687735</td>\n",
       "      <td>2009-06-24 23:04:36</td>\n",
       "      <td>lost_dog</td>\n",
       "      <td>@marissa_in_cali I am lost. Please help me fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>0</td>\n",
       "      <td>1984183693</td>\n",
       "      <td>2009-05-31 14:36:16</td>\n",
       "      <td>lost_dog</td>\n",
       "      <td>@that_much I am lost. Please help me find a go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>0</td>\n",
       "      <td>1975991079</td>\n",
       "      <td>2009-05-30 16:20:57</td>\n",
       "      <td>lost_dog</td>\n",
       "      <td>@dcunited I am lost. Please help me find a goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>0</td>\n",
       "      <td>2220606046</td>\n",
       "      <td>2009-06-18 03:30:50</td>\n",
       "      <td>lost_dog</td>\n",
       "      <td>@aiimee_x I am lost. Please help me find a goo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment         ids                date      user  \\\n",
       "30            0  1686517168 2009-05-03 05:45:51  lost_dog   \n",
       "580           0  2322687735 2009-06-24 23:04:36  lost_dog   \n",
       "785           0  1984183693 2009-05-31 14:36:16  lost_dog   \n",
       "1536          0  1975991079 2009-05-30 16:20:57  lost_dog   \n",
       "1546          0  2220606046 2009-06-18 03:30:50  lost_dog   \n",
       "\n",
       "                                                   text  \n",
       "30    @bthenextstep I am lost. Please help me find a...  \n",
       "580   @marissa_in_cali I am lost. Please help me fin...  \n",
       "785   @that_much I am lost. Please help me find a go...  \n",
       "1536  @dcunited I am lost. Please help me find a goo...  \n",
       "1546  @aiimee_x I am lost. Please help me find a goo...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[df_dev['user'] == 'lost_dog'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the tweets made by 'lost_dog' follow the same pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    412\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[df_dev['user'] == 'lost_dog']['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All its tweets are classified as negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 'webwoke'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    196\n",
      "Name: sentiment, dtype: int64\n",
      "1    63\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_dev[df_dev['user'] == 'webwoke'].loc[df_dev['text'].str.contains('drop')]['sentiment'].value_counts())\n",
    "print(df_dev[df_dev['user'] == 'webwoke'].loc[df_dev['text'].str.contains('move up')]['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the the tweets made by 'webwoke' follow the same pattern; if the tweet contains the word 'drop', it is classified as negative, instead if it contains the expression 'move up' it is classified as positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 'tweetpet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0    232\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_dev[df_dev['user'] == 'tweetpet']['text'].str.contains('Clean Me').any())\n",
    "print(df_dev[df_dev['user'] == 'tweetpet']['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the tweets made by 'tweetpet' follow the same pattern, and contain 'Clean Me' as text.  \n",
    "All the tweets are classified as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. SallytheShizzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0</td>\n",
       "      <td>1956695119</td>\n",
       "      <td>2009-05-28 22:27:44</td>\n",
       "      <td>SallytheShizzle</td>\n",
       "      <td>@ColorMeKelly -sigh- we won't get to hear for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>0</td>\n",
       "      <td>2015645364</td>\n",
       "      <td>2009-06-03 05:11:07</td>\n",
       "      <td>SallytheShizzle</td>\n",
       "      <td>@OfficialAS  oh well. It's not the hol house t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>1</td>\n",
       "      <td>2015042588</td>\n",
       "      <td>2009-06-03 03:31:13</td>\n",
       "      <td>SallytheShizzle</td>\n",
       "      <td>@MAGGIECHICKEN meh same diff. Just still with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>0</td>\n",
       "      <td>1988723315</td>\n",
       "      <td>2009-05-31 23:11:16</td>\n",
       "      <td>SallytheShizzle</td>\n",
       "      <td>@OfficialAS sorry  I've been TRYING to finish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>1</td>\n",
       "      <td>2049858625</td>\n",
       "      <td>2009-06-05 17:37:39</td>\n",
       "      <td>SallytheShizzle</td>\n",
       "      <td>#kevinjonas is the best!  happy #kevinjonas day!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment         ids                date             user  \\\n",
       "485           0  1956695119 2009-05-28 22:27:44  SallytheShizzle   \n",
       "509           0  2015645364 2009-06-03 05:11:07  SallytheShizzle   \n",
       "2130          1  2015042588 2009-06-03 03:31:13  SallytheShizzle   \n",
       "2522          0  1988723315 2009-05-31 23:11:16  SallytheShizzle   \n",
       "2894          1  2049858625 2009-06-05 17:37:39  SallytheShizzle   \n",
       "\n",
       "                                                   text  \n",
       "485   @ColorMeKelly -sigh- we won't get to hear for ...  \n",
       "509   @OfficialAS  oh well. It's not the hol house t...  \n",
       "2130  @MAGGIECHICKEN meh same diff. Just still with ...  \n",
       "2522  @OfficialAS sorry  I've been TRYING to finish ...  \n",
       "2894  #kevinjonas is the best!  happy #kevinjonas day!   "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[df_dev['user'] == 'SallytheShizzle'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first three users by number of tweets seem to be bots, while this one doesn't. I'll stop here this punctual analysis.  \n",
    "These users may be dropped during the preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. StDAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>0</td>\n",
       "      <td>2237802529</td>\n",
       "      <td>2009-06-19 06:23:00</td>\n",
       "      <td>Spidersamm</td>\n",
       "      <td>rrrrrrrr im going to bed cos im annoyed now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0</td>\n",
       "      <td>1966109943</td>\n",
       "      <td>2009-05-29 17:08:33</td>\n",
       "      <td>Spidersamm</td>\n",
       "      <td>@staaceeyy weeooow, i feel fat.... i remember ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>0</td>\n",
       "      <td>2175595270</td>\n",
       "      <td>2009-06-15 01:26:07</td>\n",
       "      <td>Spidersamm</td>\n",
       "      <td>heart burn  thanks chicken strips, hahah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>0</td>\n",
       "      <td>1956856143</td>\n",
       "      <td>2009-05-28 22:52:00</td>\n",
       "      <td>Spidersamm</td>\n",
       "      <td>@leonblair  i wanna be there  damn for living ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847</th>\n",
       "      <td>1</td>\n",
       "      <td>1693789441</td>\n",
       "      <td>2009-05-03 23:47:20</td>\n",
       "      <td>Spidersamm</td>\n",
       "      <td>@HelloLizzi  om nom nom  he looks so adorable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment         ids                date        user  \\\n",
       "994           0  2237802529 2009-06-19 06:23:00  Spidersamm   \n",
       "1717          0  1966109943 2009-05-29 17:08:33  Spidersamm   \n",
       "2466          0  2175595270 2009-06-15 01:26:07  Spidersamm   \n",
       "3483          0  1956856143 2009-05-28 22:52:00  Spidersamm   \n",
       "3847          1  1693789441 2009-05-03 23:47:20  Spidersamm   \n",
       "\n",
       "                                                   text  \n",
       "994        rrrrrrrr im going to bed cos im annoyed now   \n",
       "1717  @staaceeyy weeooow, i feel fat.... i remember ...  \n",
       "2466           heart burn  thanks chicken strips, hahah  \n",
       "3483  @leonblair  i wanna be there  damn for living ...  \n",
       "3847      @HelloLizzi  om nom nom  he looks so adorable  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[df_dev['user'] == 'Spidersamm'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'text' analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is created using sklearn's TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",  \n",
    "    binary=True, \n",
    "    use_idf=True, \n",
    "    norm='l2',\n",
    "    smooth_idf=True\n",
    ")\n",
    "\n",
    "wpm = vectorizer.fit_transform(df_dev['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "word_freq = pd.Series(\n",
    "    data = np.asarray(wpm.sum(axis=0)).squeeze(),\n",
    "    index = vectorizer.get_feature_names_out()\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "word_freq = word_freq[:N]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15,5))\n",
    "# sns.histplot(data=pd.DataFrame(word_freq,columns=['word_freq']),x='word_freq')\n",
    "# word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ind = [w in word_freq.index for w in vectorizer.get_feature_names_out()]\n",
    "\n",
    "words_df = pd.DataFrame(\n",
    "    data = wpm[:,word_ind].toarray(),\n",
    "    columns = vectorizer.get_feature_names_out()[word_ind],\n",
    "    index = df_dev.index\n",
    ").add_prefix('word_')\n",
    "\n",
    "# words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = words_df.values\n",
    "y = df_dev['sentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, train_size=0.7, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7088160731632195"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "reg = RandomForestClassifier(100, random_state=42, n_jobs=-2)\n",
    "reg.fit(X_train, y_train)\n",
    "f1_score(y_test, reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: best RandomForestClassifier hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# param_grid = {\n",
    "#     \"n_estimators\": [50,75,100],\n",
    "#     \"criterion\": [\"gini\", \"entropy\"],\n",
    "#     \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "#     \"random_state\": [42],\n",
    "#     \"n_jobs\": [-1],\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50,75],\n",
    "    \"criterion\": [\"entropy\"],\n",
    "    \"max_features\": [\"auto\"],\n",
    "    \"random_state\": [42],\n",
    "    \"n_jobs\": [-2],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    RandomForestClassifier(), \n",
    "    param_grid, \n",
    "    scoring=\"f1_macro\", \n",
    "    # n_jobs=-1, \n",
    "    cv=5,\n",
    "    verbose=3\n",
    ")\n",
    "# gs.fit(X, y)\n",
    "\n",
    "# print(gs.best_score_)\n",
    "# print(gs.best_estimator_)\n",
    "# print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinations of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",  \n",
    "    binary=True, \n",
    "    use_idf=True, \n",
    "    norm='l2',\n",
    "    smooth_idf=True\n",
    ")\n",
    "\n",
    "wpm = vectorizer.fit_transform(df_dev['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: removal of '@words' in 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_noAt = df_dev.copy()\n",
    "df_dev_noAt['text'] = df_dev['text'].str.split().apply(lambda x : ' '.join([i for i in x if not(i.startswith('@'))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",  \n",
    "    binary=True, \n",
    "    use_idf=True, \n",
    "    norm='l2',\n",
    "    smooth_idf=True\n",
    ")\n",
    "\n",
    "wpm = vectorizer.fit_transform(df_dev_noAt['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "word_freq = pd.Series(\n",
    "    data = np.asarray(wpm.sum(axis=0)).squeeze(),\n",
    "    index = vectorizer.get_feature_names_out()\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "word_freq = word_freq[:N]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15,5))\n",
    "# sns.histplot(data=pd.DataFrame(word_freq,columns=['word_freq']),x='word_freq')\n",
    "# word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ind = [w in word_freq.index for w in vectorizer.get_feature_names_out()]\n",
    "\n",
    "words_df = pd.DataFrame(\n",
    "    data = wpm[:,word_ind].toarray(),\n",
    "    columns = vectorizer.get_feature_names_out()[word_ind],\n",
    "    index = df_dev.index\n",
    ").add_prefix('word_')\n",
    "\n",
    "# words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = words_df.values\n",
    "y = df_dev['sentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, train_size=0.7, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7107991667455733"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "reg = RandomForestClassifier(100, random_state=42, n_jobs=-2)\n",
    "reg.fit(X_train, y_train)\n",
    "f1_score(y_test, reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result does not change, the words are the same. It may have some consequences if we apply stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer\n",
    "import nltk.sentiment.util as nsu\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "nltk_test = df_dev.copy()\n",
    "nltk_test['text'] = nltk_test['text'].str.split().apply(lambda x : [i for i in x if not(i.startswith('@'))])\n",
    "\n",
    "X = nltk_test['text'].values\n",
    "y = nltk_test['sentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, train_size=0.7, random_state=50)\n",
    "\n",
    "\n",
    "nltk_sa = SentimentAnalyzer()\n",
    "\n",
    "X_train_mod = nltk_sa.all_words([nsu.mark_negation(doc,shallow=True) for doc in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_feats = nltk_sa.unigram_word_feats(X_train_mod, top_n=150)\n",
    "\n",
    "nltk_sa.add_feat_extractor(nsu.extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = nltk_sa.apply_features(tuple([(x_t, y_t) for (x_t, y_t) in zip(X_train,y_train)]), labeled=True)\n",
    "test_set = nltk_sa.apply_features(tuple([(x_t, y_t) for (x_t, y_t) in zip(X_test,y_test)]), labeled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "trainer = NaiveBayesClassifier.train\n",
    "\n",
    "classifier = nltk_sa.train(trainer, training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.6568689906517133\n",
      "F-measure [0]: 0.4656838996931737\n",
      "F-measure [1]: 0.7472913552498064\n",
      "Precision [0]: 0.6775644468313641\n",
      "Precision [1]: 0.6510084976142045\n",
      "Recall [0]: 0.35475027239815826\n",
      "Recall [1]: 0.8769975414874002\n"
     ]
    }
   ],
   "source": [
    "for key,value in sorted(nltk_sa.evaluate(test_set).items()):\n",
    "    print(f'{key}: {value}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd8ee3559d665fee903f84f74f9742602cb00cb47768a52cae0fe6e115d1a823"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('data_science_lab_2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
