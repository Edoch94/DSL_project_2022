%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Based on IEEE the conference template available                     %
% at https://www.ieee.org/conferences/publishing/templates.html       %
% Adapted for the Data Science Lab course at Politecnico di Torino    %
% by Giuseppe Attanasio, Flavio Giobergia                             %
% 2020, DataBase and Data Mining Group                                %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\graphicspath{ {./images/} }

\begin{document}

\title{Sentiment prediction of Twitter contents}

\author{\IEEEauthorblockN{Edoardo Chi√≤}
\IEEEauthorblockA{\textit{Politecnico di Torino} \\
Student id: s301486 \\
s301486@studenti.polito.it}
}

\maketitle

\begin{abstract}
The abstract goes here. Keep it short (approx. 3-4 sentences)
\end{abstract}

\section{Problem overview}
This project concerns a classification problem applied to a collection of Twitter posts (i.e., \textit{tweets}) written by different users. The goal of the project is to perform a sentiment analysis of the posts contained in the dataset.\\
Training and validation of the models are conducted on a development set, containing 224,994 labelled recordings, while the set to test the models contains 74,999 recordings.\\
The development set is composed of six fields:
\begin{itemize}
    \item \textit{sentiment}: sentiment labels;
    \item \textit{ids}: numerical identifier of the tweet;
    \item \textit{date}: publication date of the tweet;
    \item \textit{flag}: query used to collect the tweet;
    \item \textit{user}: name of the user that posted the tweet;
    \item \textit{text}: text of the tweet. 
\end{itemize}
The \textit{sentiment} field contains a label for each record. There are two classes: the text is considered having a positive trait if the label value is \textbf{1}, instead it is considered negative if the label value is \textbf{0}. The classes are not well balanced, indeed there are 130,157 data points having label 1, and just 94,837 having value 0. An exploratory analysis of the dataset was performed, to study the contents of all the fields and understand which features should be taken into account.\\
The \textit{ids} field (i.e., the row identifiers) presents 278 pairs of duplicates, each one having one row of the related pair of rows associated to a positive sentiment, and the other one to a negative sentiment; since it is not possible to know the correct sentiment for these records, they do not add any insight and so they were removed during the data cleaning. \\
The \textit{date} field shows that the tweets, both in the development dataset and in the evaluation dataset, were posted between April 6\textsuperscript{th} 2009 and June 25\textsuperscript{th} 2009; this suggests that the data should be homogeneous across the datasets, with a similar distribution of sentiment. \\
The \textit{flag} field presents a unique value in all tweets in both the datasets, so it is not useful for the analysis and it can be discarded. \\
The \textit{user} field contains the names of 10,647 distinct users; each user wrote at least 15 tweets (considering the datasets), but some users were much more prolific than others (figure \ref{fig:user_per_tweet}).
\begin{figure}
    \includegraphics[width=0.5\textwidth]{user_per_tweet}
    \caption[Histogram of users per tweets]{Histogram representing the number of users per number of tweets}
    \label{fig:user_per_tweet}
\end{figure}
Analysing the user average sentiment distribution (figure \ref{fig:average_user_sentiment}), it is possible to notice that there are many users whose tweets are almost exclusively positive (or negative).
\begin{figure}
    \includegraphics[width=0.5\textwidth]{average_user_sentiment}
    \caption[User average sentiment]{Histogram representing the number of users per average sentiment}
    \label{fig:average_user_sentiment}
\end{figure}
This odd behaviour led to a deeper investigation of the texts written by each user; a mean cosine similarity among the tweets posted by the same user was computed (figure \ref{fig:internal_similarity_users}), and it showed that a small but significant group of users used the exact same words in multiple tweets.  
\begin{figure}
    \includegraphics[width=0.5\textwidth]{internal_similarity_users}
    \caption[Mean cosine similarity]{Histogram representing the number of users per mean cosine similarity computed on the posted tweets of each user}
    \label{fig:internal_similarity_users}
\end{figure}
Some of the users that posted the largest number of tweets, also have the highest average cosine similarity among their tweets; a precise inspection of these users suggests that most of them behave as bots, regularly posting the same message. The problem of bot identification can be addressed using machine learning frameworks, as the one suggested by Kantepe and Ganiz \cite{8093483}, however for this project a simple removal of suspicious users is performed in the preprocessing step. \\
Finally, the \textit{text} field collects the tweet messages. The messages are written in english, they may contain tags (i.e., starting with "@"), hashtags (i.e., starting with "\#"), and URLs. HTML character entities (e.g., "\textit{\&amp;}", "\textit{\&quot;}") are present too. \\
The \textit{text} field is the only one which is going to be considered to train the model and to predict the text sentiment; indeed, the \textit{date} field is only useful to show that both the development set and the evaluation set refer to the same time period; instead, the \textit{user} field is not taken into account because this would instruct the model to predict the sentiment based also on the user who posted the message, loosing the more general scope the model should have. 

\section{Proposed approach}
\subsection{Preprocessing}
The model evaluation is made using the evaluation set as reference. As said before, the development set and the evaluation set share the same characteristics, and, presumably, the same source. Basing the model performance on this evaluation set may lead to overfitting, making the model inadequate to predict the tweet sentiment outside this project. Therefore, two possible preprocessing paths have been designed: one more complex, composed of several steps, and more suitable to a general application; and one much simpler, consisting just in the first three steps of the complex one, that performs better, but is probably less general. \\
The message cleaning steps composing the more articulate preprocessing path were partially influenced by \cite{ZainuddinNurulhuda2016TFSa}. The steps are the following
\begin{enumerate}
    \item Removal of rows having the same \textit{id} value
    \item "\textit{\&amp;}" HTML entities are removed;
    \item "\textit{\&quot;}" HTML entities are removed; 
    \item Words staring with \textit{@} are removed: these words are user tags, and do not give insights regarding the sentiment of the message; 
    \item Words staring with \textit{http} are removed: these words are URLs, and do not give insights regarding the sentiment of the message;
    \item Punctuation is removed;
    \item Stemming is performed, using the Snowball stemmer by Porter \cite{Porter2006};
    \item Negations in sentences are stressed appending in subsequent words a "\textit{\_NEG}" suffix.
    \item Exclusion of users behaving as bots: as seen before, some users write the same message multiple times (i.e., users that have an high average cosine similarity computed on their tweets), always associated with the same sentiment. These users should be excluded, since they "pullute" the dataset. To identify the most damaging ones, for each user a \textit{user\_suspiciousness} metric was computed\footnote{The \textit{user\_suspiciousness} is computed as the product between the mean cosine similarity among the user messages, and the "extremism" of his sentiment, that is the distance between the user average sentiment and a neutral sentiment (i.e., 0.5); both the mean cosine similarity and the sentiment extremism are scaled between 0 and 1 before the product.}, and the users having a value higher than 0.9 were dropped from the development set. 
\end{enumerate}
After these steps, the features are extracted tokeninzing the messages and creating a bag-of-words; the schema applied to weight the words is the \textbf{tf-idf} (i.e., term frequency - inverse document frequency), meaning that each word importance increases as its frequency in the same meassage increases, and lowers the more the word is used across all the messages in the collection. No maximum or minimum document frequency is set for a word to be considered, and also no maximum number of words is set; this means that the bag-of-words includes all the words the appear in the document collection. Other preprocessing options, such as the removal of stopwords, the type of normalization, and the N-Gram\footnote{An N-Gram is a contiguous sequence of N items from a given sample of text or speech; if the items are words, N-Grams are also called \textit{shingles}} length, are considered as hyperparameters of the preprocessing step.
\subsection{Model selection}
Two fast and light classification models were assessed: 
\begin{itemize}
    \item Naive Bayes Classifiers: it is a classifier family commonly employed in sentiment analysis contexts \cite{MEDHAT20141093}. It uses the Bayes Theorem to predict the probability that a given feature set belongs to a particular label. The Naive Bayes Classifier family include several algorithms, in particular the Multinomial and the Bernoulli classifiers are often for document classification, with similar performance \cite{8776800}. For this project, given that the tf-idf word weighting schema is used instead of a basic term occurrence, the Multinomial Naive Bayes classifier was selected for evaluation.
    \item Support Vector Machine Classifier: these algorithms carry the classification task determining the linear separators in the search space which can best separate the different classes. SVCs have already been used in sentiment analysis contexts, achieving good results \cite{6914200}. Because of the extent of the dataset, in particular concerning the features, only the simpler Linear Support Vector Classifier was considered. 
\end{itemize}
The best performing hyperparameter configuration was identified using a grid search. 
\subsection{Hyperparameters tuning}
Both the preprocessing and the model Hyperparameters were tuned. The considered hyperparameters configurations are summarized in table \ref{table: hyperparameters_considered}.
\begin{table}[]
    \caption{Considered hyperparameters}
    \begin{tabular}{l|c|c}
    \textbf{Preprocessing / Model} & \textbf{Hyperparameter}                                                                                                                                         & \textbf{Values}                                                                                                                                                                                             \\ \hline
    TfidfVectorizer                & \textit{\begin{tabular}[c]{@{}c@{}}stopwords\\ ngram range\\ max features\\ max df\\ min df\\ binary\\ norm\\ use idf\\ smooth idf\\ sublinear tf\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\{"english", None\}\\ \{(1,2), (1,3)\}\\ \{None,20000\}\\ \{1.0\}\\ \{1\}\\ \{True, False\}\\ \{l1, l2\}\\ \{True\}\\ \{True, False\}\\ \{False\}\end{tabular} \\ \hline
    Multinomial NB                 & \textit{alpha}                                                                                                                                                  & \{.01,.02,.05,.1,.2,.5,1.0\}                                                                                                                                                                    \\ \hline
    SVC                            & \textit{\begin{tabular}[c]{@{}c@{}}penalty\\ tolerance\\ C\\ max iterations\end{tabular}}                                                                       & \begin{tabular}[c]{@{}c@{}}\{l1, l2\}\\ \{1e-2, 1e-3\}\\ \{1, 10\}\\ \{100\}\end{tabular}                                                                                                              
    \end{tabular}
	\label{table: hyperparameters_considered}
\end{table}

The development set is 90/10 train/validation splitted, given that the development set is fairly large, and a grid search is run on the traning subset. A linear SVM and a Multinomial Naive Bayes with their default configurations, without using any previous preprocessing, were trained and their performances were used as beseline for comparison.
\section{Results}


\section{Discussion}
The solution found using the tuned models outperfom the naive approaches, and both the models are able to overcome the baseline score of 0.753 at their best configuration. In particular, the Linear Support Vector Classifier reaches good results, especially considering the model simplicity and speed. However, in this project the critical step was the preprocessing: the usage of bigrams and trigrams combined with unigrams led to a wide increase in the f1-score.\\
To further improve the obtained results, other more complex machine learning models should be considered, such as non linear SVCs, ANNs, or Random Forests, maybe preceded by the application of a dimensionality reduction algorithm, like SVD, to reduce the number of features and to extract the most meaningful ones. \\Another possible solution is the usage of deep learning models with an embedding layer, that could also be based on a pretrained word embedding stack, substantially reducing the time required for the model training. The use of deep learning models in the context of sentiment analysis has been widely explored in recent years, with good results (see Zhang et al. \cite{ZhangLeiWangShuaiLiuBing}). \\It might also be interesting to apply an unsupervised machine learning method to the dataset and compare its results with the supervised methods. A possible model is VADER, that was designed specifically for sentiment analysis, and is sensitive to both polarity (positive/negative) and intensity (strength) of emotions expressed in texts; the usage of this model in a context analogous to the one of this project is discussed by Elbagir and Yang \cite{elbagir2019twitter}.

\bibliography{bibliography}
\bibliographystyle{ieeetr}

\end{document}
