{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'development.csv')\n",
    "df_eval = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74999, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_dev.append(df_eval)\n",
    "\n",
    "df = pd.concat([df_dev, df_eval], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['date'] = df['date'].\\\n",
    "#     astype('string')\\\n",
    "#     .str.split(' ')\\\n",
    "#     .apply(lambda x : ' '.join([x[i] for i in [1,2,3,5]]))\\\n",
    "#     .pipe(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords analysis: sklearn, nltk, stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Edoardo\n",
      "[nltk_data]     pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import download as nltk_download\n",
    "\n",
    "nltk_download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "sklearn_stopwords = list(text.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stop_words import get_stop_words\n",
    "\n",
    "# stop_words_stopwords = get_stop_words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should try to use both all stopwords, and sklearn and nltk stopwords singularly (and also no stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_list_gen(source_list = [sklearn_stopwords,nltk_stopwords], generate_neg = True):\n",
    "    stopwords_all_list = set()\n",
    "    for source in source_list:\n",
    "        stopwords_all_list = stopwords_all_list.union(set(source))\n",
    "        if generate_neg:\n",
    "            stopwords_all_list = stopwords_all_list.union(set([f'{i+\"_neg\"}' for i in source]))\n",
    "    return stopwords_all_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_token'] = df['text'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '@words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_noAt'] = df['text_token'].apply(lambda x : [i for i in x if not(i.startswith('@'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '&amp' and '&quot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_noAmpQuot'] = df['text_noAt']\\\n",
    "    .apply(lambda x : [i for i in x if '&amp' not in i])\\\n",
    "    .apply(lambda x : [i for i in x if '&quot' not in i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of repeated letters (incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'ulaaa'\n",
    "\n",
    "def remove_repeated_letters(word):\n",
    "    for letter in word:\n",
    "        print(letter)\n",
    "\n",
    "# nltk_stemmer.stem(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove puntuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string as py_string\n",
    "df['text_noPunct'] = df['text_noAmpQuot']\\\n",
    "    .apply(lambda x : [i.translate(str.maketrans('', '', py_string.punctuation)) for i in x])\\\n",
    "    .apply(lambda x : [i for i in x if i != ''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "df['text_neg'] = df['text_noPunct'].apply(lambda x : mark_negation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considered_stopwords = stop_words_stopwords + [f'{word}_neg' for word in stop_words_stopwords]\n",
    "\n",
    "# df['text_noStopwords'] = df['text_neg'].apply(lambda x : [i for i in x if i not in considered_stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "# nltk_download('wordnet')\n",
    "# nltk_download('omw-1.4')\n",
    "\n",
    "# nltk_stemmer = PorterStemmer()\n",
    "# # nltk_stemmer = LancasterStemmer()\n",
    "# # nltk_stemmer = SnowballStemmer('english')\n",
    "# nltk_lemmatizer = WordNetLemmatizer()\n",
    "# # from nltk.corpus import wordnet\n",
    "\n",
    "# df['text_stemmed'] = df['text_noPunct'].apply(lambda x : [nltk_stemmer.stem(word) for word in x])\n",
    "# df['text_stemmed'] = df['text_noPunct'].apply(lambda x : [nltk_lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User manual filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[\n",
    "    (~(df['user'] == 'lost_dog') &\n",
    "    ~(df['user'] == 'webwoke') &\n",
    "    ~(df['user'] == 'tweetpet') &\n",
    "    ~(df['user'].str.contains('tweeteradder')) &\n",
    "    ~(df['user'].str.contains('tweetfollow')) &\n",
    "    ~(df['user'] == 'divxdownloads')) |\n",
    "    df['sentiment'].isna()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74999, 11)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['sentiment'].isna()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "multinomialNB_params = {\n",
    "    'alpha' : [0.0,0.5,1.0,2.0],\n",
    "    'fit_prior' : [True,False]\n",
    "}\n",
    "\n",
    "# TfidfVectorizer_params = {\n",
    "#     'stop_words' : [None],\n",
    "#     'ngram_range' : [(1,1),(1,2)],\n",
    "#     'max_features' : [1500],\n",
    "#     'max_df' : [1.0,0.1,0.005],\n",
    "#     'min_df' : [1,0.0001,0.00001],\n",
    "#     'binary' : [True,False],\n",
    "#     'norm' : ['l1','l2'],\n",
    "#     'use_idf' : [True],\n",
    "#     'smooth_idf' : [True,False],\n",
    "#     'sublinear_tf' : [False]\n",
    "# }\n",
    "\n",
    "TfidfVectorizer_params = {\n",
    "    'stop_words' : [None],\n",
    "    'ngram_range' : [(1,1)],\n",
    "    'max_features' : [1500],\n",
    "    'max_df' : [1.0],\n",
    "    'min_df' : [1],\n",
    "    'binary' : [False],\n",
    "    'norm' : ['l2'],\n",
    "    'use_idf' : [True],\n",
    "    'smooth_idf' : [True],\n",
    "    'sublinear_tf' : [False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010256410256410254\n"
     ]
    }
   ],
   "source": [
    "print(len(list(ParameterGrid(TfidfVectorizer_params)))*len(list(ParameterGrid(multinomialNB_params)))/13/60/60/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df_final = df\n",
    "df_final['text_final'] = df_final['text_noPunct'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "vectorizer = TfidfVectorizer(**vectorizer_param)\n",
    "\n",
    "wpm = vectorizer.fit_transform(df_final['text_final'])\n",
    "\n",
    "word_freq = pd.Series(\n",
    "    data = np.asarray(wpm.sum(axis=0)).squeeze(),\n",
    "    index = vectorizer.get_feature_names_out()\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "word_ind = [w in word_freq.index for w in vectorizer.get_feature_names_out()]\n",
    "\n",
    "words_df = pd.DataFrame(\n",
    "    data = wpm[:,word_ind].toarray(),\n",
    "    columns = vectorizer.get_feature_names_out()[word_ind],\n",
    "    index = df_final.index\n",
    ").add_prefix('word_')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "X_train_valid = words_df.loc[mask_train_test,:].values\n",
    "y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "X_test = words_df.loc[~mask_train_test,:].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, \n",
    "    y_train_valid, \n",
    "    shuffle=True, \n",
    "    # stratify=y_train_valid, \n",
    "    train_size=0.8, \n",
    "    random_state=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations:\t1152\n",
      "1 {'binary': True, 'max_df': 1.0, 'max_features': 1500, 'min_df': 1, 'ngram_range': (1, 1), 'norm': 'l1', 'smooth_idf': True, 'stop_words': None, 'sublinear_tf': False, 'use_idf': True} {'alpha': 0.0, 'fit_prior': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\DSL_project_2\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 {'binary': True, 'max_df': 1.0, 'max_features': 1500, 'min_df': 1, 'ngram_range': (1, 1), 'norm': 'l1', 'smooth_idf': True, 'stop_words': None, 'sublinear_tf': False, 'use_idf': True} {'alpha': 0.0, 'fit_prior': False}\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.33 GiB for an array with shape (1500, 297949) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\EDOARD~1\\AppData\\Local\\Temp/ipykernel_1764/1296817449.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mword_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         words_df = pd.DataFrame(\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwpm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_ind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\DSL_project_2\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36madd_prefix\u001b[1;34m(self, prefix)\u001b[0m\n\u001b[0;32m   4301\u001b[0m         \u001b[1;31m# error: Argument 1 to \"rename\" of \"NDFrame\" has incompatible type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4302\u001b[0m         \u001b[1;31m# \"**Dict[str, partial[str]]\"; expected \"Union[str, int, None]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4303\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[return-value, arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4305\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\DSL_project_2\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mkind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOSITIONAL_OR_KEYWORD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\DSL_project_2\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mrename\u001b[1;34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[0m\n\u001b[0;32m   5037\u001b[0m         \u001b[1;36m4\u001b[0m  \u001b[1;36m3\u001b[0m  \u001b[1;36m6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5038\u001b[0m         \"\"\"\n\u001b[1;32m-> 5039\u001b[1;33m         return super().rename(\n\u001b[0m\u001b[0;32m   5040\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmapper\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5041\u001b[0m             \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\DSL_project_2\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mrename\u001b[1;34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_inplace_and_allows_duplicate_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplacements\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\DSL_project_2\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   5931\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5932\u001b[0m         \"\"\"\n\u001b[1;32m-> 5933\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5934\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5935\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"copy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\DSL_project_2\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"copy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m         \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\DSL_project_2\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\DSL_project_2\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.33 GiB for an array with shape (1500, 297949) and data type float64"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "logs_path = Path.cwd()/'logs'\n",
    "results_path = Path.cwd()/'results'\n",
    "logs_path.mkdir(exist_ok=True)\n",
    "results_path.mkdir(exist_ok=True)\n",
    "\n",
    "log_file_name_string = f'log_out_{datetime.now()}'.replace(':','-').replace('.','-')\n",
    "results_file_name = f'results_out_{datetime.now()}'.replace(':','-').replace('.','-')\n",
    "\n",
    "log_file_name = logs_path/f'{log_file_name_string}.txt'\n",
    "results_file_name = results_path/f'{results_file_name}.txt'\n",
    "with open(log_file_name, 'w') as file_log:\n",
    "    file_log.write('File output\\n\\n')\n",
    "with open(results_file_name, 'w') as file_result:\n",
    "    file_result.write('configuration_number,vectorizer_param,model_param,f1\\n')\n",
    "\n",
    "df_final = df\n",
    "df_final['text_final'] = df_final['text_noPunct'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "configuration_number = 0\n",
    "print(f'Number of combinations:\\t{len(list(ParameterGrid(TfidfVectorizer_params)))*len(list(ParameterGrid(multinomialNB_params)))}')\n",
    "for vectorizer_param in ParameterGrid(TfidfVectorizer_params):\n",
    "    for model_param in ParameterGrid(multinomialNB_params):\n",
    "        configuration_number += 1\n",
    "        print(configuration_number, vectorizer_param, model_param)\n",
    "\n",
    "        vectorizer = TfidfVectorizer(**vectorizer_param)\n",
    "\n",
    "        wpm = vectorizer.fit_transform(df_final['text_final'])\n",
    "\n",
    "        word_freq = pd.Series(\n",
    "            data = np.asarray(wpm.sum(axis=0)).squeeze(),\n",
    "            index = vectorizer.get_feature_names_out()\n",
    "        ).sort_values(ascending=False)\n",
    "\n",
    "        word_ind = [w in word_freq.index for w in vectorizer.get_feature_names_out()]\n",
    "\n",
    "        words_df = pd.DataFrame(\n",
    "            data = wpm[:,word_ind].toarray(),\n",
    "            columns = vectorizer.get_feature_names_out()[word_ind],\n",
    "            index = df_final.index\n",
    "        ).add_prefix('word_')\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "        X_train_valid = words_df.loc[mask_train_test,:].values\n",
    "        y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "        X_test = words_df.loc[~mask_train_test,:].values\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_train_valid, \n",
    "            y_train_valid, \n",
    "            shuffle=True, \n",
    "            # stratify=y_train_valid, \n",
    "            train_size=0.8, \n",
    "            random_state=50\n",
    "        )\n",
    "\n",
    "        rfc = MultinomialNB(**model_param)\n",
    "        try: \n",
    "            rfc.fit(X_train, y_train)\n",
    "\n",
    "            f1 = f1_score(y_valid, rfc.predict(X_valid),average='macro')\n",
    "            report = classification_report(y_valid, rfc.predict(X_valid))\n",
    "            confusion = confusion_matrix(y_valid, rfc.predict(X_valid))\n",
    "\n",
    "            with open(log_file_name, 'a') as file_log:\n",
    "                file_log.write(f'configuration_number: {configuration_number}\\n')\n",
    "                file_log.write(f'vectorizer_param: \\t{vectorizer_param}\\n')\n",
    "                file_log.write(f'model_param: \\t\\t{model_param}\\n')\n",
    "                file_log.write(f'f1_score:\\t\\t\\t{f1}\\n\\n')\n",
    "                file_log.write(f'{report}\\n\\n')\n",
    "                file_log.write(f'{\"*\"*150}\\n\\n')\n",
    "            with open(results_file_name, 'a') as file_result:\n",
    "                file_result.write(f'{configuration_number},{vectorizer_param},{model_param},{f1}\\n')\n",
    "\n",
    "        except ValueError as exception:\n",
    "            with open(log_file_name, 'a') as file_log:\n",
    "                file_log.write(f'configuration_number: {configuration_number}\\n')\n",
    "                file_log.write(f'vectorizer_param: \\t{vectorizer_param}\\n')\n",
    "                file_log.write(f'model_param: \\t\\t{model_param}\\n')\n",
    "                file_log.write(f'{exception}\\n\\n')\n",
    "                file_log.write(f'{\"*\"*150}\\n\\n')\n",
    "            with open(results_file_name, 'a') as file_result:\n",
    "                file_result.write(f'{configuration_number},{vectorizer_param},{model_param},NaN\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd8ee3559d665fee903f84f74f9742602cb00cb47768a52cae0fe6e115d1a823"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('data_science_lab_2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
