{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'development.csv')\n",
    "df_eval = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74999, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_dev.append(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['date'] = df['date'].\\\n",
    "#     astype('string')\\\n",
    "#     .str.split(' ')\\\n",
    "#     .apply(lambda x : ' '.join([x[i] for i in [1,2,3,5]]))\\\n",
    "#     .pipe(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords analysis: sklearn, nltk, stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/edoch/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import download as nltk_download\n",
    "\n",
    "nltk_download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "sklearn_stopwords = list(text.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stop_words import get_stop_words\n",
    "\n",
    "# stop_words_stopwords = get_stop_words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should try to use both all stopwords, and sklearn and nltk stopwords singularly (and also no stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_list_gen(source_list = [sklearn_stopwords,nltk_stopwords], generate_neg = True):\n",
    "    stopwords_all_list = set()\n",
    "    for source in source_list:\n",
    "        stopwords_all_list = stopwords_all_list.union(set(source))\n",
    "        if generate_neg:\n",
    "            stopwords_all_list = stopwords_all_list.union(set([f'{i+\"_neg\"}' for i in source]))\n",
    "    return stopwords_all_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_token'] = df['text'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '@words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_noAt'] = df['text_token'].apply(lambda x : [i for i in x if not(i.startswith('@'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '&amp' and '&quot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_noAmpQuot'] = df['text_noAt']\\\n",
    "    .apply(lambda x : [i for i in x if '&amp' not in i])\\\n",
    "    .apply(lambda x : [i for i in x if '&quot' not in i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of repeated letters (incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'ulaaa'\n",
    "\n",
    "def remove_repeated_letters(word):\n",
    "    for letter in word:\n",
    "        print(letter)\n",
    "\n",
    "# nltk_stemmer.stem(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove puntuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string as py_string\n",
    "df['text_noPunct'] = df['text_noAmpQuot']\\\n",
    "    .apply(lambda x : [i.translate(str.maketrans('', '', py_string.punctuation)) for i in x])\\\n",
    "    .apply(lambda x : [i for i in x if i != ''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "df['text_neg'] = df['text_noPunct'].apply(lambda x : mark_negation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considered_stopwords = stop_words_stopwords + [f'{word}_neg' for word in stop_words_stopwords]\n",
    "\n",
    "# df['text_noStopwords'] = df['text_neg'].apply(lambda x : [i for i in x if i not in considered_stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "# nltk_download('wordnet')\n",
    "# nltk_download('omw-1.4')\n",
    "\n",
    "# nltk_stemmer = PorterStemmer()\n",
    "# # nltk_stemmer = LancasterStemmer()\n",
    "# # nltk_stemmer = SnowballStemmer('english')\n",
    "# nltk_lemmatizer = WordNetLemmatizer()\n",
    "# # from nltk.corpus import wordnet\n",
    "\n",
    "# df['text_stemmed'] = df['text_noPunct'].apply(lambda x : [nltk_stemmer.stem(word) for word in x])\n",
    "# df['text_stemmed'] = df['text_noPunct'].apply(lambda x : [nltk_lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User manual filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[\n",
    "    (~(df['user'] == 'lost_dog') &\n",
    "    ~(df['user'] == 'webwoke') &\n",
    "    ~(df['user'] == 'tweetpet') &\n",
    "    ~(df['user'].str.contains('tweeteradder')) &\n",
    "    ~(df['user'].str.contains('tweetfollow')) &\n",
    "    ~(df['user'] == 'divxdownloads')) |\n",
    "    df['sentiment'].isna()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74999, 11)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['sentiment'].isna()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "linearSVC_params = {\n",
    "    'penalty' : ['l1','l2'],\n",
    "    'dual' : [False],\n",
    "    'tol' : [1e-3,1e-4,1e-5],\n",
    "    'fit_intercept' : [True, False],\n",
    "    'class_weight' : ['balanced', None],\n",
    "    # 'verbose' : [1],\n",
    "    'max_iter' : [100,300,500],\n",
    "    'random_state' : [42],\n",
    "    'C' : [1,10,50,100]\n",
    "}\n",
    "\n",
    "TfidfVectorizer_params = {\n",
    "    'stop_words' : ['english', None],\n",
    "    'ngram_range' : [(1,1),(1,2)],\n",
    "    'max_features' : [300,1000,2000],\n",
    "    'max_df' : [1.0,0.1,0.005],\n",
    "    'min_df' : [1,0.0001,0.00001],\n",
    "    'binary' : [True,False],\n",
    "    'norm' : ['l1','l2'],\n",
    "    'use_idf' : [True,False],\n",
    "    'smooth_idf' : [True,False],\n",
    "    'sublinear_tf' : [True,False]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8861538461538462\n"
     ]
    }
   ],
   "source": [
    "print(len(list(ParameterGrid(TfidfVectorizer_params)))*len(list(ParameterGrid(linearSVC_params)))/13/60/60/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.695929687226944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.64      0.65     18783\n",
      "         1.0       0.74      0.75      0.74     25807\n",
      "\n",
      "    accuracy                           0.70     44590\n",
      "   macro avg       0.70      0.70      0.70     44590\n",
      "weighted avg       0.70      0.70      0.70     44590\n",
      "\n",
      "[[12111  6672]\n",
      " [ 6537 19270]]\n",
      "0.7167469965669069\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.71      0.68     18783\n",
      "         1.0       0.78      0.73      0.75     25807\n",
      "\n",
      "    accuracy                           0.72     44590\n",
      "   macro avg       0.72      0.72      0.72     44590\n",
      "weighted avg       0.73      0.72      0.72     44590\n",
      "\n",
      "[[13386  5397]\n",
      " [ 7050 18757]]\n"
     ]
    }
   ],
   "source": [
    "# from datetime import datetime\n",
    "# from pathlib import Path\n",
    "\n",
    "# logs_path = Path.cwd()/'logs'\n",
    "# results_path = Path.cwd()/'results'\n",
    "# logs_path.mkdir(exist_ok=True)\n",
    "# results_path.mkdir(exist_ok=True)\n",
    "# log_file_name = logs_path/f'log_out_{datetime.now()}.txt'\n",
    "# results_file_name = results_path/f'results_out_{datetime.now()}.txt'\n",
    "# with open(log_file_name, 'w') as file_log:\n",
    "#     file_log.write('File output\\n\\n')\n",
    "# with open(results_file_name, 'w') as file_result:\n",
    "#     file_result.write('')\n",
    "\n",
    "# df_final = df\n",
    "# df_final['text_final'] = df_final['text_noPunct'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# configuration_number = 0\n",
    "# for param in ParameterGrid(TfidfVectorizer_params):\n",
    "#     configuration_number += 1\n",
    "#     vectorizer = TfidfVectorizer(**param)\n",
    "\n",
    "#     wpm = vectorizer.fit_transform(df_final['text_final'])\n",
    "\n",
    "#     word_freq = pd.Series(\n",
    "#         data = np.asarray(wpm.sum(axis=0)).squeeze(),\n",
    "#         index = vectorizer.get_feature_names_out()\n",
    "#     ).sort_values(ascending=False)\n",
    "\n",
    "#     word_ind = [w in word_freq.index for w in vectorizer.get_feature_names_out()]\n",
    "\n",
    "#     words_df = pd.DataFrame(\n",
    "#         data = wpm[:,word_ind].toarray(),\n",
    "#         columns = vectorizer.get_feature_names_out()[word_ind],\n",
    "#         index = df_final.index\n",
    "#     ).add_prefix('word_')\n",
    "\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "\n",
    "#     mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "#     X_train_valid = words_df.loc[mask_train_test,:].values\n",
    "#     y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "#     X_test = words_df.loc[~mask_train_test,:].values\n",
    "\n",
    "#     X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "#         X_train_valid, \n",
    "#         y_train_valid, \n",
    "#         shuffle=True, \n",
    "#         # stratify=y_train_valid, \n",
    "#         train_size=0.8, \n",
    "#         random_state=50\n",
    "#     )\n",
    "\n",
    "#     rfc = LinearSVC(C=10, class_weight='balanced', dual=False, max_iter=100, random_state=42, tol=0.001)\n",
    "#     rfc.fit(X_train, y_train)\n",
    "    \n",
    "#     f1 = f1_score(y_valid, rfc.predict(X_valid),average='macro')\n",
    "#     report = classification_report(y_valid, rfc.predict(X_valid))\n",
    "#     confusion = confusion_matrix(y_valid, rfc.predict(X_valid))\n",
    "\n",
    "#     print(f1)\n",
    "#     print(report)\n",
    "#     print(confusion)\n",
    "\n",
    "#     with open(log_file_name, 'a') as file_log:\n",
    "#         file_log.write(f'{configuration_number}\\n')\n",
    "#         file_log.write(f'{param}\\n')\n",
    "#         file_log.write(f'\\tf1_score:\\t{f1}\\n\\n')\n",
    "#         file_log.write(f'\\t{report}\\n\\n')\n",
    "#         file_log.write(f'{\"*\"*150}\\n\\n')\n",
    "    \n",
    "#     with open(results_file_name, 'a') as file_result:\n",
    "#         file_result.write(f'{configuration_number},{param},{f1}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combinations:\t2\n",
      "1 {'max_features': 300, 'smooth_idf': True, 'use_idf': False} {'dual': True, 'penalty': 'l1'}\n",
      "2 {'max_features': 300, 'smooth_idf': True, 'use_idf': False} {'dual': True, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "logs_path = Path.cwd()/'logs'\n",
    "results_path = Path.cwd()/'results'\n",
    "logs_path.mkdir(exist_ok=True)\n",
    "results_path.mkdir(exist_ok=True)\n",
    "log_file_name = logs_path/f'log_out_{datetime.now()}.txt'\n",
    "results_file_name = results_path/f'results_out_{datetime.now()}.txt'\n",
    "with open(log_file_name, 'w') as file_log:\n",
    "    file_log.write('File output\\n\\n')\n",
    "with open(results_file_name, 'w') as file_result:\n",
    "    file_result.write('configuration_number,vectorizer_param,model_param,f1\\n')\n",
    "\n",
    "df_final = df\n",
    "df_final['text_final'] = df_final['text_noPunct'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "configuration_number = 0\n",
    "print(f'Number of combinations:\\t{len(list(ParameterGrid(TfidfVectorizer_params)))*len(list(ParameterGrid(linearSVC_params)))}')\n",
    "for vectorizer_param in ParameterGrid(TfidfVectorizer_params):\n",
    "    for model_param in ParameterGrid(linearSVC_params):\n",
    "        configuration_number += 1\n",
    "        print(configuration_number, vectorizer_param, model_param)\n",
    "\n",
    "        vectorizer = TfidfVectorizer(**vectorizer_param)\n",
    "\n",
    "        wpm = vectorizer.fit_transform(df_final['text_final'])\n",
    "\n",
    "        word_freq = pd.Series(\n",
    "            data = np.asarray(wpm.sum(axis=0)).squeeze(),\n",
    "            index = vectorizer.get_feature_names_out()\n",
    "        ).sort_values(ascending=False)\n",
    "\n",
    "        word_ind = [w in word_freq.index for w in vectorizer.get_feature_names_out()]\n",
    "\n",
    "        words_df = pd.DataFrame(\n",
    "            data = wpm[:,word_ind].toarray(),\n",
    "            columns = vectorizer.get_feature_names_out()[word_ind],\n",
    "            index = df_final.index\n",
    "        ).add_prefix('word_')\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "        X_train_valid = words_df.loc[mask_train_test,:].values\n",
    "        y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "        X_test = words_df.loc[~mask_train_test,:].values\n",
    "\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_train_valid, \n",
    "            y_train_valid, \n",
    "            shuffle=True, \n",
    "            # stratify=y_train_valid, \n",
    "            train_size=0.8, \n",
    "            random_state=50\n",
    "        )\n",
    "\n",
    "        rfc = LinearSVC(**model_param)\n",
    "        try: \n",
    "            rfc.fit(X_train, y_train)\n",
    "\n",
    "            f1 = f1_score(y_valid, rfc.predict(X_valid),average='macro')\n",
    "            report = classification_report(y_valid, rfc.predict(X_valid))\n",
    "            confusion = confusion_matrix(y_valid, rfc.predict(X_valid))\n",
    "\n",
    "            with open(log_file_name, 'a') as file_log:\n",
    "                file_log.write(f'configuration_number: {configuration_number}\\n')\n",
    "                file_log.write(f'vectorizer_param: \\t{vectorizer_param}\\n')\n",
    "                file_log.write(f'model_param: \\t\\t{model_param}\\n')\n",
    "                file_log.write(f'f1_score:\\t\\t\\t{f1}\\n\\n')\n",
    "                file_log.write(f'{report}\\n\\n')\n",
    "                file_log.write(f'{\"*\"*150}\\n\\n')\n",
    "            with open(results_file_name, 'a') as file_result:\n",
    "                file_result.write(f'{configuration_number},{vectorizer_param},{model_param},{f1}\\n')\n",
    "\n",
    "        except ValueError as exception:\n",
    "            with open(log_file_name, 'a') as file_log:\n",
    "                file_log.write(f'configuration_number: {configuration_number}\\n')\n",
    "                file_log.write(f'vectorizer_param: \\t{vectorizer_param}\\n')\n",
    "                file_log.write(f'model_param: \\t\\t{model_param}\\n')\n",
    "                file_log.write(f'{exception}\\n\\n')\n",
    "                file_log.write(f'{\"*\"*150}\\n\\n')\n",
    "            with open(results_file_name, 'a') as file_result:\n",
    "                file_result.write(f'{configuration_number},{vectorizer_param},{model_param},NaN\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd8ee3559d665fee903f84f74f9742602cb00cb47768a52cae0fe6e115d1a823"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('data_science_lab_2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
