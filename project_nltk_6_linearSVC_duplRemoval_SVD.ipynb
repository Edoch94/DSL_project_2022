{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cupy \n",
    "# import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'development.csv')\n",
    "df_eval = pd.read_csv(Path.cwd()/'data'/'DSL2122_january_dataset'/'evaluation.csv')\n",
    "df = df_dev.append(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '&amp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_noAmpEnt(df, field_IN, field_OUT):\n",
    "    # df[field_OUT] = df[field_IN].apply(lambda x : [i.replace('&amp;','') for i in x])\n",
    "    df[field_OUT] = df[field_IN].str.replace('&amp;','',case=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '&quot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_noQuotEnt(df, field_IN, field_OUT):\n",
    "    # df[field_OUT] = df[field_IN].apply(lambda x : [i for i in x if '&quot;' not in i])\n",
    "    df[field_OUT] = df[field_IN].str.replace('&quot;','',case=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '@words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_noAt(df, field_IN, field_OUT):\n",
    "    df[field_OUT] = df[field_IN]\\\n",
    "        .str.split()\\\n",
    "        .apply(lambda x : [i for i in x if not(i.startswith('@'))])\\\n",
    "        .str.join(' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of '&words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_noAmp(df, field_IN, field_OUT):\n",
    "    df[field_OUT] = df[field_IN]\\\n",
    "        .str.split()\\\n",
    "        .apply(lambda x : [i for i in x if not(i.startswith('&'))])\\\n",
    "        .str.join(' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of 'http:words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_noHttp(df, field_IN, field_OUT):\n",
    "    df[field_OUT] = df[field_IN]\\\n",
    "        .str.split()\\\n",
    "        .apply(lambda x : [i for i in x if not(i.startswith('http'))])\\\n",
    "        .str.join(' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of repeated letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def text_noDuplLetters(df, field_IN, field_OUT):\n",
    "    df[field_OUT] = df[field_IN]\\\n",
    "        .str.split()\\\n",
    "        .apply(lambda x : [re.sub(r'(.)\\1+', r'\\1', word) for word in x])\\\n",
    "        .str.join(' ')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove puntuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string as py_string\n",
    "def text_noPunctuation(df, field_IN, field_OUT):\n",
    "    df[field_OUT] = df[field_IN].str.translate(str.maketrans('', '', py_string.punctuation))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\edo_c\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\edo_c\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import download as nltk_download\n",
    "nltk_download('wordnet')\n",
    "nltk_download('omw-1.4')\n",
    "\n",
    "def text_stem(df, field_IN, field_OUT, stemmer):\n",
    "    if stemmer == SnowballStemmer:\n",
    "        stemmer_to_use = SnowballStemmer('english')\n",
    "    else: \n",
    "        stemmer_to_use = stemmer()\n",
    "    df[field_OUT] = df[field_IN]\\\n",
    "        .str.split()\\\n",
    "        .apply(lambda x : [stemmer_to_use.stem(word) for word in x])\\\n",
    "        .str.join(' ')\n",
    "    return df\n",
    "\n",
    "def text_lemm(df, field_IN, field_OUT, lemmatizer):\n",
    "    df[field_OUT] = df[field_IN]\\\n",
    "        .str.split()\\\n",
    "        .apply(lambda x : [lemmatizer().lemmatize(word) for word in x])\\\n",
    "        .str.join(' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.util import mark_negation\n",
    "def text_neg(df, field_IN, field_OUT):\n",
    "    df[field_OUT] = df[field_IN]\\\n",
    "        .str.split()\\\n",
    "        .apply(lambda x : mark_negation(x))\\\n",
    "        .str.join(' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User manual filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.loc[\n",
    "#     (~(df['user'] == 'lost_dog') &\n",
    "#     ~(df['user'] == 'webwoke') &\n",
    "#     ~(df['user'] == 'tweetpet') &\n",
    "#     ~(df['user'].str.contains('tweeteradder')) &\n",
    "#     ~(df['user'].str.contains('tweetfollow')) &\n",
    "#     ~(df['user'] == 'divxdownloads')) |\n",
    "#     df['sentiment'].isna()\n",
    "# ]\n",
    "# df[df['sentiment'].isna()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 4.266666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "linearSVC_params = {\n",
    "    'penalty' : ['l1','l2'], #['l1','l2']\n",
    "    'dual' : [False],\n",
    "    'tol' : [1e-2,1e-3],\n",
    "    'fit_intercept' : [False],\n",
    "    'class_weight' : ['balanced'],\n",
    "    'max_iter' : [50,100],\n",
    "    'random_state' : [42],\n",
    "    'C' : [1,10]\n",
    "}\n",
    "\n",
    "TfidfVectorizer_params = {\n",
    "    'stop_words' : [None],\n",
    "    'ngram_range' : [(1,3)],\n",
    "    'max_features' : [None],\n",
    "    'max_df' : [1.0,0.001],\n",
    "    'min_df' : [1,0.000001],\n",
    "    'binary' : [True,False],\n",
    "    'norm' : ['l1','l2'],\n",
    "    'use_idf' : [True],\n",
    "    'smooth_idf' : [True,False],\n",
    "    'sublinear_tf' : [False]\n",
    "}\n",
    "\n",
    "\n",
    "number_different_configurations = len(list(ParameterGrid(TfidfVectorizer_params)))*len(list(ParameterGrid(linearSVC_params)))\n",
    "\n",
    "print(number_different_configurations, number_different_configurations*30/60/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# df_final = df\n",
    "# # df_final['text_final'] = df_final['text_noPunct'].apply(lambda x : ' '.join(x))\n",
    "# df_final['text_final'] = df_final['text_neg'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "# mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "# X_train_valid = df_final.loc[mask_train_test,:]['text_final'].values\n",
    "# y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "# X_test = df_final.loc[~mask_train_test,:]['text_final'].values\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "#     X_train_valid, \n",
    "#     y_train_valid, \n",
    "#     shuffle=True, \n",
    "#     # stratify=y_train_valid, \n",
    "#     train_size=0.9, \n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# vectorizer = 'tfidf'\n",
    "# model = 'linearSVC'\n",
    "\n",
    "# pipe = Pipeline([\n",
    "#     (vectorizer, TfidfVectorizer()),\n",
    "#     (model, LinearSVC())\n",
    "# ])\n",
    "\n",
    "# def params_for_GridSearchCV(params_IN,step_name):\n",
    "#     return {f'{step_name}__{key}':value for (key,value) in params_IN.items()}\n",
    "\n",
    "# params_all = params_for_GridSearchCV(TfidfVectorizer_params,vectorizer)\n",
    "# params_all.update(params_for_GridSearchCV(linearSVC_params,model))\n",
    "# print(params_all)\n",
    "\n",
    "# gscv = GridSearchCV(pipe, params_all, cv = 3, verbose = 4, scoring='f1_macro', n_jobs=6)\n",
    "\n",
    "# gscv.fit(X_train, y_train)\n",
    "\n",
    "# f1 = f1_score(y_valid, gscv.predict(X_valid),average='macro')\n",
    "# report = classification_report(y_valid, gscv.predict(X_valid))\n",
    "# confusion = confusion_matrix(y_valid, gscv.predict(X_valid))\n",
    "\n",
    "# print(f1)\n",
    "# print(report)\n",
    "# print(confusion)\n",
    "\n",
    "# from pathlib import Path\n",
    "# from datetime import datetime\n",
    "\n",
    "# results_name = Path.cwd()/'gscv_results'/'linearSVC_final_results.csv'\n",
    "\n",
    "# pd.DataFrame(gscv.cv_results_).to_csv(results_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.read_csv('gscv_results\\\\linearSVC_final_results2.csv')\n",
    "# print(results.loc[results['rank_test_score']==1]['params'].iloc[0])\n",
    "# print(results.loc[results['rank_test_score']==1]['params'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best models test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# df_final = df\n",
    "# # df_final['text_final'] = df_final['text_noPunct'].apply(lambda x : ' '.join(x))\n",
    "# df_final['text_final'] = df_final['text_neg'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "# mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "# X_train_valid = df_final.loc[mask_train_test,:]['text_final'].values\n",
    "# y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "# X_test = df_final.loc[~mask_train_test,:]['text_final'].values\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "#     X_train_valid, \n",
    "#     y_train_valid, \n",
    "#     shuffle=True, \n",
    "#     # stratify=y_train_valid, \n",
    "#     train_size=0.9, \n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# vectorizer = TfidfVectorizer(\n",
    "#     binary = True, \n",
    "#     max_df = 1.0, \n",
    "#     min_df = 1, \n",
    "#     max_features = None, \n",
    "#     ngram_range = (1,1), \n",
    "#     norm = 'l1', \n",
    "#     smooth_idf = True, \n",
    "#     stop_words = None, \n",
    "#     sublinear_tf = False, \n",
    "#     use_idf = True\n",
    "# )\n",
    "\n",
    "# model = LinearSVC(\n",
    "#     C = 1, \n",
    "#     class_weight = 'balanced', \n",
    "#     dual = False, \n",
    "#     fit_intercept = False, \n",
    "#     max_iter = 100, \n",
    "#     penalty = 'l1', \n",
    "#     random_state = 42, \n",
    "#     tol = 0.001\n",
    "# )\n",
    "\n",
    "# pipe = Pipeline([\n",
    "#     ('tfidf', vectorizer),\n",
    "#     ('linearSVC', model)\n",
    "# ])\n",
    "\n",
    "# pipe.fit(X_train, y_train)\n",
    "\n",
    "# f1 = f1_score(y_valid, pipe.predict(X_valid),average='macro')\n",
    "# report = classification_report(y_valid, pipe.predict(X_valid))\n",
    "# confusion = confusion_matrix(y_valid, pipe.predict(X_valid))\n",
    "\n",
    "# print(f1)\n",
    "# print(report)\n",
    "# print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIFIED: norm and ngram_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 346. GiB for an array with shape (2322826, 20010) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21492/1924100428.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m ])\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m \u001b[0mml_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mml_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\data_science_lab_2\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \"\"\"\n\u001b[0;32m    389\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\data_science_lab_2\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    346\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    349\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\data_science_lab_2\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\data_science_lab_2\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    891\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    894\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\data_science_lab_2\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    208\u001b[0m                     \u001b[1;34m\"n_components must be < n_features; got %d >= %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m                 )\n\u001b[1;32m--> 210\u001b[1;33m             U, Sigma, VT = randomized_svd(\n\u001b[0m\u001b[0;32m    211\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m             )\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\data_science_lab_2\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m     Q = randomized_range_finder(\n\u001b[0m\u001b[0;32m    396\u001b[0m         \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_random\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\data_science_lab_2\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[1;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"LU\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m             \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"QR\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\data_science_lab_2\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     if (\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\data_science_lab_2\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    558\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[0;32m    559\u001b[0m                              \"use '*' instead\")\n\u001b[1;32m--> 560\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__mul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\data_science_lab_2\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    469\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\data_science_lab_2\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m_mul_multivector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[0mn_vecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# number of column vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m         result = np.zeros((M, n_vecs),\n\u001b[0m\u001b[0;32m    487\u001b[0m                           dtype=upcast_char(self.dtype.char, other.dtype.char))\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 346. GiB for an array with shape (2322826, 20010) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "# text_noAmpEnt\n",
    "# text_noQuotEnt\n",
    "# text_noAt\n",
    "# text_noAmp\n",
    "# text_noHttp\n",
    "# text_noDuplLetters\n",
    "# text_noPunctuation\n",
    "# text_stem\n",
    "# text_lemm\n",
    "# text_neg\n",
    "\n",
    "df_final = df.copy(deep=True)\n",
    "df_final = df_final\\\n",
    "    .pipe(text_noAmpEnt, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noQuotEnt, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noAt, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noHttp, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_noPunctuation, field_IN='text', field_OUT='text')\\\n",
    "    .pipe(text_stem, field_IN='text', field_OUT='text', stemmer=SnowballStemmer)\\\n",
    "    .pipe(text_neg, field_IN='text', field_OUT='text')\n",
    "    # .pipe(text_lemm, field_IN='text', field_OUT='text', lemmatizer=WordNetLemmatizer)\n",
    "    # .pipe(text_stem, field_IN='text', field_OUT='text', stemmer=LancasterStemmer)\n",
    "\n",
    "df_final['text_final'] = df_final['text']\n",
    "\n",
    "mask_train_test = df_final['sentiment'].notna()\n",
    "\n",
    "X_train_valid = df_final.loc[mask_train_test,:]['text_final'].values\n",
    "y_train_valid = df_final.loc[mask_train_test,:]['sentiment'].values\n",
    "X_test = df_final.loc[~mask_train_test,:]['text_final'].values\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, \n",
    "    y_train_valid, \n",
    "    shuffle=True, \n",
    "    # stratify=y_train_valid, \n",
    "    train_size=0.9, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = LinearSVC(\n",
    "    C = 1, \n",
    "    class_weight = 'balanced', \n",
    "    dual = False, \n",
    "    fit_intercept = False, \n",
    "    max_iter = 100, \n",
    "    penalty = 'l2', \n",
    "    random_state = 42, \n",
    "    tol = 0.001\n",
    ")\n",
    "\n",
    "decomposer = TruncatedSVD(\n",
    "    n_components = 10,\n",
    "    n_iter = 10,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    binary = True, \n",
    "    max_df = 1.0, \n",
    "    max_features = None, \n",
    "    min_df = 1, \n",
    "    ngram_range = (1,3), \n",
    "    norm = 'l2', \n",
    "    smooth_idf = False, \n",
    "    stop_words = None, \n",
    "    sublinear_tf = False, \n",
    "    use_idf = True\n",
    ")\n",
    "\n",
    "ml_pipe = Pipeline([\n",
    "    ('tfidf', vectorizer),\n",
    "    ('TruncatedSVD', decomposer),\n",
    "    ('linearSVC', model)\n",
    "])\n",
    "\n",
    "ml_pipe.fit(X_train, y_train)\n",
    "\n",
    "f1 = f1_score(y_valid, ml_pipe.predict(X_valid),average='macro')\n",
    "report = classification_report(y_valid, ml_pipe.predict(X_valid))\n",
    "confusion = confusion_matrix(y_valid, ml_pipe.predict(X_valid))\n",
    "\n",
    "print(f1)\n",
    "print(report)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually removing users: 0.8019786529108555  \n",
    "Without manually removing users: 0.8007152526552623"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None : 0.7973008380157585  \n",
    "text_noAmpEnt : 0.806213112745712  \n",
    "text_noQuotEnt : 0.806582931090972  \n",
    "text_noAt : 0.7998564194923328 -- saltato  \n",
    "text_noAmp : 0.7994236544375262 -- saltato  \n",
    "text_noHttp : 0.7997304233939981 -- saltato  \n",
    "text_noDuplLetters : 0.8017853157058441 -- saltato  \n",
    "text_noPunctuation : 0.8020861827824957 --saltato  \n",
    "text_stem :   \n",
    "    Porter: 0.8014799266049696  \n",
    "    Lancaster : 0.7974966469180449  \n",
    "text_lemm : 0.797157501260997  \n",
    "text_neg : 0.7973008380157585  \n",
    "\n",
    "[  \n",
    "    text_noAmpEnt  \n",
    "    text_noQuotEnt  \n",
    "] : 0.806582931090972  \n",
    "  \n",
    "[  \n",
    "    text_noAmpEnt  \n",
    "    text_noQuotEnt  \n",
    "    text_noAt  \n",
    "    text_noHttp  \n",
    "    text_noPunctuation  \n",
    "    text_lemm  \n",
    "    text_neg  \n",
    "] : 0.7982539044206275  \n",
    "  \n",
    "[  \n",
    "    text_noAmpEnt  \n",
    "    text_noQuotEnt  \n",
    "    text_noAt  \n",
    "    text_noHttp  \n",
    "    text_noPunctuation  \n",
    "    text_stem(Porter)  \n",
    "    text_neg  \n",
    "] : 0.8000375978408897  \n",
    "  \n",
    "[  \n",
    "    text_noAmpEnt  \n",
    "    text_noQuotEnt  \n",
    "    text_noAt  \n",
    "    text_noHttp  \n",
    "    text_noPunctuation  \n",
    "    text_stem(Lancaster)  \n",
    "    text_neg  \n",
    "] : 0.7961716118953209  \n",
    "  \n",
    "[  \n",
    "    text_noAmpEnt  \n",
    "    text_noQuotEnt  \n",
    "    text_noAt  \n",
    "    text_noHttp  \n",
    "    text_noPunctuation  \n",
    "    text_stem(Snowball)  \n",
    "    text_neg  \n",
    "] : 0.8006420407066549  \n",
    "\n",
    "[  \n",
    "    text_noAmpEnt  \n",
    "    text_noQuotEnt  \n",
    "    text_noAt  \n",
    "    text_noHttp  \n",
    "    text_noPunctuation    \n",
    "    text_neg  \n",
    "] : 0.8006420407066549  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(\n",
    "    C = 1, \n",
    "    class_weight = 'balanced', \n",
    "    dual = False, \n",
    "    fit_intercept = False, \n",
    "    max_iter = 50, \n",
    "    penalty = 'l2', \n",
    "    random_state = 42, \n",
    "    tol = 0.001\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    binary = True, \n",
    "    max_df = 1.0, \n",
    "    max_features = None, \n",
    "    min_df = 1, \n",
    "    ngram_range = (1,3), \n",
    "    norm = 'l2', \n",
    "    smooth_idf = False, \n",
    "    stop_words = None, \n",
    "    sublinear_tf = False, \n",
    "    use_idf = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ NO stemming: 0.7953890537516292\n",
    "+ Snowball: 0.7994796670641988\n",
    "+ Lancaster: 0.7934227929849921\n",
    "+ Porter: 0.7982780155296589\n",
    "+ Wordnet: 0.7942085265529459"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_pipe.fit(X_train_valid, y_train_valid)\n",
    "# y_pred = ml_pipe.predict(X_test)\n",
    "# pd.DataFrame(y_pred.astype(int),columns=['Predicted']).to_csv(\"output_gram13_final.csv\",index_label=\"Id\", header=[\"Predicted\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd8ee3559d665fee903f84f74f9742602cb00cb47768a52cae0fe6e115d1a823"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('data_science_lab_2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
